{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SAFEDRUG.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "SUuGDBcnBqgs"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/russpv/SafeDrug/blob/main/SAFEDRUG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "oNRgh23mMLNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')\n",
        "\n",
        "!pip install memory_profiler\n",
        "!pip install rdkit-pypi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYz_mUWNIeFy",
        "outputId": "aff79907-1386-4b05-d225-13ce20f404b6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun May  8 19:17:08 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P0    27W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Your runtime has 54.8 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n",
            "Requirement already satisfied: memory_profiler in /usr/local/lib/python3.7/dist-packages (0.60.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from memory_profiler) (5.4.8)\n",
            "Requirement already satisfied: rdkit-pypi in /usr/local/lib/python3.7/dist-packages (2022.3.2.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from rdkit-pypi) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rdkit-pypi) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Args"
      ],
      "metadata": {
        "id": "8BwJfhjZ9q8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "def arg_parser():\n",
        "    \"\"\" Parse command line arguments\n",
        "\n",
        "    Outputs:\n",
        "        arguments {object} -- object containing command line arguments\n",
        "    \"\"\"\n",
        "\n",
        "    # Initializer\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # Add arguments here\n",
        "    parser.add_argument('--Test', action='store_true', default=False, help=\"test mode\")\n",
        "    parser.add_argument('--model_name', type=str, default='none', help=\"model name\")\n",
        "    parser.add_argument('--resume_path', type=str, default='none', help='resume path')\n",
        "    parser.add_argument('--lr', type=float, default=5e-4, help='learning rate')\n",
        "    parser.add_argument('--target_ddi', type=float, default=0.06, help='target ddi')\n",
        "    parser.add_argument('--kp', type=float, default=0.05, help='coefficient of P signal')\n",
        "    parser.add_argument('--dim', type=int, default=64, help='dimension')\n",
        "    parser.add_argument('--cuda', type=int, default=0, help='which cuda') ###\n",
        "\n",
        "    parser.add_argument('--smalldata', type=int, default=1, help='which cuda') ###\n",
        "    parser.add_argument('--mydata', type=int, default=1, help='which cuda') ###\n",
        "    parser.add_argument('--Inf_time', type=int, default=0, help='which cuda') ###\n",
        " \n",
        "    # Parse and return arguments\n",
        "    return(parser.parse_args(args=[]))\n",
        "\n",
        "args = arg_parser()"
      ],
      "metadata": {
        "id": "93d7Dc-My_0c"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import dill\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import pandas as pd\n",
        "import sys\n",
        "import time\n",
        "import statistics\n",
        "import datetime as dt\n",
        "import logging\n",
        "\n",
        "# set seed\n",
        "seed = 1203 #1203\n",
        "random.seed(seed)\n",
        "np.random.seed(seed) #2048\n",
        "torch.manual_seed(seed)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "\n",
        "# define data path\n",
        "DATA_PATH = \"drive/MyDrive/DL4H/Project/PaperCode/processed_orig/\"\n",
        "MYDATA_PATH = \"drive/MyDrive/DL4H/Project/SAFEDRUG_lib/data/processed/\"\n",
        "WORKING_PATH = \"drive/MyDrive/DL4H/Project/SAFEDRUG/\"\n",
        "TEST_PATH = \"drive/MyDrive/DL4H/Project/SAFEDRUG/results/\"\n",
        "\n",
        "# define dataset\n",
        "args.mydata = 0\n",
        "args.smalldata = 0\n",
        "EPOCH = 50\n",
        "\n",
        "# define routine\n",
        "args.Test = False\n",
        "args.Inf_time = True\n",
        "\n",
        "# setting\n",
        "args.model_name = 'SafeDrug_Repl_orig'\n",
        "args.resume_path = WORKING_PATH + 'saved/' + 'SafeDrug_Repl_origEpoch_25_TARGET_0.06_JA_0.5323_DDI_0.06914_2022-05-08 20:20:05.975347.model'\n",
        "# 'SafeDrug_ReplEpoch_39_TARGET_0.06_JA_0.2375_DDI_0.3543_2022-05-08 18:38:56.320473.model'\n",
        "logger = logging.getLogger('')\n",
        "logger.setLevel(logging.WARNING)"
      ],
      "metadata": {
        "id": "3MsT8WVD4TcK"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "4FkmKGB6zNqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVOs9wFb7CPK",
        "outputId": "c5532b08-a71c-46ee-efa8-a1cdf600c5c6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data switch\n",
        "if args.mydata == 1:\n",
        "    data_path = MYDATA_PATH + 'ehr.pkl'\n",
        "    voc_path = MYDATA_PATH + 'vocabs.pkl'\n",
        "\n",
        "    ehr_adj_path = MYDATA_PATH + 'ehradj.pkl'\n",
        "    ddi_adj_path = MYDATA_PATH + 'ddiadj.pkl'\n",
        "    ddi_mask_path = MYDATA_PATH + 'hmask.pkl'\n",
        "    molecule_path = MYDATA_PATH + 'atc2SMILES.pkl'\n",
        "\n",
        "    voc = dill.load(open(voc_path, 'rb'))\n",
        "    diag_voc, pro_voc, med_voc = voc['diag_vocab'].index2word, voc['pro_vocab'].index2word, voc['med_vocab'].index2word\n",
        "\n",
        "else:\n",
        "    data_path = DATA_PATH + 'records_final.pkl'\n",
        "    voc_path = DATA_PATH + 'voc_final.pkl'\n",
        "\n",
        "\n",
        "    ehr_adj_path = DATA_PATH + 'ehr_adj_final.pkl'\n",
        "    ddi_adj_path = DATA_PATH + 'ddi_A_final.pkl'\n",
        "    ddi_mask_path = DATA_PATH + 'ddi_mask_H.pkl'\n",
        "    molecule_path = DATA_PATH + 'atc3toSMILES.pkl'\n",
        "    \n",
        "    voc = dill.load(open(voc_path, 'rb'))\n",
        "    diag_voc, pro_voc, med_voc = voc['diag_voc'].idx2word, voc['pro_voc'].idx2word, voc['med_voc'].idx2word\n",
        "\n",
        "ehr_adj = dill.load(open(ehr_adj_path, 'rb'))\n",
        "ddi_adj = dill.load(open(ddi_adj_path, 'rb'))\n",
        "ddi_mask_H = dill.load(open(ddi_mask_path, 'rb'))\n",
        "data = dill.load(open(data_path, 'rb'))\n",
        "molecule = dill.load(open(molecule_path, 'rb')) \n",
        "\n",
        "if args.smalldata == 1:\n",
        "    data_train = data[:200] \n",
        "    data_test = data[200:250]\n",
        "    data_eval = data[250:300]\n",
        "else:\n",
        "    split_point = int(len(data) * 2 / 3)\n",
        "    data_train = data[:split_point]\n",
        "    eval_len = int(len(data[split_point:]) / 2)\n",
        "    data_test = data[split_point:split_point + eval_len]\n",
        "    data_eval = data[split_point+eval_len:]"
      ],
      "metadata": {
        "id": "j0dDadMmgAJV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "rkE-CJ6GpOIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import jaccard_score, roc_auc_score, precision_score, f1_score, average_precision_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "\n",
        "from collections import Counter\n",
        "from rdkit import Chem\n",
        "from collections import defaultdict\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def get_n_params(model):\n",
        "    pp=0\n",
        "    for p in list(model.parameters()):\n",
        "        nn=1\n",
        "        for s in list(p.size()):\n",
        "            nn = nn*s\n",
        "        pp += nn\n",
        "    return pp\n",
        "\n",
        "# use the same metric from DMNC\n",
        "def llprint(message):\n",
        "    sys.stdout.write(message)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "def transform_split(X, Y):\n",
        "    x_train, x_eval, y_train, y_eval = train_test_split(X, Y, train_size=2/3, random_state=1203)\n",
        "    x_eval, x_test, y_eval, y_test = train_test_split(x_eval, y_eval, test_size=0.5, random_state=1203)\n",
        "    return x_train, x_eval, x_test, y_train, y_eval, y_test\n",
        "\n",
        "def sequence_output_process(output_logits, filter_token):\n",
        "    pind = np.argsort(output_logits, axis=-1)[:, ::-1]\n",
        "\n",
        "    out_list = []\n",
        "    break_flag = False\n",
        "    for i in range(len(pind)):\n",
        "        if break_flag:\n",
        "            break\n",
        "        for j in range(pind.shape[1]):\n",
        "            label = pind[i][j]\n",
        "            if label in filter_token:\n",
        "                break_flag = True\n",
        "                break\n",
        "            if label not in out_list:\n",
        "                out_list.append(label)\n",
        "                break\n",
        "    y_pred_prob_tmp = []\n",
        "    for idx, item in enumerate(out_list):\n",
        "        y_pred_prob_tmp.append(output_logits[idx, item])\n",
        "    sorted_predict = [x for _, x in sorted(zip(y_pred_prob_tmp, out_list), reverse=True)]\n",
        "    return out_list, sorted_predict\n",
        "\n",
        "\n",
        "def sequence_metric(y_gt, y_pred, y_prob, y_label):\n",
        "    def average_prc(y_gt, y_label):\n",
        "        score = []\n",
        "        for b in range(y_gt.shape[0]):\n",
        "            target = np.where(y_gt[b]==1)[0]\n",
        "            out_list = y_label[b]\n",
        "            inter = set(out_list) & set(target)\n",
        "            prc_score = 0 if len(out_list) == 0 else len(inter) / len(out_list)\n",
        "            score.append(prc_score)\n",
        "        return score\n",
        "\n",
        "\n",
        "    def average_recall(y_gt, y_label):\n",
        "        score = []\n",
        "        for b in range(y_gt.shape[0]):\n",
        "            target = np.where(y_gt[b] == 1)[0]\n",
        "            out_list = y_label[b]\n",
        "            inter = set(out_list) & set(target)\n",
        "            recall_score = 0 if len(target) == 0 else len(inter) / len(target)\n",
        "            score.append(recall_score)\n",
        "        return score\n",
        "\n",
        "\n",
        "    def average_f1(average_prc, average_recall):\n",
        "        score = []\n",
        "        for idx in range(len(average_prc)):\n",
        "            if (average_prc[idx] + average_recall[idx]) == 0:\n",
        "                score.append(0)\n",
        "            else:\n",
        "                score.append(2*average_prc[idx]*average_recall[idx] / (average_prc[idx] + average_recall[idx]))\n",
        "        return score\n",
        "\n",
        "\n",
        "    def jaccard(y_gt, y_label):\n",
        "        score = []\n",
        "        for b in range(y_gt.shape[0]):\n",
        "            target = np.where(y_gt[b] == 1)[0]\n",
        "            out_list = y_label[b]\n",
        "            inter = set(out_list) & set(target)\n",
        "            union = set(out_list) | set(target)\n",
        "            jaccard_score = 0 if union == 0 else len(inter) / len(union)\n",
        "            score.append(jaccard_score)\n",
        "        return np.mean(score)\n",
        "\n",
        "    def f1(y_gt, y_pred):\n",
        "        all_micro = []\n",
        "        for b in range(y_gt.shape[0]):\n",
        "            all_micro.append(f1_score(y_gt[b], y_pred[b], average='macro'))\n",
        "        return np.mean(all_micro)\n",
        "\n",
        "    def roc_auc(y_gt, y_pred_prob):\n",
        "        all_micro = []\n",
        "        for b in range(len(y_gt)):\n",
        "            all_micro.append(roc_auc_score(y_gt[b], y_pred_prob[b], average='macro'))\n",
        "        return np.mean(all_micro)\n",
        "\n",
        "    def precision_auc(y_gt, y_prob):\n",
        "        all_micro = []\n",
        "        for b in range(len(y_gt)):\n",
        "            all_micro.append(average_precision_score(y_gt[b], y_prob[b], average='macro'))\n",
        "        return np.mean(all_micro)\n",
        "\n",
        "    def precision_at_k(y_gt, y_prob_label, k):\n",
        "        precision = 0\n",
        "        for i in range(len(y_gt)):\n",
        "            TP = 0\n",
        "            for j in y_prob_label[i][:k]:\n",
        "                if y_gt[i, j] == 1:\n",
        "                    TP += 1\n",
        "            precision += TP / k\n",
        "        return precision / len(y_gt)\n",
        "    try:\n",
        "        auc = roc_auc(y_gt, y_prob)\n",
        "    except ValueError:\n",
        "        auc = 0\n",
        "    p_1 = precision_at_k(y_gt, y_label, k=1)\n",
        "    p_3 = precision_at_k(y_gt, y_label, k=3)\n",
        "    p_5 = precision_at_k(y_gt, y_label, k=5)\n",
        "    f1 = f1(y_gt, y_pred)\n",
        "    prauc = precision_auc(y_gt, y_prob)\n",
        "    ja = jaccard(y_gt, y_label)\n",
        "    avg_prc = average_prc(y_gt, y_label)\n",
        "    avg_recall = average_recall(y_gt, y_label)\n",
        "    avg_f1 = average_f1(avg_prc, avg_recall)\n",
        "\n",
        "    return ja, prauc, np.mean(avg_prc), np.mean(avg_recall), np.mean(avg_f1)\n",
        "\n",
        "\n",
        "def multi_label_metric(y_gt, y_pred, y_prob):\n",
        "\n",
        "    def jaccard(y_gt, y_pred):\n",
        "        score = []\n",
        "        for b in range(y_gt.shape[0]):\n",
        "            target = np.where(y_gt[b] == 1)[0]\n",
        "            out_list = np.where(y_pred[b] == 1)[0]\n",
        "            inter = set(out_list) & set(target)\n",
        "            union = set(out_list) | set(target)\n",
        "            jaccard_score = 0 if union == 0 else len(inter) / len(union)\n",
        "            score.append(jaccard_score)\n",
        "        return np.mean(score)\n",
        "\n",
        "    def average_prc(y_gt, y_pred):\n",
        "        score = []\n",
        "        for b in range(y_gt.shape[0]):\n",
        "            target = np.where(y_gt[b] == 1)[0]\n",
        "            out_list = np.where(y_pred[b] == 1)[0]\n",
        "            inter = set(out_list) & set(target)\n",
        "            prc_score = 0 if len(out_list) == 0 else len(inter) / len(out_list)\n",
        "            score.append(prc_score)\n",
        "        return score\n",
        "\n",
        "    def average_recall(y_gt, y_pred):\n",
        "        score = []\n",
        "        for b in range(y_gt.shape[0]):\n",
        "            target = np.where(y_gt[b] == 1)[0]\n",
        "            out_list = np.where(y_pred[b] == 1)[0]\n",
        "            inter = set(out_list) & set(target)\n",
        "            recall_score = 0 if len(target) == 0 else len(inter) / len(target)\n",
        "            score.append(recall_score)\n",
        "        return score\n",
        "\n",
        "    def average_f1(average_prc, average_recall):\n",
        "        score = []\n",
        "        for idx in range(len(average_prc)):\n",
        "            if average_prc[idx] + average_recall[idx] == 0:\n",
        "                score.append(0)\n",
        "            else:\n",
        "                score.append(2*average_prc[idx]*average_recall[idx] / (average_prc[idx] + average_recall[idx]))\n",
        "        return score\n",
        "\n",
        "    def f1(y_gt, y_pred):\n",
        "        all_micro = []\n",
        "        for b in range(y_gt.shape[0]):\n",
        "            all_micro.append(f1_score(y_gt[b], y_pred[b], average='macro'))\n",
        "        return np.mean(all_micro)\n",
        "\n",
        "    def roc_auc(y_gt, y_prob):\n",
        "        all_micro = []\n",
        "        for b in range(len(y_gt)):\n",
        "            all_micro.append(roc_auc_score(y_gt[b], y_prob[b], average='macro'))\n",
        "        return np.mean(all_micro)\n",
        "\n",
        "    def precision_auc(y_gt, y_prob):\n",
        "        all_micro = []\n",
        "        for b in range(len(y_gt)):\n",
        "            all_micro.append(average_precision_score(y_gt[b], y_prob[b], average='macro'))\n",
        "        return np.mean(all_micro)\n",
        "\n",
        "    def precision_at_k(y_gt, y_prob, k=3):\n",
        "        precision = 0\n",
        "        sort_index = np.argsort(y_prob, axis=-1)[:, ::-1][:, :k]\n",
        "        for i in range(len(y_gt)):\n",
        "            TP = 0\n",
        "            for j in range(len(sort_index[i])):\n",
        "                if y_gt[i, sort_index[i, j]] == 1:\n",
        "                    TP += 1\n",
        "            precision += TP / len(sort_index[i])\n",
        "        return precision / len(y_gt)\n",
        "\n",
        "    # roc_auc\n",
        "    try:\n",
        "        auc = roc_auc(y_gt, y_prob)\n",
        "    except:\n",
        "        auc = 0\n",
        "    # precision\n",
        "    p_1 = precision_at_k(y_gt, y_prob, k=1)\n",
        "    p_3 = precision_at_k(y_gt, y_prob, k=3)\n",
        "    p_5 = precision_at_k(y_gt, y_prob, k=5)\n",
        "    # macro f1\n",
        "    f1 = f1(y_gt, y_pred)\n",
        "    # precision\n",
        "    prauc = precision_auc(y_gt, y_prob)\n",
        "    # jaccard\n",
        "    ja = jaccard(y_gt, y_pred)\n",
        "    # pre, recall, f1\n",
        "    avg_prc = average_prc(y_gt, y_pred)\n",
        "    avg_recall = average_recall(y_gt, y_pred)\n",
        "    avg_f1 = average_f1(avg_prc, avg_recall)\n",
        "\n",
        "    return ja, prauc, np.mean(avg_prc), np.mean(avg_recall), np.mean(avg_f1)\n",
        "\n",
        "def ddi_rate_score(record, path=ddi_adj_path): ###\n",
        "    # ddi rate\n",
        "    ddi_A = dill.load(open(path, 'rb'))\n",
        "    all_cnt = 0\n",
        "    dd_cnt = 0\n",
        "    for patient in record:\n",
        "        for adm in patient:\n",
        "            med_code_set = adm\n",
        "            for i, med_i in enumerate(med_code_set):\n",
        "                for j, med_j in enumerate(med_code_set):\n",
        "                    if j <= i:\n",
        "                        continue\n",
        "                    all_cnt += 1\n",
        "                    if ddi_A[med_i, med_j] == 1 or ddi_A[med_j, med_i] == 1:\n",
        "                        dd_cnt += 1\n",
        "    if all_cnt == 0:\n",
        "        return 0\n",
        "    return dd_cnt / all_cnt"
      ],
      "metadata": {
        "id": "kgRJkTTzpPPt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process molecules into graphs"
      ],
      "metadata": {
        "id": "t2JOux1J3NlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rdkit-pypi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L51UKQSE5CAH",
        "outputId": "7fed6e54-1af5-49c7-84f3-404e6a9f55df"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rdkit-pypi in /usr/local/lib/python3.7/dist-packages (2022.3.2.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from rdkit-pypi) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rdkit-pypi) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Util function that makes a drug graph\n",
        "# Input: dict of ATC:SMILES strings (up to 3); \n",
        "# Output: adj matrix\n",
        "# Leverage the BRICS package to parse SMILES\n",
        "\n",
        "from rdkit import Chem\n",
        "import dill, random, torch\n",
        "from collections import defaultdict\n",
        "\n",
        "DATA_PATH = \"drive/MyDrive/DL4H/Project/SAFEDRUG_lib/data/processed/\"\n",
        "SMILES_path = DATA_PATH + 'atc2SMILES.pkl'\n",
        "SMILES_file = dill.load(open(SMILES_path, 'rb')) \n",
        "\n",
        "#random.choice(list(SMILES_file.items()))\n",
        "# https://www.rdkit.org/docs/GettingStartedInPython.html\n",
        "# https://towardsdatascience.com/expressive-power-of-graph-neural-networks-and-the-weisefeiler-lehman-test-b883db3c7c49\n",
        "# https://www.youtube.com/watch?v=zCEYiCxrL_0&ab_channel=MicrosoftResearch\n",
        "\n",
        "def get_elements_by_index(mol, atomtyp_dict):\n",
        "    # Input: rdkit object\n",
        "    # Returns a list indexed on atom symbol (+ 'aromatic') for the molecule, sorry periodic table\n",
        "    symbols = [a.GetSymbol() for a in mol.GetAtoms()]\n",
        "    aromatics = [a.GetIdx() for a in mol.GetAromaticAtoms()]\n",
        "    symbols_with_aromatics = [(sym, 'aromatic') if i in aromatics else sym for i, sym in enumerate(symbols)]\n",
        "\n",
        "    # debugging: {symbol: node#s}\n",
        "    atom_dict = defaultdict(lambda: []) \n",
        "    for i, symbol in enumerate(symbols_with_aromatics):\n",
        "        atom_dict[symbol].append(i)\n",
        "\n",
        "    # update index of symbols\n",
        "    counter = len(atomtyp_dict)\n",
        "    for sym in symbols_with_aromatics:\n",
        "        if sym not in atomtyp_dict:\n",
        "            atomtyp_dict[sym] = counter\n",
        "            counter = len(atomtyp_dict)\n",
        "\n",
        "    # return list of nodes indexed on symbol list(atom#)\n",
        "    atom_list = [atomtyp_dict[a] for a in symbols_with_aromatics]\n",
        "\n",
        "    return atom_list, atomtyp_dict, atom_dict\n",
        "\n",
        "\n",
        "def get_bonds_by_index(mol, bondtyp_dict):\n",
        "    # Returns a dict of EDGETYPES specific to atom node# and bond type\n",
        "    # Definition of edgetype: {node#: (node#, bondtyp#)}\n",
        "    bonds = [(bond.GetBeginAtomIdx(), bond.GetEndAtomIdx(), str(bond.GetBondType())) for bond in mol.GetBonds()]\n",
        "\n",
        "    # update index of bond types (single, double, etc.)\n",
        "    counter = len(bondtyp_dict)\n",
        "    for (_, _, b) in bonds:\n",
        "        if b not in bondtyp_dict:\n",
        "            bondtyp_dict[b] = counter\n",
        "            counter = len(bondtyp_dict)\n",
        "\n",
        "    # make index of nodes with nodes connected to it {i: (j, single#)}\n",
        "    bond_dict = {}\n",
        "    for i, j, btyp in bonds:\n",
        "        if i not in bond_dict:\n",
        "            bond_dict[i] = [(j, bondtyp_dict[btyp])]\n",
        "        else:\n",
        "            bond_dict[i].append((j, bondtyp_dict[btyp]))\n",
        "        if j not in bond_dict:\n",
        "            bond_dict[j] = [(i, bondtyp_dict[btyp])]\n",
        "        else:\n",
        "            bond_dict[j].append((i, bondtyp_dict[btyp]))\n",
        "\n",
        "    #print(f'total bonds: {len(bonds)}')\n",
        "\n",
        "    # list(node#, node#, bondtyp#)\n",
        "    bond_list = [(bond.GetBeginAtomIdx(), bond.GetEndAtomIdx(), bondtyp_dict[str(bond.GetBondType())]) for bond in mol.GetBonds()]\n",
        "\n",
        "    return bond_list, bondtyp_dict, bond_dict"
      ],
      "metadata": {
        "id": "jUShcxHSqDrs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# have index (numbering) of nodes, bonds based on node id\n",
        "# Get the fingerprints for nodes, defined as the symbols and bond types from a node\n",
        "# Also, update index of edges hashed to nodeID by indexing edge types so nodeIdx: (neighborIdx, edgetype)\n",
        "\n",
        "def get_fingerprints(bonds, atoms, fptyp_dict, radius, fpbond_dict):\n",
        "    # Returns a list of FINGERPRINT IDS or subgraphs\n",
        "    # Definition of fingerprint: (atomtyp#, ((atomtyp#, bondtyp#), (atomtyp#, bondtyp#), ...))\n",
        "    # Input bonds = {node#: (node#, bondtyp#)}\n",
        "    # Input atoms = list(atomtyp#)\n",
        "    fp_list = []\n",
        "    fp_indexed = []\n",
        "    fp_dict_counter = len(fptyp_dict)\n",
        "    fpbond_dict_counter = len(fpbond_dict)\n",
        "\n",
        "    if radius == 0 or len(atoms) == 1:\n",
        "        for a in atoms:\n",
        "            # build/update index of fingerprint types (atomtyp#, atomtyp#, bondtyp#)\n",
        "            if a not in fptyp_dict:\n",
        "                fptyp_dict[a] = fp_dict_counter\n",
        "                fp_dict_counter = len(fptyp_dict)\n",
        "\n",
        "        fp_indexed = [fptyp_dict[a] for a in atoms]\n",
        "\n",
        "    else:\n",
        "        subgroups = atoms\n",
        "        subgroup_bonds = bonds\n",
        "\n",
        "        for _ in range(radius):\n",
        "            new_subgroups = []\n",
        "            new_bonds = defaultdict(list)\n",
        "\n",
        "            for i, bond in subgroup_bonds.items():\n",
        "                neighborhood = [(subgroups[j], btyp) for (j, btyp) in bond]\n",
        "                fingerprint_tuple = tuple([subgroups[i], tuple(sorted(neighborhood))])\n",
        "                \n",
        "                # build/update index of fingerprint types (subgrouptyp#, (subgrouptyp#, bondtyp#))\n",
        "                if fingerprint_tuple not in fptyp_dict:\n",
        "                    fptyp_dict[fingerprint_tuple] = fp_dict_counter\n",
        "                    fp_dict_counter = len(fptyp_dict)\n",
        "                \n",
        "                fp_list.append(fingerprint_tuple)\n",
        "                fp_indexed.append(fptyp_dict[fingerprint_tuple])\n",
        "\n",
        "                # build nodes to include fingerprints found for radius x+1\n",
        "                new_subgroups.append(fptyp_dict[fingerprint_tuple])\n",
        "\n",
        "                ''' redefine bonds between radius x and x+1 constituents '''\n",
        "                for j, bondtyp in bond:\n",
        "                    pairs = tuple(sorted((subgroups[i], subgroups[j])))\n",
        "\n",
        "                    # build/update dict of bonds between subgroups\n",
        "                    if (pairs, bondtyp) not in fpbond_dict:\n",
        "                        fpbond_dict[(pairs, bondtyp)] = fpbond_dict_counter\n",
        "                        fpbond_dict_counter = len(fpbond_dict)\n",
        "\n",
        "                    bond_ = fpbond_dict[(pairs, bondtyp)]\n",
        "                    new_bonds[i].append((j, bond_))\n",
        "            \n",
        "            subgroups = new_subgroups\n",
        "            subgroup_bonds = new_bonds\n",
        "\n",
        "    return fp_list, fptyp_dict, fp_indexed, subgroups"
      ],
      "metadata": {
        "id": "cGOcsSx--f89"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_MPNN_data(med_voc, SMILES_file, radius, device):\n",
        "    data = []\n",
        "    \n",
        "    fingerprinttyp_dict = {}\n",
        "    fingerprintbond_dict = {}\n",
        "\n",
        "    atomtyp_dict = {}\n",
        "    bondtyp_dict = {}\n",
        "    \n",
        "    smiles_count_index = []\n",
        "    for index, atc in med_voc.items():\n",
        "        counter = 0\n",
        "        for smiles_code in SMILES_file[atc]:\n",
        "            try: # some molecules cannot be processed; bond indexing shifted > list index mismatch\n",
        "                mol = Chem.AddHs(Chem.MolFromSmiles(smiles_code))\n",
        "                atoms, atomtyp_dict, atom_dict = get_elements_by_index(mol, atomtyp_dict)\n",
        "                bonds, bondtyp_dict, bond_dict = get_bonds_by_index(mol, bondtyp_dict)\n",
        "                atom_count = len(atoms)\n",
        "                _, fingerprinttyp_dict, fingerprints_hist, fingerprints = get_fingerprints(bond_dict, atoms, fingerprinttyp_dict, radius, fingerprintbond_dict)\n",
        "                \n",
        "                adj_matrix = Chem.GetAdjacencyMatrix(mol)\n",
        "                if adj_matrix.shape[0] != len(fingerprints):\n",
        "                    #print(f'mismatch shape: {atc} {adj_matrix.shape[0]} {len(fingerprints)}')\n",
        "                    for _ in range(adj_matrix.shape[0] - len(fingerprints)):\n",
        "                        fingerprints.append(1) # pad with '1'\n",
        "                    #print(f'fixed shape: {atc} {adj_matrix.shape[0]} {len(fingerprints)}')\n",
        "                fingerprints = torch.tensor(fingerprints, dtype=torch.long)\n",
        "                adj_matrix = torch.tensor(adj_matrix, dtype=torch.float32)\n",
        "                data.append((fingerprints, adj_matrix, atom_count))\n",
        "                counter += 1\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "                    \n",
        "        smiles_count_index.append(counter)\n",
        "\n",
        "    # build grid matrix for drugs x smiles (for convolution?)\n",
        "    total_cols = sum(smiles_count_index) # moleculecount\n",
        "    total_rows = len(smiles_count_index) # drugcount: paper does len(smiles_count_index) instead of med_voc\n",
        "    current_col = 0\n",
        "    \n",
        "    grid_matrix = [[0 for _ in range(total_cols)] for _ in range(total_rows)]\n",
        "    for i, smiles_count in enumerate(smiles_count_index):\n",
        "        for x in range(smiles_count):\n",
        "            grid_matrix[i][current_col+x] = 1 / smiles_count # normalized\n",
        "        current_col += smiles_count\n",
        "    \n",
        "    grid_matrix = torch.tensor(np.array(grid_matrix)).to(device)\n",
        "\n",
        "    # fingerprints of each molecule are appended with adj_matrix to the data object  \n",
        "    return data, len(fingerprinttyp_dict), bonds, bondtyp_dict, atomtyp_dict, fingerprinttyp_dict, fingerprints, grid_matrix"
      ],
      "metadata": {
        "id": "8iUTLSa8kUwq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' TESTING BLOCK\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    # MPNN Unit tests for small data set\n",
        "    #med_voc1 = {40: 'N06A', 39: 'N05C'}\n",
        "    med_voc1 = med_voc.index2word\n",
        "    molecule_test = SMILES_file\n",
        "    radius1 = 2\n",
        "    dataset1, fp_count1, bonds1, btyp1, atyp1, fptyp1, tempdata1, grid1 = make_MPNN_data(med_voc1, molecule_test, radius1, device='cuda')\n",
        "    \n",
        "    \n",
        "    import unittest\n",
        "    class myTest(unittest.TestCase):\n",
        "        def __init__(self, expected, actual):\n",
        "            super().__init__()\n",
        "            self.expected = expected\n",
        "            self.actual = actual\n",
        "        def test(self):         \n",
        "            for i, _ in enumerate(self.expected):\n",
        "                self.assertEqual(type(self.expected), type(self.actual), \"Should be same type\")  # check they are the same type\n",
        "                self.assertEqual(len(self.expected), len(self.actual), \"Should be same length\")  # check they are the same length \n",
        "                torch.testing.assert_close(self.expected[i][0], self.actual[i][0])  # check fingerprints\n",
        "                torch.testing.assert_close(self.expected[i][1], self.actual[i][1])  # check adjacency matrix \n",
        "                assert self.expected[i][2] == self.actual[i][2], \"Fingerprint count should be the same\"\n",
        "    \n",
        "    # COMPARE WITH PAPER\n",
        "    #datapaper, fpcountpaper, gridpaper = buildMPNN(molecule_test, med_voc1, radius1)\n",
        "    #test1 = myTest(dataset, datapaper)\n",
        "    #test1.test()\n",
        "'''"
      ],
      "metadata": {
        "id": "HT89H8GORklp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b146f1cc-906b-41da-a40e-c8e3a7daca4a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' TESTING BLOCK\\nif __name__ == \\'__main__\\':\\n    \\n    # MPNN Unit tests for small data set\\n    #med_voc1 = {40: \\'N06A\\', 39: \\'N05C\\'}\\n    med_voc1 = med_voc.index2word\\n    molecule_test = SMILES_file\\n    radius1 = 2\\n    dataset1, fp_count1, bonds1, btyp1, atyp1, fptyp1, tempdata1, grid1 = make_MPNN_data(med_voc1, molecule_test, radius1, device=\\'cuda\\')\\n    \\n    \\n    import unittest\\n    class myTest(unittest.TestCase):\\n        def __init__(self, expected, actual):\\n            super().__init__()\\n            self.expected = expected\\n            self.actual = actual\\n        def test(self):         \\n            for i, _ in enumerate(self.expected):\\n                self.assertEqual(type(self.expected), type(self.actual), \"Should be same type\")  # check they are the same type\\n                self.assertEqual(len(self.expected), len(self.actual), \"Should be same length\")  # check they are the same length \\n                torch.testing.assert_close(self.expected[i][0], self.actual[i][0])  # check fingerprints\\n                torch.testing.assert_close(self.expected[i][1], self.actual[i][1])  # check adjacency matrix \\n                assert self.expected[i][2] == self.actual[i][2], \"Fingerprint count should be the same\"\\n    \\n    # COMPARE WITH PAPER\\n    #datapaper, fpcountpaper, gridpaper = buildMPNN(molecule_test, med_voc1, radius1)\\n    #test1 = myTest(dataset, datapaper)\\n    #test1.test()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PAPER CODE"
      ],
      "metadata": {
        "id": "SUuGDBcnBqgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# PAPER CODE\n",
        "def create_atoms(mol, atom_dict):\n",
        "    \"\"\"Transform the atom types in a molecule (e.g., H, C, and O)\n",
        "    into the indices (e.g., H=0, C=1, and O=2).\n",
        "    Note that each atom index considers the aromaticity.\n",
        "    \"\"\"\n",
        "    atoms = [a.GetSymbol() for a in mol.GetAtoms()]\n",
        "    for a in mol.GetAromaticAtoms():\n",
        "        i = a.GetIdx()\n",
        "        atoms[i] = (atoms[i], 'aromatic')\n",
        "    atoms = [atom_dict[a] for a in atoms]\n",
        "    return np.array(atoms)\n",
        "\n",
        "def create_ijbonddict(mol, bond_dict):\n",
        "    \"\"\"Create a dictionary, in which each key is a node ID\n",
        "    and each value is the tuples of its neighboring node\n",
        "    and chemical bond (e.g., single and double) IDs.\n",
        "    \"\"\"\n",
        "    i_jbond_dict = defaultdict(lambda: [])\n",
        "    for b in mol.GetBonds():\n",
        "        i, j = b.GetBeginAtomIdx(), b.GetEndAtomIdx()\n",
        "        bond = bond_dict[str(b.GetBondType())]\n",
        "        i_jbond_dict[i].append((j, bond))\n",
        "        i_jbond_dict[j].append((i, bond))\n",
        "    return i_jbond_dict\n",
        "\n",
        "def extract_fingerprints(radius, atoms, i_jbond_dict,\n",
        "                         fingerprint_dict, edge_dict):\n",
        "    \"\"\"Extract the fingerprints from a molecular graph\n",
        "    based on Weisfeiler-Lehman algorithm.\n",
        "    \"\"\"\n",
        "\n",
        "    if (len(atoms) == 1) or (radius == 0):\n",
        "        nodes = [fingerprint_dict[a] for a in atoms]\n",
        "\n",
        "    else:\n",
        "        nodes = atoms\n",
        "        i_jedge_dict = i_jbond_dict\n",
        "\n",
        "        for _ in range(radius):\n",
        "\n",
        "            \"\"\"Update each node ID considering its neighboring nodes and edges.\n",
        "            The updated node IDs are the fingerprint IDs.\n",
        "            \"\"\"\n",
        "            nodes_ = []\n",
        "            for i, j_edge in i_jedge_dict.items():\n",
        "                neighbors = [(nodes[j], edge) for j, edge in j_edge]\n",
        "                fingerprint = (nodes[i], tuple(sorted(neighbors)))\n",
        "                nodes_.append(fingerprint_dict[fingerprint])\n",
        "\n",
        "            \"\"\"Also update each edge ID considering\n",
        "            its two nodes on both sides.\n",
        "            \"\"\"\n",
        "            i_jedge_dict_ = defaultdict(lambda: [])\n",
        "            for i, j_edge in i_jedge_dict.items():\n",
        "                for j, edge in j_edge:\n",
        "                    both_side = tuple(sorted((nodes[i], nodes[j])))\n",
        "                    edge = edge_dict[(both_side, edge)]\n",
        "                    i_jedge_dict_[i].append((j, edge))\n",
        "\n",
        "            nodes = nodes_\n",
        "            i_jedge_dict = i_jedge_dict_\n",
        "\n",
        "    return np.array(nodes)\n",
        "\n",
        "def buildMPNN(molecule, med_voc, radius=1, device=\"cpu:0\"):\n",
        "\n",
        "    atom_dict = defaultdict(lambda: len(atom_dict))\n",
        "    bond_dict = defaultdict(lambda: len(bond_dict))\n",
        "    fingerprint_dict = defaultdict(lambda: len(fingerprint_dict))\n",
        "    edge_dict = defaultdict(lambda: len(edge_dict))\n",
        "    MPNNSet, average_index = [], []\n",
        "\n",
        "    for index, atc3 in med_voc.items():\n",
        "\n",
        "        smilesList = list(molecule[atc3])\n",
        "        \"\"\"Create each data with the above defined functions.\"\"\"\n",
        "        counter = 0 # counter how many drugs are under that ATC-3\n",
        "        for smiles in smilesList:\n",
        "            try:\n",
        "                mol = Chem.AddHs(Chem.MolFromSmiles(smiles))\n",
        "                atoms = create_atoms(mol, atom_dict)\n",
        "                molecular_size = len(atoms)\n",
        "                i_jbond_dict = create_ijbonddict(mol, bond_dict)\n",
        "                fingerprints = extract_fingerprints(radius, atoms, i_jbond_dict,\n",
        "                                                    fingerprint_dict, edge_dict)\n",
        "                adjacency = Chem.GetAdjacencyMatrix(mol)\n",
        "                # if fingerprints.shape[0] == adjacency.shape[0]:\n",
        "                for _ in range(adjacency.shape[0] - fingerprints.shape[0]):\n",
        "                    #print(f'mismatch: {atc3} {adjacency.shape[0]} {fingerprints.shape[0]} ')\n",
        "                    fingerprints = np.append(fingerprints, 1)\n",
        "                \n",
        "                fingerprints = torch.LongTensor(fingerprints).to(device)\n",
        "                adjacency = torch.FloatTensor(adjacency).to(device)\n",
        "                MPNNSet.append((fingerprints, adjacency, molecular_size))\n",
        "                counter += 1\n",
        "            except:\n",
        "                continue\n",
        "        \n",
        "        average_index.append(counter)\n",
        "\n",
        "        \"\"\"Transform the above each data of numpy\n",
        "        to pytorch tensor on a device (i.e., CPU or GPU).\n",
        "        \"\"\"\n",
        "\n",
        "    N_fingerprint = len(fingerprint_dict)\n",
        "    # transform into projection matrix\n",
        "    n_col = sum(average_index)\n",
        "    n_row = len(average_index)\n",
        "\n",
        "    average_projection = np.zeros((n_row, n_col))\n",
        "    col_counter = 0\n",
        "    for i, item in enumerate(average_index):\n",
        "        if item > 0:\n",
        "            average_projection[i, col_counter : col_counter + item] = 1 / item\n",
        "        col_counter += item\n",
        "\n",
        "    return MPNNSet, N_fingerprint, torch.FloatTensor(average_projection)\n",
        "\n"
      ],
      "metadata": {
        "id": "RGSMO0IgeOB4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "EPR3vuO-pmfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MPNN(nn.Module):\n",
        "    ''' Create the drug memory embeddings '''\n",
        "    # z_next = message(fpi_l, fpj_l, W_l)\n",
        "    # fpi_next = update(fpj_l, z_next)\n",
        "    # concat fingerprints, add embedding dimension to them, activate feedforward, sumproduct (mm)\n",
        "    def __init__(self, fingerprint_count, embed_dim, L_layers, device):\n",
        "        super(MPNN, self).__init__()\n",
        "        self.device = device\n",
        "        self.hidden_layers = L_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(num_embeddings = fingerprint_count, embedding_dim = embed_dim).to(self.device)\n",
        "        self.linears = nn.ModuleList([nn.Linear(in_features = embed_dim, out_features = embed_dim).to(self.device)\n",
        "                                     for _ in range(L_layers)])\n",
        "\n",
        "    def pad(self, matrices, pad_value= 0.):\n",
        "        ''' Make one big adjacency matrix '''\n",
        "        shapes = [m.shape for m in matrices]\n",
        "        M, N = sum([m[0] for m in shapes]), sum([m[1] for m in shapes])\n",
        "        assert M == N, \"something wrong with adjacencies\"\n",
        "\n",
        "        bigmat = torch.zeros(M, M).to(self.device) + pad_value\n",
        "        col_marker = 0\n",
        "        row_marker = 0\n",
        "        for i, matrix in enumerate(matrices):\n",
        "            m, n = matrix.shape[0], matrix.shape[1]\n",
        "            bigmat[row_marker:row_marker+m, col_marker:col_marker+n] = matrix\n",
        "            col_marker += n\n",
        "            row_marker += m\n",
        "        return bigmat\n",
        "\n",
        "    def message(self, layer, input):\n",
        "        # feedforward across embedding dim (fingerprints x embed)\n",
        "        return torch.relu(self.linears[layer](input)) # Eq7: MESSAGE, the W(l) part\n",
        "        \n",
        "    def update(self, layer, matrix, input):\n",
        "        # sumproduct operation across connected nodes\n",
        "        return torch.mm(matrix, input) + input # Eq8: UPDATE\n",
        "\n",
        "    def readoutprep(self, input, splits):\n",
        "        # sum of vectors per molecule\n",
        "        result = [torch.sum(vector, dim=0) for vector in torch.split(input, splits)]\n",
        "        return torch.stack(result)\n",
        "\n",
        "    def forward(self, MPNN_data, grid):\n",
        "        # Input: Zipped dataset fingerprint lists, adjacency matrices, fingerprint lengths\n",
        "        # Operation: do ops on each set of nodes and adjacency matrix\n",
        "        # Output: convolved vectors\n",
        "\n",
        "        fp, adj, fp_lengths = MPNN_data\n",
        "        # combine and process at once\n",
        "        fp = torch.cat(fp).to(self.device)\n",
        "        adj = self.pad(adj)\n",
        "\n",
        "        fp = self.embedding(fp)\n",
        "        for i in range(self.hidden_layers):\n",
        "            x = self.message(i, fp)\n",
        "            x = self.update(i, adj, x)\n",
        "\n",
        "        # apply 1/n operation across connected nodes in one direction only\n",
        "        x = self.readoutprep(x, fp_lengths)\n",
        "        x = torch.mm(grid.float().to(self.device), x) # Eq9: READOUT average pooling\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "X83Hwi6O0sTA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatientQuery(nn.Module):\n",
        "    def __init__(self, emb_dim, vocab_size_diag, vocab_size_proc, device):\n",
        "        super(PatientQuery, self).__init__()\n",
        "        \"\"\" Output: patient representation, concat of diag/proc codes\n",
        "            RNNs process the embedding dimension\n",
        "        \"\"\"\n",
        "        self.device = device\n",
        "\n",
        "        self.embeddings_diag = nn.Embedding(vocab_size_diag, emb_dim)\n",
        "        self.embeddings_proc = nn.Embedding(vocab_size_proc, emb_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.encoder_diag = nn.GRU(input_size=emb_dim, hidden_size=emb_dim, batch_first=True)\n",
        "        self.encoder_proc = nn.GRU(input_size=emb_dim, hidden_size=emb_dim, batch_first=True)\n",
        "\n",
        "        self.query = nn.Sequential(\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(2 * emb_dim, emb_dim)\n",
        "        )\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.embeddings_diag.weight.data.uniform_(-initrange, initrange)\n",
        "        self.embeddings_proc.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, codes_diag, codes_proc):\n",
        "\n",
        "        diag_seq = []\n",
        "        proc_seq = []\n",
        "\n",
        "        # make sequences of visit embeddings\n",
        "        for visit in codes_diag:\n",
        "            diag = self.embeddings_diag(torch.LongTensor(visit).unsqueeze(dim=0).to(self.device))\n",
        "            diag = self.dropout(diag)\n",
        "            diag = torch.sum(diag, dim=1).unsqueeze(dim=0) # (1, 1, dim)\n",
        "            diag_seq.append(diag)\n",
        "\n",
        "        for visit in codes_proc:\n",
        "            proc = self.embeddings_diag(torch.LongTensor(visit).unsqueeze(dim=0).to(self.device))\n",
        "            proc = self.dropout(proc)\n",
        "            proc = torch.sum(proc, dim=1).unsqueeze(dim=0) # (1, 1, dim)\n",
        "            proc_seq.append(proc)\n",
        "\n",
        "        diag_seq = torch.cat(diag_seq, dim=1) #(1, seq, dim)\n",
        "        proc_seq = torch.cat(proc_seq, dim=1) #(1, seq, dim)\n",
        "\n",
        "        emb_diag, _ = self.encoder_diag(diag_seq) #(batch, seq, dim)\n",
        "        emb_proc, _ = self.encoder_proc(proc_seq)\n",
        "\n",
        "        emb_cat = torch.cat((emb_diag, emb_proc), dim=-1).squeeze(dim=0) # (seq, dim*2)\n",
        "        patient_representations = self.query(emb_cat)[-1:, :] # (seq, dim) \"-1:\" preserves shape\n",
        "\n",
        "        return patient_representations"
      ],
      "metadata": {
        "id": "T_npUTrlgnEF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PAPER CODE: See if this hand-roll controls loss runaway\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "class MaskLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super(MaskLinear, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, input, mask):\n",
        "        weight = torch.mul(self.weight, mask)\n",
        "        output = torch.mm(input, weight)\n",
        "\n",
        "        if self.bias is not None:\n",
        "            return output + self.bias\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' \\\n",
        "               + str(self.in_features) + ' -> ' \\\n",
        "               + str(self.out_features) + ')'"
      ],
      "metadata": {
        "id": "IgQyYugOIq0X"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BipartiteEncoder(nn.Module):\n",
        "    def __init__(self, emb_dim, mask_size, drug_size, bias=False):\n",
        "        super(BipartiteEncoder, self).__init__()\n",
        "        \"\"\" \n",
        "            Bipartite Learning\n",
        "            Input: mask_size = subgroups, drug_size = drugs\n",
        "            Output: \n",
        "            m_f functionality vector, dim transformation: feed-forward layer NN3, emb_dim -> S subgroups\n",
        "                sigmoid\n",
        "            m_l local drug representation, feed-forward layer NN4, m_f * H mask, S -> M drugs\n",
        "        \"\"\"\n",
        "        # adjust shape of patient query to ddi table - subgroups S\n",
        "        self.transformation1 = nn.Linear(in_features=emb_dim, out_features=mask_size)\n",
        "        # apply ddi mask\n",
        "        #self.mask = HMaskApplicator()\n",
        "        # adjust shape of result to ddi table - drugs M\n",
        "        self.transformation2 = nn.Linear(in_features=mask_size, out_features=drug_size, bias=bias)\n",
        "        # PAPER CODE\n",
        "        self.bipartite_output = MaskLinear(in_features=mask_size, out_features=drug_size, bias=False)\n",
        "\n",
        "    def forward(self, input, mask):\n",
        "        x = self.transformation1(input)\n",
        "        ''' POINT OF DIFF, no sigmoid in paper code'''\n",
        "        #x = torch.sigmoid(x) #Eq12: NN3, mf, sigmoid not in the paper code!\n",
        "        ''' POINT OF DIFF, handroll with param-reset vs in-place'''\n",
        "        self.transformation2.weight.data.mul_(mask) # Eq13: NN4 w/o bias, apply ddi mask H to weights\n",
        "        x = self.transformation2(x)\n",
        "        # PAPER CODE\n",
        "        # x = self.bipartite_output(x, mask.t())\n",
        "       \n",
        "        return x"
      ],
      "metadata": {
        "id": "CYyCAFTogzn0"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SafeDrug(nn.Module): \n",
        "    def __init__(self, vocab_size_diag, vocab_size_proc, H_mask, ddi_adj, MPNN_data, fingerprint_count, averaging_grid, emb_dim=256, L_layers=2, device='cuda'):\n",
        "        super(SafeDrug, self).__init__()\n",
        "        self.device = device\n",
        "        self.H_mask = H_mask\n",
        "\n",
        "        # embeddings > RNN encoders > patient query\n",
        "        self.patient_rep = PatientQuery(emb_dim, vocab_size_diag, vocab_size_proc, device)\n",
        "\n",
        "        # bipartite encoder\n",
        "        subgroup_size = H_mask.shape[1]\n",
        "        drug_size = H_mask.shape[0]\n",
        "        self.bipartite = BipartiteEncoder(emb_dim, subgroup_size, drug_size, False)\n",
        "\n",
        "        # MPNN, do message passing first\n",
        "        MPNN_data = list(zip(*MPNN_data))\n",
        "        self.MPNN = MPNN(fingerprint_count, emb_dim, L_layers, device).forward(MPNN_data, averaging_grid)\n",
        "        self.MPNN_match = nn.Sigmoid() #Eq10 patient-to-drug matching, m_r\n",
        "        self.MPNN_output = nn.Linear(drug_size, drug_size) # NN2\n",
        "        self.MPNN_layernorm = nn.LayerNorm(drug_size) #Eq11: LN, m_g\n",
        "\n",
        "        # setup\n",
        "        self.ddi_adj = torch.FloatTensor(ddi_adj).to(device)\n",
        "        self.H_mask = torch.FloatTensor(H_mask).to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        input_diag = [visit[0] for visit in x]\n",
        "        input_proc = [visit[1] for visit in x]\n",
        "        \n",
        "        # patient representation\n",
        "        query = self.patient_rep(input_diag, input_proc) # (seq, dim)\n",
        "        # dual molecular graph encoders\n",
        "        emb_bipartite = self.bipartite(query, self.H_mask)\n",
        "\n",
        "        matches = self.MPNN_match(torch.mm(query, self.MPNN.t())) # Eq10: m_r relevant drugs, (seq, dim) x (dim, drug)\n",
        "        scores = self.MPNN_output(matches)\n",
        "        attention = self.MPNN_layernorm(matches + scores) #Eq11: NN2\n",
        "\n",
        "        # medication representation (1,M or voc_size)\n",
        "        result = torch.mul(emb_bipartite, attention) #Eq14: (elementwise mult) but do sigmoid in training\n",
        "\n",
        "        if self.training:\n",
        "            ''' DDI Loss '''\n",
        "            neg_pred_prob = torch.sigmoid(result)\n",
        "            neg_pred_prob = torch.mul(neg_pred_prob.t(), neg_pred_prob)  # (voc_size, voc_size)\n",
        "            loss_ddi = 0.0005 * neg_pred_prob.mul(self.ddi_adj).sum() # Eq 17: L_ddi, the coefficient here is not shown\n",
        "\n",
        "            return result, loss_ddi\n",
        "        else:\n",
        "            return result"
      ],
      "metadata": {
        "id": "Khb2gaOs0v5f"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train & Inference"
      ],
      "metadata": {
        "id": "-02X8vcDpXI8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eval"
      ],
      "metadata": {
        "id": "MSMGdJtz5rKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval(model, data_eval, voc_size, epoch):\n",
        "    model.eval()\n",
        "\n",
        "    smm_record = []\n",
        "    ja, prauc, avg_p, avg_r, avg_f1 = [[] for _ in range(5)]\n",
        "    med_cnt, visit_cnt = 0, 0\n",
        "\n",
        "    for step, input in enumerate(data_eval):\n",
        "        y_gt, y_pred, y_pred_prob, y_pred_label = [], [], [], []\n",
        "        for adm_idx, adm in enumerate(input):\n",
        "            target_output = model(input[:adm_idx+1])\n",
        "\n",
        "            y_gt_tmp = np.zeros(voc_size[2])\n",
        "            y_gt_tmp[adm[2]] = 1\n",
        "            y_gt.append(y_gt_tmp)\n",
        "\n",
        "            # prediction prod\n",
        "            target_output = torch.sigmoid(target_output).detach().cpu().numpy()[0]\n",
        "            y_pred_prob.append(target_output)\n",
        "            \n",
        "            # prediction med set\n",
        "            y_pred_tmp = target_output.copy()\n",
        "            y_pred_tmp[y_pred_tmp>=0.5] = 1\n",
        "            y_pred_tmp[y_pred_tmp<0.5] = 0\n",
        "            y_pred.append(y_pred_tmp)\n",
        "\n",
        "            # prediction label\n",
        "            y_pred_label_tmp = np.where(y_pred_tmp == 1)[0]\n",
        "            y_pred_label.append(sorted(y_pred_label_tmp))\n",
        "            visit_cnt += 1\n",
        "            med_cnt += len(y_pred_label_tmp)\n",
        "\n",
        "        smm_record.append(y_pred_label)\n",
        "        adm_ja, adm_prauc, adm_avg_p, adm_avg_r, adm_avg_f1 = multi_label_metric(np.array(y_gt), np.array(y_pred), np.array(y_pred_prob))\n",
        "\n",
        "        ja.append(adm_ja)\n",
        "        prauc.append(adm_prauc)\n",
        "        avg_p.append(adm_avg_p)\n",
        "        avg_r.append(adm_avg_r)\n",
        "        avg_f1.append(adm_avg_f1)\n",
        "        llprint('\\rtest step: {} / {}'.format(step+1, len(data_eval)))\n",
        "\n",
        "    # ddi rate\n",
        "    ddi_rate = ddi_rate_score(smm_record, path=ddi_adj_path) ###\n",
        "\n",
        "    llprint('\\nDDI Rate: {:.4}, Jaccard: {:.4},  PRAUC: {:.4}, AVG_PRC: {:.4}, AVG_RECALL: {:.4}, AVG_F1: {:.4}, AVG_MED: {:.4}\\n'.format(\n",
        "        ddi_rate, np.mean(ja), np.mean(prauc), np.mean(avg_p), np.mean(avg_r), np.mean(avg_f1), med_cnt / visit_cnt\n",
        "    ))\n",
        "\n",
        "    return ddi_rate, np.mean(ja), np.mean(prauc), np.mean(avg_p), np.mean(avg_r), np.mean(avg_f1), med_cnt / visit_cnt"
      ],
      "metadata": {
        "id": "CGHooO8Ho1ZO"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "dGOQiP9o5tb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "def main():\n",
        "    device = torch.device(f'cuda:{args.cuda}' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    if args.mydata == 1:\n",
        "        MPNNSet, N_fingerprint, _, _, _, _, _, average_projection = make_MPNN_data(med_voc, molecule, 2, device)\n",
        "        voc_size = (len(diag_voc), len(pro_voc), len(med_voc))\n",
        "    else:\n",
        "        MPNNSet, N_fingerprint, average_projection = buildMPNN(molecule, med_voc, 2, device)\n",
        "        voc_size = (len(diag_voc), len(pro_voc), len(med_voc))\n",
        "\n",
        "    model = SafeDrug(voc_size[0], voc_size[1], ddi_mask_H, ddi_adj, MPNNSet, N_fingerprint, average_projection, emb_dim=args.dim, L_layers=2, device=device)\n",
        "    # model.load_state_dict(torch.load(open(args.resume_path, 'rb')))\n",
        "    \n",
        "    if args.Inf_time:\n",
        "        #https://towardsdatascience.com/the-correct-way-to-measure-inference-time-of-deep-neural-networks-304a54e5187f\n",
        "        model.load_state_dict(torch.load(open(args.resume_path, 'rb')))\n",
        "        model.to(device=device)\n",
        "        tic = time.time()\n",
        "\n",
        "        starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
        "        repetitions = len(data_test)\n",
        "        timings = np.zeros((repetitions,1))\n",
        "        dummy_input = [[[13, 98, 585, 1065, 21, 37, 454, 278], [69, 47], [4, 22, 12, 2, 67, 0, 86]],\\\n",
        "                       [[377, 326, 21, 46, 454], [115, 94], [3, 6, 12, 14, 5, 22, 2, 29, 1, 16, 11, 86]],\\\n",
        "                       [[377, 246, 453, 46, 21, 454], [151, 127, 128], [14, 2, 6, 29, 18, 0, 86]], [[963, 258, 32, 93, 94, 13, 103, 571, 21], [164, 423, 424, 425, 95, 426, 361, 48, 46, 2], [5, 4, 6, 7, 9, 11, 12, 3, 13, 16, 14, 22, 1, 2, 29, 44, 45, 48, 56, 20, 76, 86]]]\n",
        "\n",
        "        #GPU-WARM-UP\n",
        "        for _ in range(10):\n",
        "            _ = model(dummy_input)\n",
        "        count = 0\n",
        "\n",
        "        # MEASURE PERFORMANCE\n",
        "        with torch.no_grad():\n",
        "            #for rep in range(repetitions):\n",
        "            for rep, example in enumerate(data_test):\n",
        "                starter.record()\n",
        "                _ = model(example)\n",
        "                ender.record()\n",
        "                # WAIT FOR GPU SYNC\n",
        "                torch.cuda.synchronize()\n",
        "                curr_time = starter.elapsed_time(ender)\n",
        "                timings[rep] = curr_time\n",
        "                count += 1\n",
        "\n",
        "        mean_syn = np.sum(timings) / repetitions\n",
        "        std_syn = np.std(timings)\n",
        "        print(f'Inference reps {count}, average: {mean_syn} \\u00B1 {std_syn} seconds')\n",
        "\n",
        "        data = np.array([mean_syn, std_syn, count])\n",
        "        df = pd.DataFrame(data, index=['mean inference time', 'stdev', 'reps'])\n",
        "        df.to_csv(TEST_PATH + 'Inf_' + args.model_name + device.type + f'{dt.datetime.now()}' + '.csv' )\n",
        "\n",
        "        return\n",
        "\n",
        "    if args.Test:\n",
        "        model.load_state_dict(torch.load(open(args.resume_path, 'rb')))\n",
        "        model.to(device=device)\n",
        "        tic = time.time()\n",
        "\n",
        "        ddi_list, ja_list, prauc_list, f1_list, med_list = [], [], [], [], []\n",
        "\n",
        "        result = []\n",
        "        for _ in range(10):\n",
        "            time_start = time.time()\n",
        "            test_sample = np.random.choice(data_test, round(len(data_test) * 0.8), replace=True)\n",
        "            ddi_rate, ja, prauc, avg_p, avg_r, avg_f1, avg_med = eval(model, test_sample, voc_size, 0)\n",
        "            time_sample = time.time() - time_start ###\n",
        "            result.append([ddi_rate, ja, avg_f1, prauc, avg_med, time_sample])\n",
        "            \n",
        "        result = np.array(result)\n",
        "        mean = result.mean(axis=0)\n",
        "        std = result.std(axis=0)\n",
        "\n",
        "        outstring = \"\"\n",
        "        for m, s in zip(mean, std):\n",
        "            outstring += \"{:.4f} \"u\"\\u00B1\"\" {:.4f} & \".format(m, s) ###\n",
        "\n",
        "        print(outstring)\n",
        "        time_round = time.time() - tic\n",
        "        print(f'test time: {time_round}')\n",
        "        \n",
        "        elapsed_time = [0. for _ in range(5)]\n",
        "        elapsed_time.append(time_round)\n",
        "        data = np.array([mean, std, elapsed_time])\n",
        "\n",
        "        df = pd.DataFrame(data, columns=['ddi', 'ja', 'prauc', 'f1', 'med', 'time'], index=['mean', 'std', 'seconds'])\n",
        "        df.to_csv(TEST_PATH + 'Test_' + args.model_name + device.type + f'{dt.datetime.now()}' + '.csv' )\n",
        "\n",
        "        return \n",
        "\n",
        "    if 'cpu' not in device.type:\n",
        "        torch.cuda.reset_peak_memory_stats() # flush \n",
        "    model.to(device=device)\n",
        "\n",
        "    print('parameters', sum(p.numel() for p in model.parameters() if p.requires_grad)) ###\n",
        "    # exit()\n",
        "    optimizer = Adam(list(model.parameters()), lr=args.lr)\n",
        "\n",
        "    # start iterations\n",
        "    history = defaultdict(list)\n",
        "    best_epoch, best_ja = 0, 0\n",
        "\n",
        "    times_train, times_eval = [], [] ###\n",
        "    for epoch in range(EPOCH):\n",
        "        time_start = time.time() ###\n",
        "        print ('\\nepoch {} --------------------------'.format(epoch + 1))\n",
        "        \n",
        "        model.train()\n",
        "        beta_log = []\n",
        "        for step, patient in enumerate(data_train): # PATIENT, visit, (diag, proc, med), codes\n",
        "           \n",
        "            loss = 0\n",
        "            for idx, visit in enumerate(patient): # patient, VISIT, (diag, proc, med), codes\n",
        "                \n",
        "                seq_patient = patient[:idx+1] #  sequential expansion wise\n",
        "\n",
        "                loss_bce_target = np.zeros((1, voc_size[2]))\n",
        "                loss_bce_target[:, visit[2]] = 1\n",
        "                loss_multi_target = np.full((1, voc_size[2]), -1)\n",
        "                for idx, item in enumerate(visit[2]): # patient, visit, (diag, prod, MED), codes\n",
        "                    loss_multi_target[0][idx] = item\n",
        "\n",
        "                result, loss_ddi = model(seq_patient)\n",
        "\n",
        "                loss_bce = F.binary_cross_entropy_with_logits(result, torch.FloatTensor(loss_bce_target).to(device))\n",
        "                loss_multi = F.multilabel_margin_loss(torch.sigmoid(result), torch.LongTensor(loss_multi_target).to(device))\n",
        "\n",
        "                result = torch.sigmoid(result).detach().cpu().numpy()[0] # Apply final sigmoid()\n",
        "                result[result >= 0.5] = 1\n",
        "                result[result < 0.5] = 0\n",
        "                y_label = np.where(result == 1)[0]\n",
        "                current_ddi_rate = ddi_rate_score([[y_label]], path=ddi_adj_path) # External DDI knowledge\n",
        "                \n",
        "                if current_ddi_rate <= args.target_ddi:\n",
        "                    loss = 0.95 * loss_bce + 0.05 * loss_multi\n",
        "                    beta = 1\n",
        "                else:\n",
        "                    beta = max(0, 1 - (current_ddi_rate - args.target_ddi) / args.kp) # per published paper\n",
        "                    loss = beta * (0.95 * loss_bce + 0.05 * loss_multi) + (1 - beta) * loss_ddi # alpha = 0.95\n",
        "                beta_log.append(beta)\n",
        "                \n",
        "                optimizer.zero_grad()\n",
        "                loss.backward(retain_graph=True)\n",
        "                optimizer.step()\n",
        "\n",
        "            llprint(f'\\rtraining step: {step+1} / {len(data_train)} loss: {loss} loss_ddi: {loss_ddi}  beta: {beta}') ###\n",
        "        \n",
        "        print()\n",
        "        print(f'\\navg_beta: {statistics.mean(beta_log)}') ###\n",
        "        time_end = time.time()  ###\n",
        "        ddi_rate, ja, prauc, avg_p, avg_r, avg_f1, avg_med = eval(model, data_eval, voc_size, epoch)\n",
        "        time_train = time_end - time_start ###\n",
        "        time_eval = time.time() - time_end  ###\n",
        "        print(f'training time: {time_train}, test time: {time_eval}') ###\n",
        "\n",
        "        times_train.append(time_train) ###\n",
        "        times_eval.append(time_eval) ###\n",
        "\n",
        "        history['ja'].append(ja)\n",
        "        history['ddi_rate'].append(ddi_rate)\n",
        "        history['avg_p'].append(avg_p)\n",
        "        history['avg_r'].append(avg_r)\n",
        "        history['avg_f1'].append(avg_f1)\n",
        "        history['prauc'].append(prauc)\n",
        "        history['med'].append(avg_med)\n",
        "\n",
        "        if epoch >= 5:\n",
        "            print('ddi: {}, Med: {}, Ja: {}, F1: {}, PRAUC: {}'.format(\n",
        "                np.mean(history['ddi_rate'][-5:]),\n",
        "                np.mean(history['med'][-5:]),\n",
        "                np.mean(history['ja'][-5:]),\n",
        "                np.mean(history['avg_f1'][-5:]),\n",
        "                np.mean(history['prauc'][-5:])\n",
        "                ))\n",
        "\n",
        "        torch.save(model.state_dict(), open(WORKING_PATH +''.join(('saved/', args.model_name, \\\n",
        "            'Epoch_{}_TARGET_{:.2}_JA_{:.4}_DDI_{:.4}_{}.model'.format(epoch, args.target_ddi, ja, ddi_rate, dt.datetime.now()))), 'wb')) ###\n",
        "\n",
        "        if epoch != 0 and best_ja < ja:\n",
        "            best_epoch = epoch\n",
        "            best_ja = ja\n",
        "\n",
        "        print('best_epoch: {}'.format(best_epoch))\n",
        "\n",
        "    dill.dump(history, open(WORKING_PATH +'history_{}_{}.pkl'.format(args.model_name, dt.datetime.now()), 'wb')) ###\n",
        "    \n",
        "    timings = np.array(list(zip(times_train, times_eval))) ###\n",
        "    df = pd.DataFrame(timings, columns=['train', 'test']) ###\n",
        "    df.to_csv(TEST_PATH + 'TimesTrain_' + args.model_name + f'{dt.datetime.now()}' + '.csv' ) ###\n",
        "\n",
        "    # Maximum cuda memory allocated\n",
        "    if 'cpu' not in device.type:\n",
        "        print(f'peak training memory allocated: {torch.cuda.max_memory_allocated(device)}')\n"
      ],
      "metadata": {
        "id": "EKHT7z24G8xa"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execute"
      ],
      "metadata": {
        "id": "2C9UVxkm1KJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    %reload_ext memory_profiler\n",
        "    %memit -r1 main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQCXbnYM1IX5",
        "outputId": "7061b9e3-cffa-48ce-dd90-3fc684c5be26"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference reps 1058, average: 1.197962977597754  0.22361619573222505 seconds\n",
            "peak memory: 4506.52 MiB, increment: 0.36 MiB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8iT_ANCJMwq",
        "outputId": "37766195-82f2-4b55-f966-ca11501386f6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun May  8 21:18:14 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   68C    P0    31W /  70W |   2728MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    }
  ]
}