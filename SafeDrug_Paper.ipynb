{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SafeDrug_Paper.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "SUuGDBcnBqgs"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/russpv/SafeDrug/blob/main/SafeDrug_Paper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Args"
      ],
      "metadata": {
        "id": "R6p8FuV5AgL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')\n",
        "\n",
        "! pip install memory_profiler\n",
        "from torch.profiler import profile, record_function, ProfilerActivity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyXxLXmLAx21",
        "outputId": "2be4a355-3312-4df0-bb77-a4e4e8eaad69"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon May  9 00:04:59 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Your runtime has 54.8 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n",
            "Collecting memory_profiler\n",
            "  Downloading memory_profiler-0.60.0.tar.gz (38 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from memory_profiler) (5.4.8)\n",
            "Building wheels for collected packages: memory-profiler\n",
            "  Building wheel for memory-profiler (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for memory-profiler: filename=memory_profiler-0.60.0-py3-none-any.whl size=31284 sha256=8f59c005e4d67b02c20b0d54f8ab2f9cb0f34e92572fa8623bd0f9a5759f6596\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/2b/fb/326e30d638c538e69a5eb0aa47f4223d979f502bbdb403950f\n",
            "Successfully built memory-profiler\n",
            "Installing collected packages: memory-profiler\n",
            "Successfully installed memory-profiler-0.60.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "def arg_parser():\n",
        "    \"\"\" Parse command line arguments\n",
        "\n",
        "    Outputs:\n",
        "        arguments {object} -- object containing command line arguments\n",
        "    \"\"\"\n",
        "\n",
        "    # Initializer\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # Add arguments here\n",
        "    parser.add_argument('--Test', action='store_true', default=False, help=\"test mode\")\n",
        "    parser.add_argument('--model_name', type=str, default='none', help=\"model name\")\n",
        "    parser.add_argument('--resume_path', type=str, default='none', help='resume path')\n",
        "    parser.add_argument('--lr', type=float, default=5e-4, help='learning rate')\n",
        "    parser.add_argument('--target_ddi', type=float, default=0.06, help='target ddi')\n",
        "    parser.add_argument('--kp', type=float, default=0.05, help='coefficient of P signal')\n",
        "    parser.add_argument('--dim', type=int, default=64, help='dimension')\n",
        "    parser.add_argument('--cuda', type=int, default=0, help='which cuda') ###\n",
        "\n",
        "    parser.add_argument('--smalldata', type=int, default=1, help='which cuda') ###\n",
        "    parser.add_argument('--mydata', type=int, default=1, help='which cuda') ###\n",
        "    parser.add_argument('--Inf_time', type=int, default=0, help='which cuda') ###\n",
        " \n",
        "    # Parse and return arguments\n",
        "    return(parser.parse_args(args=[]))\n",
        "\n",
        "args = arg_parser()"
      ],
      "metadata": {
        "id": "AQdlVYzRPJSr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import dill\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import pandas as pd\n",
        "import sys\n",
        "import time\n",
        "import statistics\n",
        "import datetime as dt\n",
        "\n",
        "# set seed\n",
        "seed = 1203 #1203\n",
        "random.seed(seed)\n",
        "np.random.seed(seed) #2048\n",
        "torch.manual_seed(seed)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "\n",
        "# define data path\n",
        "DATA_PATH = \"drive/MyDrive/DL4H/Project/PaperCode/processed/\"\n",
        "MYDATA_PATH = \"drive/MyDrive/DL4H/Project/SAFEDRUG_lib/data/processed/\"\n",
        "WORKING_PATH = \"drive/MyDrive/DL4H/Project/PaperCode/\"\n",
        "TEST_PATH = \"drive/MyDrive/DL4H/Project/PaperCode/results/\"\n",
        "\n",
        "# define dataset\n",
        "args.mydata = 0\n",
        "args.smallset = 0\n",
        "EPOCH = 50\n",
        "\n",
        "# define routine\n",
        "args.Test = True\n",
        "args.Inf_time = False\n",
        "\n",
        "# setting\n",
        "args.model_name = 'SafeDrug_PAPER'\n",
        "\n",
        "args.resume_path = WORKING_PATH + 'saved/' + 'SafeDrug_PAPEREpoch_2_TARGET_0.06_JA_0.174_DDI_0.1013_2022-05-08 19:12:49.459697.model'"
      ],
      "metadata": {
        "id": "3MsT8WVD4TcK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "iFy2RiHZjGMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PodCRs3ISvlt",
        "outputId": "ff85f0ea-e59b-4c54-d91c-6b46dd84534a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data switch\n",
        "if args.mydata == 1:\n",
        "    data_path = MYDATA_PATH + 'ehr.pkl'\n",
        "    voc_path = MYDATA_PATH + 'vocabs.pkl'\n",
        "\n",
        "    ddi_adj_path = MYDATA_PATH + 'ddiadj.pkl'\n",
        "    ddi_mask_path = MYDATA_PATH + 'hmask.pkl'\n",
        "    molecule_path = MYDATA_PATH + 'atc2SMILES.pkl'\n",
        "\n",
        "    vocab = dill.load(open(voc_path, 'rb'))\n",
        "    diag_voc, pro_voc, med_voc = vocab['diag_vocab'], vocab['pro_vocab'], vocab['med_vocab']\n",
        "\n",
        "else:\n",
        "    data_path = DATA_PATH + 'records_final.pkl'\n",
        "    voc_path = DATA_PATH + 'voc_final.pkl'\n",
        "\n",
        "    ddi_adj_path = DATA_PATH + 'ddi_A_final.pkl'\n",
        "    ddi_mask_path = DATA_PATH + 'ddi_mask_H.pkl'\n",
        "    molecule_path = DATA_PATH + 'atc3toSMILES.pkl'\n",
        "    \n",
        "    voc = dill.load(open(voc_path, 'rb'))\n",
        "    diag_voc, pro_voc, med_voc = voc['diag_voc'], voc['pro_voc'], voc['med_voc']\n",
        "\n",
        "ddi_adj = dill.load(open(ddi_adj_path, 'rb'))\n",
        "ddi_mask_H = dill.load(open(ddi_mask_path, 'rb'))\n",
        "data = dill.load(open(data_path, 'rb'))\n",
        "molecule = dill.load(open(molecule_path, 'rb')) \n",
        "\n",
        "if args.smallset == 1:\n",
        "    data_train = data[:200] \n",
        "    data_test = data[200:250]\n",
        "    data_eval = data[250:300]\n",
        "else:\n",
        "    split_point = int(len(data) * 2 / 3)\n",
        "    data_train = data[:split_point]\n",
        "    eval_len = int(len(data[split_point:]) / 2)\n",
        "    data_test = data[split_point:split_point + eval_len]\n",
        "    data_eval = data[split_point+eval_len:]\n",
        "\n",
        "!pip install rdkit-pypi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVP5Ait9B7jk",
        "outputId": "b7224b0b-e8e5-4437-f2a0-63f7faacaa87"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdkit-pypi\n",
            "  Downloading rdkit_pypi-2022.3.2.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.7 MB 84.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from rdkit-pypi) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rdkit-pypi) (1.21.6)\n",
            "Installing collected packages: rdkit-pypi\n",
            "Successfully installed rdkit-pypi-2022.3.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "2WyyPqmnnDI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "#from dnc import DNC\n",
        "#from layers import GraphConvolution\n",
        "import math\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "'''\n",
        "Our model\n",
        "'''\n",
        "\n",
        "\n",
        "class MaskLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super(MaskLinear, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, input, mask):\n",
        "        weight = torch.mul(self.weight, mask)\n",
        "        output = torch.mm(input, weight)\n",
        "\n",
        "        if self.bias is not None:\n",
        "            return output + self.bias\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' \\\n",
        "               + str(self.in_features) + ' -> ' \\\n",
        "               + str(self.out_features) + ')'\n",
        "\n",
        "\n",
        "class MolecularGraphNeuralNetwork(nn.Module):\n",
        "    def __init__(self, N_fingerprint, dim, layer_hidden, device):\n",
        "        super(MolecularGraphNeuralNetwork, self).__init__()\n",
        "        self.device = device\n",
        "        self.embed_fingerprint = nn.Embedding(N_fingerprint, dim).to(self.device)\n",
        "        self.W_fingerprint = nn.ModuleList([nn.Linear(dim, dim).to(self.device)\n",
        "                                            for _ in range(layer_hidden)])\n",
        "        self.layer_hidden = layer_hidden\n",
        "\n",
        "    def pad(self, matrices, pad_value):\n",
        "        \"\"\"Pad the list of matrices\n",
        "        with a pad_value (e.g., 0) for batch proc essing.\n",
        "        For example, given a list of matrices [A, B, C],\n",
        "        we obtain a new matrix [A00, 0B0, 00C],\n",
        "        where 0 is the zero (i.e., pad value) matrix.\n",
        "        \"\"\"\n",
        "        shapes = [m.shape for m in matrices]\n",
        "        M, N = sum([s[0] for s in shapes]), sum([s[1] for s in shapes])\n",
        "        zeros = torch.FloatTensor(np.zeros((M, N))).to(self.device)\n",
        "        pad_matrices = pad_value + zeros\n",
        "        i, j = 0, 0\n",
        "        for k, matrix in enumerate(matrices):\n",
        "            m, n = shapes[k]\n",
        "            pad_matrices[i:i+m, j:j+n] = matrix\n",
        "            i += m\n",
        "            j += n\n",
        "        return pad_matrices\n",
        "\n",
        "    def update(self, matrix, vectors, layer):\n",
        "        hidden_vectors = torch.relu(self.W_fingerprint[layer](vectors))\n",
        "        return hidden_vectors + torch.mm(matrix, hidden_vectors)\n",
        "\n",
        "    def sum(self, vectors, axis):\n",
        "        sum_vectors = [torch.sum(v, 0) for v in torch.split(vectors, axis)]\n",
        "        return torch.stack(sum_vectors)\n",
        "\n",
        "    def mean(self, vectors, axis):\n",
        "        mean_vectors = [torch.mean(v, 0) for v in torch.split(vectors, axis)]\n",
        "        return torch.stack(mean_vectors)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "\n",
        "        \"\"\"Cat or pad each input data for batch processing.\"\"\"\n",
        "        fingerprints, adjacencies, molecular_sizes = inputs\n",
        "        fingerprints = torch.cat(fingerprints)\n",
        "        adjacencies = self.pad(adjacencies, 0)\n",
        "\n",
        "        \"\"\"MPNN layer (update the fingerprint vectors).\"\"\"\n",
        "        fingerprint_vectors = self.embed_fingerprint(fingerprints)\n",
        "        for l in range(self.layer_hidden):\n",
        "            hs = self.update(adjacencies, fingerprint_vectors, l)\n",
        "            # fingerprint_vectors = F.normalize(hs, 2, 1)  # normalize.\n",
        "            fingerprint_vectors = hs\n",
        "\n",
        "        \"\"\"Molecular vector by sum or mean of the fingerprint vectors.\"\"\"\n",
        "        molecular_vectors = self.sum(fingerprint_vectors, molecular_sizes)\n",
        "        # molecular_vectors = self.mean(fingerprint_vectors, molecular_sizes)\n",
        "\n",
        "        return molecular_vectors\n",
        "\n",
        "\n",
        "class SafeDrugModel(nn.Module):\n",
        "    def __init__(self, vocab_size, ddi_adj, ddi_mask_H, MPNNSet, N_fingerprints, average_projection, emb_dim=256, device=torch.device('cpu:0')):\n",
        "        super(SafeDrugModel, self).__init__()\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        # pre-embedding\n",
        "        self.embeddings = nn.ModuleList(\n",
        "            [nn.Embedding(vocab_size[i], emb_dim) for i in range(2)])\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.encoders = nn.ModuleList([nn.GRU(emb_dim, emb_dim, batch_first=True) for _ in range(2)])\n",
        "        self.query = nn.Sequential(\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(2 * emb_dim, emb_dim)\n",
        "        )\n",
        "\n",
        "        # bipartite local embedding\n",
        "        self.bipartite_transform = nn.Sequential(\n",
        "            nn.Linear(emb_dim, ddi_mask_H.shape[1])\n",
        "        )\n",
        "        self.bipartite_output = MaskLinear(ddi_mask_H.shape[1], vocab_size[2], False)\n",
        "        \n",
        "        # MPNN global embedding\n",
        "        self.MPNN_molecule_Set = list(zip(*MPNNSet))\n",
        "\n",
        "        self.MPNN_emb = MolecularGraphNeuralNetwork(N_fingerprints, emb_dim, layer_hidden=2, device=device).forward(self.MPNN_molecule_Set)\n",
        "        self.MPNN_emb = torch.mm(average_projection.to(device=self.device), self.MPNN_emb.to(device=self.device))\n",
        "        self.MPNN_emb.to(device=self.device)\n",
        "        # self.MPNN_emb = torch.tensor(self.MPNN_emb, requires_grad=True)\n",
        "        self.MPNN_output = nn.Linear(vocab_size[2], vocab_size[2])\n",
        "        self.MPNN_layernorm = nn.LayerNorm(vocab_size[2])\n",
        "        \n",
        "        # graphs, bipartite matrix\n",
        "        self.tensor_ddi_adj = torch.FloatTensor(ddi_adj).to(device)\n",
        "        self.tensor_ddi_mask_H = torch.FloatTensor(ddi_mask_H).to(device)\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "\t    # patient health representation\n",
        "        i1_seq = []\n",
        "        i2_seq = []\n",
        "        def sum_embedding(embedding):\n",
        "            return embedding.sum(dim=1).unsqueeze(dim=0)  # (1,1,dim)\n",
        "        for adm in input:\n",
        "            i1 = sum_embedding(self.dropout(self.embeddings[0](torch.LongTensor(adm[0]).unsqueeze(dim=0).to(self.device)))) # (1,1,dim)\n",
        "            i2 = sum_embedding(self.dropout(self.embeddings[1](torch.LongTensor(adm[1]).unsqueeze(dim=0).to(self.device))))\n",
        "            i1_seq.append(i1)\n",
        "            i2_seq.append(i2)\n",
        "        i1_seq = torch.cat(i1_seq, dim=1) #(1,seq,dim)\n",
        "        i2_seq = torch.cat(i2_seq, dim=1) #(1,seq,dim)\n",
        "\n",
        "        o1, h1 = self.encoders[0](\n",
        "            i1_seq\n",
        "        )\n",
        "        o2, h2 = self.encoders[1](\n",
        "            i2_seq\n",
        "        )\n",
        "        patient_representations = torch.cat([o1, o2], dim=-1).squeeze(dim=0) # (seq, dim*2)\n",
        "        query = self.query(patient_representations)[-1:, :] # (seq, dim)\n",
        "        \n",
        "\t    # MPNN embedding\n",
        "        MPNN_match = F.sigmoid(torch.mm(query, self.MPNN_emb.t()))\n",
        "        MPNN_att = self.MPNN_layernorm(MPNN_match + self.MPNN_output(MPNN_match))\n",
        "        \n",
        "\t    # local embedding\n",
        "        bipartite_emb = self.bipartite_output(F.sigmoid(self.bipartite_transform(query)), self.tensor_ddi_mask_H.t())\n",
        "        \n",
        "        result = torch.mul(bipartite_emb, MPNN_att)\n",
        "        \n",
        "        if self.training:\n",
        "            neg_pred_prob = torch.sigmoid(result)\n",
        "            neg_pred_prob = neg_pred_prob.t() * neg_pred_prob  # (voc_size, voc_size)\n",
        "            batch_neg = 0.0005 * neg_pred_prob.mul(self.tensor_ddi_adj).sum()\n",
        "\n",
        "            return result, batch_neg\n",
        "        else:\n",
        "            return result\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"Initialize weights.\"\"\"\n",
        "        initrange = 0.1\n",
        "        for item in self.embeddings:\n",
        "            item.weight.data.uniform_(-initrange, initrange)\n"
      ],
      "metadata": {
        "id": "P0v-yBPx3kGQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Util\n"
      ],
      "metadata": {
        "id": "AxQBL6kynFXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import jaccard_score, roc_auc_score, precision_score, f1_score, average_precision_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sys\n",
        "import warnings\n",
        "import dill\n",
        "from collections import Counter\n",
        "from rdkit import Chem\n",
        "from collections import defaultdict\n",
        "import torch\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def get_n_params(model):\n",
        "    pp=0\n",
        "    for p in list(model.parameters()):\n",
        "        nn=1\n",
        "        for s in list(p.size()):\n",
        "            nn = nn*s\n",
        "        pp += nn\n",
        "    return pp\n",
        "\n",
        "# use the same metric from DMNC\n",
        "def llprint(message):\n",
        "    sys.stdout.write(message)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "def transform_split(X, Y):\n",
        "    x_train, x_eval, y_train, y_eval = train_test_split(X, Y, train_size=2/3, random_state=1203)\n",
        "    x_eval, x_test, y_eval, y_test = train_test_split(x_eval, y_eval, test_size=0.5, random_state=1203)\n",
        "    return x_train, x_eval, x_test, y_train, y_eval, y_test\n",
        "\n",
        "def sequence_output_process(output_logits, filter_token):\n",
        "    pind = np.argsort(output_logits, axis=-1)[:, ::-1]\n",
        "\n",
        "    out_list = []\n",
        "    break_flag = False\n",
        "    for i in range(len(pind)):\n",
        "        if break_flag:\n",
        "            break\n",
        "        for j in range(pind.shape[1]):\n",
        "            label = pind[i][j]\n",
        "            if label in filter_token:\n",
        "                break_flag = True\n",
        "                break\n",
        "            if label not in out_list:\n",
        "                out_list.append(label)\n",
        "                break\n",
        "    y_pred_prob_tmp = []\n",
        "    for idx, item in enumerate(out_list):\n",
        "        y_pred_prob_tmp.append(output_logits[idx, item])\n",
        "    sorted_predict = [x for _, x in sorted(zip(y_pred_prob_tmp, out_list), reverse=True)]\n",
        "    return out_list, sorted_predict\n",
        "\n",
        "\n",
        "def sequence_metric(y_gt, y_pred, y_prob, y_label):\n",
        "    def average_prc(y_gt, y_label):\n",
        "        score = []\n",
        "        for b in range(y_gt.shape[0]):\n",
        "            target = np.where(y_gt[b]==1)[0]\n",
        "            out_list = y_label[b]\n",
        "            inter = set(out_list) & set(target)\n",
        "            prc_score = 0 if len(out_list) == 0 else len(inter) / len(out_list)\n",
        "            score.append(prc_score)\n",
        "        return score\n",
        "\n",
        "\n",
        "    def average_recall(y_gt, y_label):\n",
        "        score = []\n",
        "        for b in range(y_gt.shape[0]):\n",
        "            target = np.where(y_gt[b] == 1)[0]\n",
        "            out_list = y_label[b]\n",
        "            inter = set(out_list) & set(target)\n",
        "            recall_score = 0 if len(target) == 0 else len(inter) / len(target)\n",
        "            score.append(recall_score)\n",
        "        return score\n",
        "\n",
        "\n",
        "    def average_f1(average_prc, average_recall):\n",
        "        score = []\n",
        "        for idx in range(len(average_prc)):\n",
        "            if (average_prc[idx] + average_recall[idx]) == 0:\n",
        "                score.append(0)\n",
        "            else:\n",
        "                score.append(2*average_prc[idx]*average_recall[idx] / (average_prc[idx] + average_recall[idx]))\n",
        "        return score\n",
        "\n",
        "\n",
        "    def jaccard(y_gt, y_label):\n",
        "        score = []\n",
        "        for b in range(y_gt.shape[0]):\n",
        "            target = np.where(y_gt[b] == 1)[0]\n",
        "            out_list = y_label[b]\n",
        "            inter = set(out_list) & set(target)\n",
        "            union = set(out_list) | set(target)\n",
        "            jaccard_score = 0 if union == 0 else len(inter) / len(union)\n",
        "            score.append(jaccard_score)\n",
        "        return np.mean(score)\n",
        "\n",
        "    def f1(y_gt, y_pred):\n",
        "        all_micro = []\n",
        "        for b in range(y_gt.shape[0]):\n",
        "            all_micro.append(f1_score(y_gt[b], y_pred[b], average='macro'))\n",
        "        return np.mean(all_micro)\n",
        "\n",
        "    def roc_auc(y_gt, y_pred_prob):\n",
        "        all_micro = []\n",
        "        for b in range(len(y_gt)):\n",
        "            all_micro.append(roc_auc_score(y_gt[b], y_pred_prob[b], average='macro'))\n",
        "        return np.mean(all_micro)\n",
        "\n",
        "    def precision_auc(y_gt, y_prob):\n",
        "        all_micro = []\n",
        "        for b in range(len(y_gt)):\n",
        "            all_micro.append(average_precision_score(y_gt[b], y_prob[b], average='macro'))\n",
        "        return np.mean(all_micro)\n",
        "\n",
        "    def precision_at_k(y_gt, y_prob_label, k):\n",
        "        precision = 0\n",
        "        for i in range(len(y_gt)):\n",
        "            TP = 0\n",
        "            for j in y_prob_label[i][:k]:\n",
        "                if y_gt[i, j] == 1:\n",
        "                    TP += 1\n",
        "            precision += TP / k\n",
        "        return precision / len(y_gt)\n",
        "    try:\n",
        "        auc = roc_auc(y_gt, y_prob)\n",
        "    except ValueError:\n",
        "        auc = 0\n",
        "    p_1 = precision_at_k(y_gt, y_label, k=1)\n",
        "    p_3 = precision_at_k(y_gt, y_label, k=3)\n",
        "    p_5 = precision_at_k(y_gt, y_label, k=5)\n",
        "    f1 = f1(y_gt, y_pred)\n",
        "    prauc = precision_auc(y_gt, y_prob)\n",
        "    ja = jaccard(y_gt, y_label)\n",
        "    avg_prc = average_prc(y_gt, y_label)\n",
        "    avg_recall = average_recall(y_gt, y_label)\n",
        "    avg_f1 = average_f1(avg_prc, avg_recall)\n",
        "\n",
        "    return ja, prauc, np.mean(avg_prc), np.mean(avg_recall), np.mean(avg_f1)\n",
        "\n",
        "\n",
        "def multi_label_metric(y_gt, y_pred, y_prob):\n",
        "\n",
        "    def jaccard(y_gt, y_pred):\n",
        "        score = []\n",
        "        for b in range(y_gt.shape[0]):\n",
        "            target = np.where(y_gt[b] == 1)[0]\n",
        "            out_list = np.where(y_pred[b] == 1)[0]\n",
        "            inter = set(out_list) & set(target)\n",
        "            union = set(out_list) | set(target)\n",
        "            jaccard_score = 0 if union == 0 else len(inter) / len(union)\n",
        "            score.append(jaccard_score)\n",
        "        return np.mean(score)\n",
        "\n",
        "    def average_prc(y_gt, y_pred):\n",
        "        score = []\n",
        "        for b in range(y_gt.shape[0]):\n",
        "            target = np.where(y_gt[b] == 1)[0]\n",
        "            out_list = np.where(y_pred[b] == 1)[0]\n",
        "            inter = set(out_list) & set(target)\n",
        "            prc_score = 0 if len(out_list) == 0 else len(inter) / len(out_list)\n",
        "            score.append(prc_score)\n",
        "        return score\n",
        "\n",
        "    def average_recall(y_gt, y_pred):\n",
        "        score = []\n",
        "        for b in range(y_gt.shape[0]):\n",
        "            target = np.where(y_gt[b] == 1)[0]\n",
        "            out_list = np.where(y_pred[b] == 1)[0]\n",
        "            inter = set(out_list) & set(target)\n",
        "            recall_score = 0 if len(target) == 0 else len(inter) / len(target)\n",
        "            score.append(recall_score)\n",
        "        return score\n",
        "\n",
        "    def average_f1(average_prc, average_recall):\n",
        "        score = []\n",
        "        for idx in range(len(average_prc)):\n",
        "            if average_prc[idx] + average_recall[idx] == 0:\n",
        "                score.append(0)\n",
        "            else:\n",
        "                score.append(2*average_prc[idx]*average_recall[idx] / (average_prc[idx] + average_recall[idx]))\n",
        "        return score\n",
        "\n",
        "    def f1(y_gt, y_pred):\n",
        "        all_micro = []\n",
        "        for b in range(y_gt.shape[0]):\n",
        "            all_micro.append(f1_score(y_gt[b], y_pred[b], average='macro'))\n",
        "        return np.mean(all_micro)\n",
        "\n",
        "    def roc_auc(y_gt, y_prob):\n",
        "        all_micro = []\n",
        "        for b in range(len(y_gt)):\n",
        "            all_micro.append(roc_auc_score(y_gt[b], y_prob[b], average='macro'))\n",
        "        return np.mean(all_micro)\n",
        "\n",
        "    def precision_auc(y_gt, y_prob):\n",
        "        all_micro = []\n",
        "        for b in range(len(y_gt)):\n",
        "            all_micro.append(average_precision_score(y_gt[b], y_prob[b], average='macro'))\n",
        "        return np.mean(all_micro)\n",
        "\n",
        "    def precision_at_k(y_gt, y_prob, k=3):\n",
        "        precision = 0\n",
        "        sort_index = np.argsort(y_prob, axis=-1)[:, ::-1][:, :k]\n",
        "        for i in range(len(y_gt)):\n",
        "            TP = 0\n",
        "            for j in range(len(sort_index[i])):\n",
        "                if y_gt[i, sort_index[i, j]] == 1:\n",
        "                    TP += 1\n",
        "            precision += TP / len(sort_index[i])\n",
        "        return precision / len(y_gt)\n",
        "\n",
        "    # roc_auc\n",
        "    try:\n",
        "        auc = roc_auc(y_gt, y_prob)\n",
        "    except:\n",
        "        auc = 0\n",
        "    # precision\n",
        "    p_1 = precision_at_k(y_gt, y_prob, k=1)\n",
        "    p_3 = precision_at_k(y_gt, y_prob, k=3)\n",
        "    p_5 = precision_at_k(y_gt, y_prob, k=5)\n",
        "    # macro f1\n",
        "    f1 = f1(y_gt, y_pred)\n",
        "    # precision\n",
        "    prauc = precision_auc(y_gt, y_prob)\n",
        "    # jaccard\n",
        "    ja = jaccard(y_gt, y_pred)\n",
        "    # pre, recall, f1\n",
        "    avg_prc = average_prc(y_gt, y_pred)\n",
        "    avg_recall = average_recall(y_gt, y_pred)\n",
        "    avg_f1 = average_f1(avg_prc, avg_recall)\n",
        "\n",
        "    return ja, prauc, np.mean(avg_prc), np.mean(avg_recall), np.mean(avg_f1)\n",
        "\n",
        "def ddi_rate_score(record, path=ddi_adj_path): ###\n",
        "    # ddi rate\n",
        "    ddi_A = dill.load(open(path, 'rb'))\n",
        "    all_cnt = 0\n",
        "    dd_cnt = 0\n",
        "    for patient in record:\n",
        "        for adm in patient:\n",
        "            med_code_set = adm\n",
        "            for i, med_i in enumerate(med_code_set):\n",
        "                for j, med_j in enumerate(med_code_set):\n",
        "                    if j <= i:\n",
        "                        continue\n",
        "                    all_cnt += 1\n",
        "                    if ddi_A[med_i, med_j] == 1 or ddi_A[med_j, med_i] == 1:\n",
        "                        dd_cnt += 1\n",
        "    if all_cnt == 0:\n",
        "        return 0\n",
        "    return dd_cnt / all_cnt\n",
        "\n",
        "\n",
        "def create_atoms(mol, atom_dict):\n",
        "    \"\"\"Transform the atom types in a molecule (e.g., H, C, and O)\n",
        "    into the indices (e.g., H=0, C=1, and O=2).\n",
        "    Note that each atom index considers the aromaticity.\n",
        "    \"\"\"\n",
        "    atoms = [a.GetSymbol() for a in mol.GetAtoms()]\n",
        "    for a in mol.GetAromaticAtoms():\n",
        "        i = a.GetIdx()\n",
        "        atoms[i] = (atoms[i], 'aromatic')\n",
        "    atoms = [atom_dict[a] for a in atoms]\n",
        "    return np.array(atoms)\n",
        "\n",
        "def create_ijbonddict(mol, bond_dict):\n",
        "    \"\"\"Create a dictionary, in which each key is a node ID\n",
        "    and each value is the tuples of its neighboring node\n",
        "    and chemical bond (e.g., single and double) IDs.\n",
        "    \"\"\"\n",
        "    i_jbond_dict = defaultdict(lambda: [])\n",
        "    for b in mol.GetBonds():\n",
        "        i, j = b.GetBeginAtomIdx(), b.GetEndAtomIdx()\n",
        "        bond = bond_dict[str(b.GetBondType())]\n",
        "        i_jbond_dict[i].append((j, bond))\n",
        "        i_jbond_dict[j].append((i, bond))\n",
        "    return i_jbond_dict\n",
        "\n",
        "def extract_fingerprints(radius, atoms, i_jbond_dict,\n",
        "                         fingerprint_dict, edge_dict):\n",
        "    \"\"\"Extract the fingerprints from a molecular graph\n",
        "    based on Weisfeiler-Lehman algorithm.\n",
        "    \"\"\"\n",
        "\n",
        "    if (len(atoms) == 1) or (radius == 0):\n",
        "        nodes = [fingerprint_dict[a] for a in atoms]\n",
        "\n",
        "    else:\n",
        "        nodes = atoms\n",
        "        i_jedge_dict = i_jbond_dict\n",
        "\n",
        "        for _ in range(radius):\n",
        "\n",
        "            \"\"\"Update each node ID considering its neighboring nodes and edges.\n",
        "            The updated node IDs are the fingerprint IDs.\n",
        "            \"\"\"\n",
        "            nodes_ = []\n",
        "            for i, j_edge in i_jedge_dict.items():\n",
        "                neighbors = [(nodes[j], edge) for j, edge in j_edge]\n",
        "                fingerprint = (nodes[i], tuple(sorted(neighbors)))\n",
        "                nodes_.append(fingerprint_dict[fingerprint])\n",
        "\n",
        "            \"\"\"Also update each edge ID considering\n",
        "            its two nodes on both sides.\n",
        "            \"\"\"\n",
        "            i_jedge_dict_ = defaultdict(lambda: [])\n",
        "            for i, j_edge in i_jedge_dict.items():\n",
        "                for j, edge in j_edge:\n",
        "                    both_side = tuple(sorted((nodes[i], nodes[j])))\n",
        "                    edge = edge_dict[(both_side, edge)]\n",
        "                    i_jedge_dict_[i].append((j, edge))\n",
        "\n",
        "            nodes = nodes_\n",
        "            i_jedge_dict = i_jedge_dict_\n",
        "\n",
        "    return np.array(nodes)\n",
        "\n",
        "\n",
        "def buildMPNN(molecule, med_voc, radius=1, device=\"cpu:0\"):\n",
        "\n",
        "    atom_dict = defaultdict(lambda: len(atom_dict))\n",
        "    bond_dict = defaultdict(lambda: len(bond_dict))\n",
        "    fingerprint_dict = defaultdict(lambda: len(fingerprint_dict))\n",
        "    edge_dict = defaultdict(lambda: len(edge_dict))\n",
        "    MPNNSet, average_index = [], []\n",
        "\n",
        "    for index, atc3 in med_voc.items():\n",
        "\n",
        "        smilesList = list(molecule[atc3])\n",
        "        \"\"\"Create each data with the above defined functions.\"\"\"\n",
        "        counter = 0 # counter how many drugs are under that ATC-3\n",
        "        for smiles in smilesList:\n",
        "            try:\n",
        "                mol = Chem.AddHs(Chem.MolFromSmiles(smiles))\n",
        "                atoms = create_atoms(mol, atom_dict)\n",
        "                molecular_size = len(atoms)\n",
        "                i_jbond_dict = create_ijbonddict(mol, bond_dict)\n",
        "                fingerprints = extract_fingerprints(radius, atoms, i_jbond_dict,\n",
        "                                                    fingerprint_dict, edge_dict)\n",
        "                adjacency = Chem.GetAdjacencyMatrix(mol)\n",
        "                # if fingerprints.shape[0] == adjacency.shape[0]:\n",
        "                for _ in range(adjacency.shape[0] - fingerprints.shape[0]):\n",
        "                    fingerprints = np.append(fingerprints, 1)\n",
        "                \n",
        "                fingerprints = torch.LongTensor(fingerprints).to(device)\n",
        "                adjacency = torch.FloatTensor(adjacency).to(device)\n",
        "                MPNNSet.append((fingerprints, adjacency, molecular_size))\n",
        "                counter += 1\n",
        "            except:\n",
        "                continue\n",
        "        \n",
        "        average_index.append(counter)\n",
        "\n",
        "        \"\"\"Transform the above each data of numpy\n",
        "        to pytorch tensor on a device (i.e., CPU or GPU).\n",
        "        \"\"\"\n",
        "\n",
        "    N_fingerprint = len(fingerprint_dict)\n",
        "    # transform into projection matrix\n",
        "    n_col = sum(average_index)\n",
        "    n_row = len(average_index)\n",
        "\n",
        "    average_projection = np.zeros((n_row, n_col))\n",
        "    col_counter = 0\n",
        "    for i, item in enumerate(average_index):\n",
        "        if item > 0:\n",
        "            average_projection[i, col_counter : col_counter + item] = 1 / item\n",
        "        col_counter += item\n",
        "\n",
        "    return MPNNSet, N_fingerprint, torch.FloatTensor(average_projection)"
      ],
      "metadata": {
        "id": "LP4JTig5nJsB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n"
      ],
      "metadata": {
        "id": "DpxsUdqsnGQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dill\n",
        "import numpy as np\n",
        "import argparse\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import jaccard_score\n",
        "from torch.optim import Adam\n",
        "import os\n",
        "import torch\n",
        "import time\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# evaluate\n",
        "def eval(model, data_eval, voc_size, epoch):\n",
        "    model.eval()\n",
        "\n",
        "    smm_record = []\n",
        "    ja, prauc, avg_p, avg_r, avg_f1 = [[] for _ in range(5)]\n",
        "    med_cnt, visit_cnt = 0, 0\n",
        "\n",
        "    for step, input in enumerate(data_eval):\n",
        "        y_gt, y_pred, y_pred_prob, y_pred_label = [], [], [], []\n",
        "        for adm_idx, adm in enumerate(input):\n",
        "            target_output = model(input[:adm_idx+1])\n",
        "\n",
        "            y_gt_tmp = np.zeros(voc_size[2])\n",
        "            y_gt_tmp[adm[2]] = 1\n",
        "            y_gt.append(y_gt_tmp)\n",
        "\n",
        "            # prediction prod\n",
        "            target_output = F.sigmoid(target_output).detach().cpu().numpy()[0]\n",
        "            y_pred_prob.append(target_output)\n",
        "            \n",
        "            # prediction med set\n",
        "            y_pred_tmp = target_output.copy()\n",
        "            y_pred_tmp[y_pred_tmp>=0.5] = 1\n",
        "            y_pred_tmp[y_pred_tmp<0.5] = 0\n",
        "            y_pred.append(y_pred_tmp)\n",
        "\n",
        "            # prediction label\n",
        "            y_pred_label_tmp = np.where(y_pred_tmp == 1)[0]\n",
        "            y_pred_label.append(sorted(y_pred_label_tmp))\n",
        "            visit_cnt += 1\n",
        "            med_cnt += len(y_pred_label_tmp)\n",
        "\n",
        "        smm_record.append(y_pred_label)\n",
        "        adm_ja, adm_prauc, adm_avg_p, adm_avg_r, adm_avg_f1 = multi_label_metric(np.array(y_gt), np.array(y_pred), np.array(y_pred_prob))\n",
        "\n",
        "        ja.append(adm_ja)\n",
        "        prauc.append(adm_prauc)\n",
        "        avg_p.append(adm_avg_p)\n",
        "        avg_r.append(adm_avg_r)\n",
        "        avg_f1.append(adm_avg_f1)\n",
        "        llprint('\\rtest step: {} / {}'.format(step+1, len(data_eval)))\n",
        "\n",
        "    # ddi rate\n",
        "    ddi_rate = ddi_rate_score(smm_record, path=ddi_adj_path) ###\n",
        "\n",
        "    llprint('\\nDDI Rate: {:.4}, Jaccard: {:.4},  PRAUC: {:.4}, AVG_PRC: {:.4}, AVG_RECALL: {:.4}, AVG_F1: {:.4}, AVG_MED: {:.4}\\n'.format(\n",
        "        ddi_rate, np.mean(ja), np.mean(prauc), np.mean(avg_p), np.mean(avg_r), np.mean(avg_f1), med_cnt / visit_cnt\n",
        "    ))\n",
        "\n",
        "    return ddi_rate, np.mean(ja), np.mean(prauc), np.mean(avg_p), np.mean(avg_r), np.mean(avg_f1), med_cnt / visit_cnt\n",
        "\n",
        "def main():\n",
        "\n",
        "    device = torch.device('cuda:{}'.format(args.cuda))\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "\n",
        "     # out of main()\n",
        "    if args.mydata == 1:\n",
        "        MPNNSet, N_fingerprint, average_projection = buildMPNN(molecule, med_voc.index2word, 2, device)\n",
        "        voc_size = (len(diag_voc.index2word), len(pro_voc.index2word), len(med_voc.index2word))\n",
        "    else:\n",
        "        MPNNSet, N_fingerprint, average_projection = buildMPNN(molecule, med_voc.idx2word, 2, device)\n",
        "        voc_size = (len(diag_voc.idx2word), len(pro_voc.idx2word), len(med_voc.idx2word))\n",
        "\n",
        "\n",
        "    model = SafeDrugModel(voc_size, ddi_adj, ddi_mask_H, MPNNSet, N_fingerprint, average_projection, emb_dim=args.dim, device=device)\n",
        "    # model.load_state_dict(torch.load(open(args.resume_path, 'rb')))\n",
        "\n",
        "    if args.Inf_time:\n",
        "        #https://towardsdatascience.com/the-correct-way-to-measure-inference-time-of-deep-neural-networks-304a54e5187f\n",
        "        model.load_state_dict(torch.load(open(args.resume_path, 'rb')))\n",
        "        model.to(device=device)\n",
        "        tic = time.time()\n",
        "\n",
        "        starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
        "        repetitions = len(data_test)\n",
        "        timings = np.zeros((repetitions,1))\n",
        "        dummy_input = [[[13, 98, 585, 1065, 21, 37, 454, 278], [69, 47], [4, 22, 12, 2, 67, 0, 86]],\\\n",
        "                       [[377, 326, 21, 46, 454], [115, 94], [3, 6, 12, 14, 5, 22, 2, 29, 1, 16, 11, 86]],\\\n",
        "                       [[377, 246, 453, 46, 21, 454], [151, 127, 128], [14, 2, 6, 29, 18, 0, 86]], [[963, 258, 32, 93, 94, 13, 103, 571, 21], [164, 423, 424, 425, 95, 426, 361, 48, 46, 2], [5, 4, 6, 7, 9, 11, 12, 3, 13, 16, 14, 22, 1, 2, 29, 44, 45, 48, 56, 20, 76, 86]]]\n",
        "\n",
        "        #GPU-WARM-UP\n",
        "        for _ in range(10):\n",
        "            _ = model(dummy_input)\n",
        "        count = 0\n",
        "\n",
        "        # MEASURE PERFORMANCE\n",
        "        with torch.no_grad():\n",
        "            #for rep in range(repetitions):\n",
        "            for rep, example in enumerate(data_test):\n",
        "                starter.record()\n",
        "                _ = model(example)\n",
        "                ender.record()\n",
        "                # WAIT FOR GPU SYNC\n",
        "                torch.cuda.synchronize()\n",
        "                curr_time = starter.elapsed_time(ender)\n",
        "                timings[rep] = curr_time\n",
        "                count += 1\n",
        "\n",
        "        mean_syn = np.sum(timings) / repetitions\n",
        "        std_syn = np.std(timings)\n",
        "        print(f'Inference reps {count}, average: {mean_syn} \\u00B1 {std_syn} seconds')\n",
        "\n",
        "        data = np.array([mean_syn, std_syn, count])\n",
        "        df = pd.DataFrame(data, index=['mean inference time', 'stdev', 'reps'])\n",
        "        df.to_csv(TEST_PATH + 'Inf_' + args.model_name + f'{dt.datetime.now()}' + '.csv' )\n",
        "\n",
        "        return\n",
        "\n",
        "    if args.Test:\n",
        "        model.load_state_dict(torch.load(open(args.resume_path, 'rb')))\n",
        "        model.to(device=device)\n",
        "        tic = time.time()\n",
        "\n",
        "        ddi_list, ja_list, prauc_list, f1_list, med_list = [], [], [], [], []\n",
        "        # ###\n",
        "        # for threshold in np.linspace(0.00, 0.20, 30):\n",
        "        #     print ('threshold = {}'.format(threshold))\n",
        "        #     ddi, ja, prauc, _, _, f1, avg_med = eval(model, data_test, voc_size, 0, threshold)\n",
        "        #     ddi_list.append(ddi)\n",
        "        #     ja_list.append(ja)\n",
        "        #     prauc_list.append(prauc)\n",
        "        #     f1_list.append(f1)\n",
        "        #     med_list.append(avg_med)\n",
        "        # total = [ddi_list, ja_list, prauc_list, f1_list, med_list]\n",
        "        # with open('ablation_ddi.pkl', 'wb') as infile:\n",
        "        #     dill.dump(total, infile)\n",
        "        # ###\n",
        "        result = []\n",
        "        for _ in range(10):\n",
        "            time_start = time.time()\n",
        "            test_sample = np.random.choice(data_test, round(len(data_test) * 0.8), replace=True)\n",
        "            ddi_rate, ja, prauc, avg_p, avg_r, avg_f1, avg_med = eval(model, test_sample, voc_size, 0)\n",
        "            time_sample = time.time() - time_start ###\n",
        "            result.append([ddi_rate, ja, avg_f1, prauc, avg_med, time_sample])\n",
        "            \n",
        "        result = np.array(result)\n",
        "        mean = result.mean(axis=0)\n",
        "        std = result.std(axis=0)\n",
        "\n",
        "        outstring = \"\"\n",
        "        for m, s in zip(mean, std):\n",
        "            outstring += \"{:.4f} \"u\"\\u00B1\"\" {:.4f} & \".format(m, s) ###\n",
        "\n",
        "        print(outstring)\n",
        "        time_round = time.time() - tic\n",
        "        print(f'test time: {time_round}')\n",
        "        \n",
        "        elapsed_time = [0. for _ in range(5)]\n",
        "        elapsed_time.append(time_round)\n",
        "        data = np.array([mean, std, elapsed_time])\n",
        "\n",
        "        df = pd.DataFrame(data, columns=['ddi', 'ja', 'prauc', 'f1', 'med', 'time'], index=['mean', 'std', 'seconds'])\n",
        "        df.to_csv(TEST_PATH + 'Test_' + args.model_name + f'{dt.datetime.now()}' + '.csv' )\n",
        "\n",
        "        return \n",
        "\n",
        "    model.to(device=device)\n",
        "    print('parameters', get_n_params(model))\n",
        "\n",
        "    # exit()\n",
        "    optimizer = Adam(list(model.parameters()), lr=args.lr)\n",
        "\n",
        "    # start iterations\n",
        "    history = defaultdict(list)\n",
        "    best_epoch, best_ja = 0, 0\n",
        "\n",
        "    times_train, times_eval = [], [] ###\n",
        "    for epoch in range(EPOCH):\n",
        "        time_start = time.time() ###\n",
        "        print('\\nepoch {} --------------------------'.format(epoch + 1))\n",
        "        \n",
        "        model.train()\n",
        "        beta_log = [] ###\n",
        "        for step, input in enumerate(data_train): \n",
        "\n",
        "            loss = 0\n",
        "            for idx, adm in enumerate(input):\n",
        "\n",
        "                seq_input = input[:idx+1]\n",
        "                loss_bce_target = np.zeros((1, voc_size[2]))\n",
        "                loss_bce_target[:, adm[2]] = 1\n",
        "\n",
        "                loss_multi_target = np.full((1, voc_size[2]), -1)\n",
        "                for idx, item in enumerate(adm[2]):\n",
        "                    loss_multi_target[0][idx] = item\n",
        "                \n",
        "                result, loss_ddi = model(seq_input)\n",
        "\n",
        "                loss_bce = F.binary_cross_entropy_with_logits(result, torch.FloatTensor(loss_bce_target).to(device))\n",
        "                loss_multi = F.multilabel_margin_loss(F.sigmoid(result), torch.LongTensor(loss_multi_target).to(device))\n",
        "\n",
        "                result = F.sigmoid(result).detach().cpu().numpy()[0]\n",
        "                result[result >= 0.5] = 1\n",
        "                result[result < 0.5] = 0\n",
        "                y_label = np.where(result == 1)[0]\n",
        "                current_ddi_rate = ddi_rate_score([[y_label]], path=ddi_adj_path) ###\n",
        "                \n",
        "                if current_ddi_rate <= args.target_ddi:\n",
        "                    loss = 0.95 * loss_bce + 0.05 * loss_multi\n",
        "                    beta = 1\n",
        "                else:\n",
        "                    beta = min(0, 1 + (args.target_ddi - current_ddi_rate) / args.kp) ###\n",
        "                    loss = beta * (0.95 * loss_bce + 0.05 * loss_multi) + (1 - beta) * loss_ddi\n",
        "                beta_log.append(beta)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward(retain_graph=True)\n",
        "                optimizer.step()\n",
        "\n",
        "            llprint(f'\\rtraining step: {step+1} / {len(data_train)} loss: {loss} loss_ddi: {loss_ddi}  beta: {beta}') ###\n",
        "        \n",
        "        print()\n",
        "        print(f'\\navg_beta: {statistics.mean(beta_log)}') ###\n",
        "        time_end = time.time()  ###\n",
        "        ddi_rate, ja, prauc, avg_p, avg_r, avg_f1, avg_med = eval(model, data_eval, voc_size, epoch)\n",
        "        time_train = time_end - time_start ###\n",
        "        time_eval = time.time() - time_end  ###\n",
        "        print(f'training time: {time_train}, test time: {time_eval}') ###\n",
        "\n",
        "        times_train.append(time_train) ###\n",
        "        times_eval.append(time_eval) ###\n",
        "\n",
        "        history['ja'].append(ja)\n",
        "        history['ddi_rate'].append(ddi_rate)\n",
        "        history['avg_p'].append(avg_p)\n",
        "        history['avg_r'].append(avg_r)\n",
        "        history['avg_f1'].append(avg_f1)\n",
        "        history['prauc'].append(prauc)\n",
        "        history['med'].append(avg_med)\n",
        "\n",
        "        if epoch >= 5:\n",
        "            print('ddi: {}, Med: {}, Ja: {}, F1: {}, PRAUC: {}'.format(\n",
        "                np.mean(history['ddi_rate'][-5:]),\n",
        "                np.mean(history['med'][-5:]),\n",
        "                np.mean(history['ja'][-5:]),\n",
        "                np.mean(history['avg_f1'][-5:]),\n",
        "                np.mean(history['prauc'][-5:])\n",
        "                ))\n",
        "\n",
        "        torch.save(model.state_dict(), open(WORKING_PATH +''.join(('saved/', args.model_name, \\\n",
        "            'Epoch_{}_TARGET_{:.2}_JA_{:.4}_DDI_{:.4}_{}.model'.format(epoch, args.target_ddi, ja, ddi_rate, dt.datetime.now()))), 'wb')) ###\n",
        "\n",
        "        if epoch != 0 and best_ja < ja:\n",
        "            best_epoch = epoch\n",
        "            best_ja = ja\n",
        "\n",
        "        print('best_epoch: {}'.format(best_epoch))\n",
        "\n",
        "    dill.dump(history, open(WORKING_PATH +'history_{}_{}.pkl'.format(args.model_name, dt.datetime.now()), 'wb')) ###\n",
        "    \n",
        "    timings = np.array(list(zip(times_train, times_eval))) ###\n",
        "    df = pd.DataFrame(timings, columns=['train', 'test']) ###\n",
        "    df.to_csv(TEST_PATH + 'TimesTrain_' + args.model_name + f'{dt.datetime.now()}' + '.csv' ) ###\n",
        "\n",
        "    # Maximum cuda memory used\n",
        "    print(f'peak memory allocated: {torch.cuda.max_memory_allocated(device)}')\n",
        "\n"
      ],
      "metadata": {
        "id": "OVGKVCjwnKAO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execute"
      ],
      "metadata": {
        "id": "gHxU3KXtIRO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext memory_profiler\n",
        "%memit -r1 main()\n",
        "\n",
        "#if __name__ == '__main__':\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVgA61t4kA1C",
        "outputId": "c2bc4c28-72e1-47d9-e61c-d30cdaeed9f6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test step: 846 / 846\n",
            "DDI Rate: 0.1013, Jaccard: 0.1722,  PRAUC: 0.6077, AVG_PRC: 0.6505, AVG_RECALL: 0.1934, AVG_F1: 0.2894, AVG_MED: 5.516\n",
            "test step: 846 / 846\n",
            "DDI Rate: 0.1005, Jaccard: 0.1713,  PRAUC: 0.6195, AVG_PRC: 0.6622, AVG_RECALL: 0.1909, AVG_F1: 0.2888, AVG_MED: 5.51\n",
            "test step: 846 / 846\n",
            "DDI Rate: 0.1023, Jaccard: 0.1723,  PRAUC: 0.6129, AVG_PRC: 0.6516, AVG_RECALL: 0.1941, AVG_F1: 0.2893, AVG_MED: 5.51\n",
            "test step: 846 / 846\n",
            "DDI Rate: 0.1037, Jaccard: 0.1722,  PRAUC: 0.614, AVG_PRC: 0.6591, AVG_RECALL: 0.1923, AVG_F1: 0.2899, AVG_MED: 5.504\n",
            "test step: 846 / 846\n",
            "DDI Rate: 0.1011, Jaccard: 0.1735,  PRAUC: 0.616, AVG_PRC: 0.6599, AVG_RECALL: 0.1954, AVG_F1: 0.2913, AVG_MED: 5.495\n",
            "test step: 846 / 846\n",
            "DDI Rate: 0.1013, Jaccard: 0.171,  PRAUC: 0.6169, AVG_PRC: 0.6598, AVG_RECALL: 0.191, AVG_F1: 0.2881, AVG_MED: 5.509\n",
            "test step: 846 / 846\n",
            "DDI Rate: 0.103, Jaccard: 0.1734,  PRAUC: 0.6156, AVG_PRC: 0.656, AVG_RECALL: 0.1957, AVG_F1: 0.2912, AVG_MED: 5.542\n",
            "test step: 846 / 846\n",
            "DDI Rate: 0.1034, Jaccard: 0.1729,  PRAUC: 0.6159, AVG_PRC: 0.6567, AVG_RECALL: 0.194, AVG_F1: 0.2909, AVG_MED: 5.538\n",
            "test step: 846 / 846\n",
            "DDI Rate: 0.1013, Jaccard: 0.1722,  PRAUC: 0.6161, AVG_PRC: 0.6617, AVG_RECALL: 0.1923, AVG_F1: 0.29, AVG_MED: 5.52\n",
            "test step: 846 / 846\n",
            "DDI Rate: 0.1026, Jaccard: 0.1704,  PRAUC: 0.6143, AVG_PRC: 0.6535, AVG_RECALL: 0.1909, AVG_F1: 0.2872, AVG_MED: 5.526\n",
            "0.1020 ± 0.0010 & 0.1721 ± 0.0010 & 0.2896 ± 0.0013 & 0.6149 ± 0.0029 & 5.5170 ± 0.0142 & 11.0925 ± 0.2088 & \n",
            "test time: 110.92587494850159\n",
            "peak memory: 4888.44 MiB, increment: 4438.84 MiB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xvk3qsopFPRu"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}