{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "process_data_paper.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "VK5LnY46LnRs",
        "Ph1OdzyKLq3H",
        "ErOOmZKdqBmY"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/russpv/SafeDrug/blob/main/process_data_paper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preprocessing**"
      ],
      "metadata": {
        "id": "-4C5xTciLaGv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load things"
      ],
      "metadata": {
        "id": "VK5LnY46LnRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BeBF8sufjvQ",
        "outputId": "936d718d-ac81-49cf-85de-c351482dbeec"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install dnc\n",
        "!pip install rdkit-pypi"
      ],
      "metadata": {
        "id": "tQY3TIIPhOAy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ab6ae42-3910-42c8-c0ad-100ccd7c5795"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rdkit-pypi in /usr/local/lib/python3.7/dist-packages (2022.3.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rdkit-pypi) (1.21.6)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from rdkit-pypi) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn, dill\n",
        "'''\n",
        "pandas: 1.3.0\n",
        "dill: 0.3.4\n",
        "torch: 1.8.0+cu111\n",
        "rdkit: 2021.03.4\n",
        "scikit-learn: 0.24.2\n",
        "numpy: 1.21.1'''\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import csv\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import BRICS\n",
        "from collections import defaultdict\n",
        "\n",
        "# set seed\n",
        "seed = 1234\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "\n",
        "# define data path\n",
        "DATA_PATH = \"drive/MyDrive/DL4H/Project/SAFEDRUG_lib/data/\"\n",
        "WORKING_PATH = \"drive/MyDrive/DL4H/Project/PaperCode/processed_orig/\""
      ],
      "metadata": {
        "id": "I6nzUao3fr8-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_medications_file = DATA_PATH + 'PRESCRIPTIONS.csv'\n",
        "data_diagnoses_file = DATA_PATH + 'DIAGNOSES_ICD.csv'\n",
        "data_procedures_file = DATA_PATH + 'PROCEDURES_ICD.csv'  \n",
        "\n",
        "map_CID_RXCUI_file = DATA_PATH + 'ndc2rxnorm_mapping.txt'\n",
        "map_RXCUI_atc4_file = DATA_PATH + 'ndc2atc_level4.csv'\n",
        "map_CID_ATC6_file = DATA_PATH + 'drug-atc.csv'\n",
        "\n",
        "# processed output\n",
        "data_processed_ehr_file = WORKING_PATH + 'records_final.pkl'\n",
        "vocabs_file  = WORKING_PATH + 'voc_final.pkl'\n",
        "matrix_ddi_graph_file = WORKING_PATH + 'ddi_A_final.pkl'\n",
        "matrix_ehr_graph_file = WORKING_PATH + 'ehr_adj_final.pkl' # no need for this\n",
        "map_ATC_SMILES_file = WORKING_PATH +  'atc3toSMILES.pkl'\n",
        "matrix_h_mask_file = WORKING_PATH + 'ddi_mask_H.pkl'"
      ],
      "metadata": {
        "id": "jwp57YUpozvL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section Folder"
      ],
      "metadata": {
        "id": "goj6qeTCp-L_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##### process medications #####\n",
        "# load med data\n",
        "def med_process(med_file):\n",
        "    med_pd = pd.read_csv(med_file, dtype={'NDC':'category'})\n",
        "\n",
        "    # med_pd.drop(columns=['ROW_ID','DRUG_TYPE','DRUG_NAME_POE','DRUG_NAME_GENERIC',\n",
        "    #                     'FORMULARY_DRUG_CD','PROD_STRENGTH','DOSE_VAL_RX',\n",
        "    #                     'DOSE_UNIT_RX','FORM_VAL_DISP','FORM_UNIT_DISP', 'GSN', 'FORM_UNIT_DISP',\n",
        "    #                     'ROUTE','ENDDATE','DRUG'], axis=1, inplace=True)\n",
        "    med_pd.drop(columns=['ROW_ID','DRUG_TYPE', 'DRUG_NAME_POE', 'DRUG_NAME_GENERIC',\n",
        "                        'FORMULARY_DRUG_CD','PROD_STRENGTH','DOSE_VAL_RX',\n",
        "                        'DOSE_UNIT_RX','FORM_VAL_DISP','FORM_UNIT_DISP', 'GSN', 'FORM_UNIT_DISP',\n",
        "                        'ROUTE','ENDDATE'], axis=1, inplace=True)\n",
        "    med_pd.drop(index = med_pd[med_pd['NDC'] == '0'].index, axis=0, inplace=True)\n",
        "    med_pd.fillna(method='pad', inplace=True)\n",
        "    med_pd.dropna(inplace=True)\n",
        "    med_pd.drop_duplicates(inplace=True)\n",
        "    med_pd['ICUSTAY_ID'] = med_pd['ICUSTAY_ID'].astype('int64')\n",
        "    med_pd['STARTDATE'] = pd.to_datetime(med_pd['STARTDATE'], format='%Y-%m-%d %H:%M:%S')    \n",
        "    med_pd.sort_values(by=['SUBJECT_ID', 'HADM_ID', 'ICUSTAY_ID', 'STARTDATE'], inplace=True)\n",
        "    med_pd = med_pd.reset_index(drop=True)\n",
        "\n",
        "    med_pd = med_pd.drop(columns=['ICUSTAY_ID'])\n",
        "    med_pd = med_pd.drop_duplicates()\n",
        "    med_pd = med_pd.reset_index(drop=True)\n",
        "\n",
        "    return med_pd\n",
        "\n",
        "# ATC3-to-drugname\n",
        "def ATC3toDrug(med_pd):\n",
        "    atc3toDrugDict = {}\n",
        "    for atc3, drugname in med_pd[['ATC3', 'DRUG']].values:\n",
        "        if atc3 in atc3toDrugDict:\n",
        "            atc3toDrugDict[atc3].add(drugname)\n",
        "        else:\n",
        "            atc3toDrugDict[atc3] = set(drugname)\n",
        "\n",
        "    return atc3toDrugDict\n",
        "\n",
        "def F_atc3toSMILES(ATC3toDrugDict, druginfo):\n",
        "    drug2smiles = {}\n",
        "    atc3tosmiles = {}\n",
        "    for drugname, smiles in druginfo[['name', 'moldb_smiles']].values:\n",
        "        if type(smiles) == type('a'):\n",
        "            drug2smiles[drugname] = smiles\n",
        "    for atc3, drug in ATC3toDrugDict.items():\n",
        "        temp = []\n",
        "        for d in drug:\n",
        "            try:\n",
        "                temp.append(drug2smiles[d])\n",
        "            except:\n",
        "                pass\n",
        "        if len(temp) > 0:\n",
        "            atc3tosmiles[atc3] = temp[:3]\n",
        "    \n",
        "    return atc3tosmiles\n",
        "\n",
        "# medication mapping\n",
        "def codeMapping2atc4(med_pd):\n",
        "    with open(rxnorm2RXCUI_file, 'r') as f:\n",
        "        rxnorm2RXCUI = eval(f.read())\n",
        "    med_pd['RXCUI'] = med_pd['NDC'].map(rxnorm2RXCUI)\n",
        "    med_pd.dropna(inplace=True)\n",
        "\n",
        "    rxnorm2atc4 = pd.read_csv(RXCUI2atc4_file)\n",
        "    rxnorm2atc4 = rxnorm2atc4.drop(columns=['YEAR','MONTH','NDC'])\n",
        "    rxnorm2atc4.drop_duplicates(subset=['RXCUI'], inplace=True)\n",
        "    med_pd.drop(index = med_pd[med_pd['RXCUI'].isin([''])].index, axis=0, inplace=True)\n",
        "    \n",
        "    med_pd['RXCUI'] = med_pd['RXCUI'].astype('int64')\n",
        "    med_pd = med_pd.reset_index(drop=True)\n",
        "    med_pd = med_pd.merge(rxnorm2atc4, on=['RXCUI'])\n",
        "    med_pd.drop(columns=['NDC', 'RXCUI'], inplace=True)\n",
        "    med_pd['ATC4'] = med_pd['ATC4'].map(lambda x: x[:4])\n",
        "    med_pd = med_pd.rename(columns={'ATC4':'ATC3'})\n",
        "    med_pd = med_pd.drop_duplicates()    \n",
        "    med_pd = med_pd.reset_index(drop=True)\n",
        "    return med_pd\n",
        "\n",
        "# visit >= 2\n",
        "def process_visit_lg2(med_pd):\n",
        "    a = med_pd[['SUBJECT_ID', 'HADM_ID']].groupby(by='SUBJECT_ID')['HADM_ID'].unique().reset_index()\n",
        "    a['HADM_ID_Len'] = a['HADM_ID'].map(lambda x:len(x))\n",
        "    a = a[a['HADM_ID_Len'] > 1]\n",
        "    return a \n",
        "\n",
        "# most common medications\n",
        "def filter_300_most_med(med_pd):\n",
        "    med_count = med_pd.groupby(by=['ATC3']).size().reset_index().rename(columns={0:'count'}).sort_values(by=['count'],ascending=False).reset_index(drop=True)\n",
        "    med_pd = med_pd[med_pd['ATC3'].isin(med_count.loc[:299, 'ATC3'])]\n",
        "    \n",
        "    return med_pd.reset_index(drop=True)\n",
        "\n",
        "##### process diagnosis #####\n",
        "def diag_process(diag_file):\n",
        "    diag_pd = pd.read_csv(diag_file)\n",
        "    diag_pd.dropna(inplace=True)\n",
        "    diag_pd.drop(columns=['SEQ_NUM','ROW_ID'],inplace=True)\n",
        "    diag_pd.drop_duplicates(inplace=True)\n",
        "    diag_pd.sort_values(by=['SUBJECT_ID','HADM_ID'], inplace=True)\n",
        "    diag_pd = diag_pd.reset_index(drop=True)\n",
        "\n",
        "    def filter_2000_most_diag(diag_pd):\n",
        "        diag_count = diag_pd.groupby(by=['ICD9_CODE']).size().reset_index().rename(columns={0:'count'}).sort_values(by=['count'],ascending=False).reset_index(drop=True)\n",
        "        diag_pd = diag_pd[diag_pd['ICD9_CODE'].isin(diag_count.loc[:1999, 'ICD9_CODE'])]\n",
        "        \n",
        "        return diag_pd.reset_index(drop=True)\n",
        "\n",
        "    diag_pd = filter_2000_most_diag(diag_pd)\n",
        "\n",
        "    return diag_pd\n",
        "\n",
        "##### process procedure #####\n",
        "def procedure_process(procedure_file):\n",
        "    pro_pd = pd.read_csv(procedure_file, dtype={'ICD9_CODE':'category'})\n",
        "    pro_pd.drop(columns=['ROW_ID'], inplace=True)\n",
        "    pro_pd.drop_duplicates(inplace=True)\n",
        "    pro_pd.sort_values(by=['SUBJECT_ID', 'HADM_ID', 'SEQ_NUM'], inplace=True)\n",
        "    pro_pd.drop(columns=['SEQ_NUM'], inplace=True)\n",
        "    pro_pd.drop_duplicates(inplace=True)\n",
        "    pro_pd.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    return pro_pd\n",
        "\n",
        "def filter_1000_most_pro(pro_pd):\n",
        "    pro_count = pro_pd.groupby(by=['ICD9_CODE']).size().reset_index().rename(columns={0:'count'}).sort_values(by=['count'],ascending=False).reset_index(drop=True)\n",
        "    pro_pd = pro_pd[pro_pd['ICD9_CODE'].isin(pro_count.loc[:1000, 'ICD9_CODE'])]\n",
        "    \n",
        "    return pro_pd.reset_index(drop=True) \n",
        "\n",
        "###### combine three tables #####\n",
        "def combine_process(med_pd, diag_pd, pro_pd):\n",
        "\n",
        "    med_pd_key = med_pd[['SUBJECT_ID', 'HADM_ID']].drop_duplicates()\n",
        "    diag_pd_key = diag_pd[['SUBJECT_ID', 'HADM_ID']].drop_duplicates()\n",
        "    pro_pd_key = pro_pd[['SUBJECT_ID', 'HADM_ID']].drop_duplicates()\n",
        "\n",
        "    combined_key = med_pd_key.merge(diag_pd_key, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
        "    combined_key = combined_key.merge(pro_pd_key, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
        "\n",
        "    diag_pd = diag_pd.merge(combined_key, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
        "    med_pd = med_pd.merge(combined_key, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
        "    pro_pd = pro_pd.merge(combined_key, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
        "\n",
        "    # flatten and merge\n",
        "    diag_pd = diag_pd.groupby(by=['SUBJECT_ID','HADM_ID'])['ICD9_CODE'].unique().reset_index()  \n",
        "    med_pd = med_pd.groupby(by=['SUBJECT_ID', 'HADM_ID'])['ATC3'].unique().reset_index()\n",
        "    pro_pd = pro_pd.groupby(by=['SUBJECT_ID','HADM_ID'])['ICD9_CODE'].unique().reset_index().rename(columns={'ICD9_CODE':'PRO_CODE'})  \n",
        "    med_pd['ATC3'] = med_pd['ATC3'].map(lambda x: list(x))\n",
        "    pro_pd['PRO_CODE'] = pro_pd['PRO_CODE'].map(lambda x: list(x))\n",
        "    data = diag_pd.merge(med_pd, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
        "    data = data.merge(pro_pd, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
        "    #     data['ICD9_CODE_Len'] = data['ICD9_CODE'].map(lambda x: len(x))\n",
        "    data['ATC3_num'] = data['ATC3'].map(lambda x: len(x))\n",
        "\n",
        "    return data\n",
        "\n",
        "def statistics(data):\n",
        "    print('#patients ', data['SUBJECT_ID'].unique().shape)\n",
        "    print('#clinical events ', len(data))\n",
        "    \n",
        "    diag = data['ICD9_CODE'].values\n",
        "    med = data['ATC3'].values\n",
        "    pro = data['PRO_CODE'].values\n",
        "    \n",
        "    unique_diag = set([j for i in diag for j in list(i)])\n",
        "    unique_med = set([j for i in med for j in list(i)])\n",
        "    unique_pro = set([j for i in pro for j in list(i)])\n",
        "    \n",
        "    print('#diagnosis ', len(unique_diag))\n",
        "    print('#med ', len(unique_med))\n",
        "    print('#procedure', len(unique_pro))\n",
        "    \n",
        "    avg_diag, avg_med, avg_pro, max_diag, max_med, max_pro, cnt, max_visit, avg_visit = [0 for i in range(9)]\n",
        "\n",
        "    for subject_id in data['SUBJECT_ID'].unique():\n",
        "        item_data = data[data['SUBJECT_ID'] == subject_id]\n",
        "        x, y, z = [], [], []\n",
        "        visit_cnt = 0\n",
        "        for index, row in item_data.iterrows():\n",
        "            visit_cnt += 1\n",
        "            cnt += 1\n",
        "            x.extend(list(row['ICD9_CODE']))\n",
        "            y.extend(list(row['ATC3']))\n",
        "            z.extend(list(row['PRO_CODE']))\n",
        "        x, y, z = set(x), set(y), set(z)\n",
        "        avg_diag += len(x)\n",
        "        avg_med += len(y)\n",
        "        avg_pro += len(z)\n",
        "        avg_visit += visit_cnt\n",
        "        if len(x) > max_diag:\n",
        "            max_diag = len(x)\n",
        "        if len(y) > max_med:\n",
        "            max_med = len(y) \n",
        "        if len(z) > max_pro:\n",
        "            max_pro = len(z)\n",
        "        if visit_cnt > max_visit:\n",
        "            max_visit = visit_cnt\n",
        "        \n",
        "    print('#avg of diagnoses ', avg_diag/ cnt)\n",
        "    print('#avg of medicines ', avg_med/ cnt)\n",
        "    print('#avg of procedures ', avg_pro/ cnt)\n",
        "    print('#avg of vists ', avg_visit/ len(data['SUBJECT_ID'].unique()))\n",
        "    \n",
        "    print('#max of diagnoses ', max_diag)\n",
        "    print('#max of medicines ', max_med)\n",
        "    print('#max of procedures ', max_pro)\n",
        "    print('#max of visit ', max_visit)\n",
        "\n",
        "##### indexing file and final record\n",
        "class Voc(object):\n",
        "    def __init__(self):\n",
        "        self.idx2word = {}\n",
        "        self.word2idx = {}\n",
        "\n",
        "    def add_sentence(self, sentence):\n",
        "        for word in sentence:\n",
        "            if word not in self.word2idx:\n",
        "                self.idx2word[len(self.word2idx)] = word\n",
        "                self.word2idx[word] = len(self.word2idx)\n",
        "                \n",
        "# create voc set\n",
        "def create_str_token_mapping(df):\n",
        "    diag_voc = Voc()\n",
        "    med_voc = Voc()\n",
        "    pro_voc = Voc()\n",
        "    \n",
        "    for index, row in df.iterrows():\n",
        "        diag_voc.add_sentence(row['ICD9_CODE'])\n",
        "        med_voc.add_sentence(row['ATC3'])\n",
        "        pro_voc.add_sentence(row['PRO_CODE'])\n",
        "    \n",
        "    dill.dump(obj={'diag_voc':diag_voc, 'med_voc':med_voc ,'pro_voc':pro_voc}, file=open(vocabulary_file,'wb'))\n",
        "    return diag_voc, med_voc, pro_voc\n",
        "\n",
        "# create final records\n",
        "def create_patient_record(df, diag_voc, med_voc, pro_voc):\n",
        "    records = [] # (patient, code_kind:3, codes)  code_kind:diag, proc, med\n",
        "    for subject_id in df['SUBJECT_ID'].unique():\n",
        "        item_df = df[df['SUBJECT_ID'] == subject_id]\n",
        "        patient = []\n",
        "        for index, row in item_df.iterrows():\n",
        "            admission = []\n",
        "            admission.append([diag_voc.word2idx[i] for i in row['ICD9_CODE']])\n",
        "            admission.append([pro_voc.word2idx[i] for i in row['PRO_CODE']])\n",
        "            admission.append([med_voc.word2idx[i] for i in row['ATC3']])\n",
        "            patient.append(admission)\n",
        "        records.append(patient) \n",
        "    dill.dump(obj=records, file=open(ehr_sequence_file, 'wb'))\n",
        "    return records\n",
        "        \n",
        "# get ddi matrix\n",
        "def get_ddi_matrix(records, med_voc, ddi_file):\n",
        "\n",
        "    TOPK = 40 # topk drug-drug interaction\n",
        "    cid2atc_dic = defaultdict(set)\n",
        "    med_voc_size = len(med_voc.idx2word)\n",
        "    med_unique_word = [med_voc.idx2word[i] for i in range(med_voc_size)]\n",
        "    atc3_atc4_dic = defaultdict(set)\n",
        "    for item in med_unique_word:\n",
        "        atc3_atc4_dic[item[:4]].add(item)\n",
        "    \n",
        "    with open(cid2atc6_file, 'r') as f:\n",
        "        for line in f:\n",
        "            line_ls = line[:-1].split(',')\n",
        "            cid = line_ls[0]\n",
        "            atcs = line_ls[1:]\n",
        "            for atc in atcs:\n",
        "                if len(atc3_atc4_dic[atc[:4]]) != 0:\n",
        "                    cid2atc_dic[cid].add(atc[:4])\n",
        "            \n",
        "    # ddi load\n",
        "    ddi_df = pd.read_csv(ddi_file)\n",
        "    # fliter sever side effect \n",
        "    ddi_most_pd = ddi_df.groupby(by=['Polypharmacy Side Effect', 'Side Effect Name'])\\\n",
        "        .size().reset_index().rename(columns={0:'count'}).sort_values(by=['count'],ascending=False).reset_index(drop=True)\n",
        "    ddi_most_pd = ddi_most_pd.iloc[-TOPK:,:] #orig\n",
        "    #ddi_most_pd = ddi_most_pd.iloc[:TOPK,:] #correct\n",
        "    # ddi_most_pd = pd.DataFrame(columns=['Side Effect Name'], data=['as','asd','as'])\n",
        "    fliter_ddi_df = ddi_df.merge(ddi_most_pd[['Side Effect Name']], how='inner', on=['Side Effect Name'])\n",
        "    ddi_df = fliter_ddi_df[['STITCH 1','STITCH 2']].drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "    # weighted ehr adj \n",
        "    ehr_adj = np.zeros((med_voc_size, med_voc_size))\n",
        "    for patient in records:\n",
        "        for adm in patient:\n",
        "            med_set = adm[2]\n",
        "            for i, med_i in enumerate(med_set):\n",
        "                for j, med_j in enumerate(med_set):\n",
        "                    if j<=i:\n",
        "                        continue\n",
        "                    ehr_adj[med_i, med_j] = 1\n",
        "                    ehr_adj[med_j, med_i] = 1\n",
        "    dill.dump(ehr_adj, open(ehr_adjacency_file, 'wb'))  \n",
        "\n",
        "    # ddi adj\n",
        "    ddi_adj = np.zeros((med_voc_size,med_voc_size)) \n",
        "\n",
        "    for index, row in ddi_df.iterrows():\n",
        "        # ddi\n",
        "        cid1 = row['STITCH 1']\n",
        "        cid2 = row['STITCH 2']\n",
        "        \n",
        "        # cid -> atc_level3\n",
        "        for atc_i in cid2atc_dic[cid1]:\n",
        "            for atc_j in cid2atc_dic[cid2]:\n",
        "                \n",
        "                # atc_level3 -> atc_level4\n",
        "                for i in atc3_atc4_dic[atc_i]:\n",
        "                    for j in atc3_atc4_dic[atc_j]:\n",
        "                        if med_voc.word2idx[i] != med_voc.word2idx[j]:\n",
        "                            ddi_adj[med_voc.word2idx[i], med_voc.word2idx[j]] = 1\n",
        "                            ddi_adj[med_voc.word2idx[j], med_voc.word2idx[i]] = 1\n",
        "    dill.dump(ddi_adj, open(ddi_adjacency_file, 'wb')) \n",
        "    print(ddi_adj.sum())\n",
        "    print(f'shape: {ddi_adj.shape}')\n",
        "    return ddi_adj\n",
        "\n",
        "def get_ddi_mask(atc42SMLES, med_voc):\n",
        "\n",
        "    # ATC3_List[22] = {0}\n",
        "    # ATC3_List[25] = {0}\n",
        "    # ATC3_List[27] = {0}\n",
        "    fraction = []\n",
        "    for k, v in med_voc.idx2word.items():\n",
        "        tempF = set()\n",
        "        for SMILES in atc42SMLES[v]:\n",
        "            try:\n",
        "                m = BRICS.BRICSDecompose(Chem.MolFromSmiles(SMILES))\n",
        "                for frac in m:\n",
        "                    tempF.add(frac)\n",
        "            except:\n",
        "                pass\n",
        "        fraction.append(tempF)\n",
        "    fracSet = []\n",
        "    for i in fraction:\n",
        "        fracSet += i\n",
        "    fracSet = list(set(fracSet)) # set of all segments\n",
        "\n",
        "    ddi_matrix = np.zeros((len(med_voc.idx2word), len(fracSet)))\n",
        "    for i, fracList in enumerate(fraction):\n",
        "        for frac in fracList:\n",
        "            ddi_matrix[i, fracSet.index(frac)] = 1\n",
        "    return ddi_matrix\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "co3EKLeLppgr"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MAIN"
      ],
      "metadata": {
        "id": "_lWLue3SqBhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    # files can be downloaded from https://mimic.physionet.org/gettingstarted/dbsetup/\n",
        "    # please change into your own MIMIC folder\n",
        "    med_file = data_medications_file\n",
        "    diag_file = data_diagnoses_file\n",
        "    procedure_file = data_procedures_file\n",
        "\n",
        "    # input auxiliary files\n",
        "    med_structure_file = map_ATC_SMILES_file\n",
        "    RXCUI2atc4_file = map_RXCUI_atc4_file\n",
        "    cid2atc6_file = map_CID_ATC6_file\n",
        "\n",
        "    rxnorm2RXCUI_file = DATA_PATH + 'ndc2rxnorm_mapping.txt' # this is updated from README\n",
        "    ddi_file = DATA_PATH + 'drug-DDI.csv'\n",
        "    drugbankinfo = DATA_PATH + 'drugbank_drugs_info.csv'\n",
        "\n",
        "    # output files\n",
        "    ddi_adjacency_file = matrix_ddi_graph_file\n",
        "    ehr_adjacency_file = matrix_ehr_graph_file\n",
        "    ehr_sequence_file = data_processed_ehr_file\n",
        "    vocabulary_file = vocabs_file\n",
        "    ddi_mask_H_file = matrix_h_mask_file\n",
        "    atc3toSMILES_file = map_ATC_SMILES_file"
      ],
      "metadata": {
        "id": "HGQ0241XqIpc"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    # for med\n",
        "    med_pd = med_process(med_file)\n",
        "    med_pd_lg2 = process_visit_lg2(med_pd).reset_index(drop=True)    \n",
        "    med_pd = med_pd.merge(med_pd_lg2[['SUBJECT_ID']], on='SUBJECT_ID', how='inner').reset_index(drop=True) \n",
        "\n",
        "    med_pd = codeMapping2atc4(med_pd) # rxnorm file, RXCUI file\n",
        "    med_pd = filter_300_most_med(med_pd) \n",
        "\n",
        "    # med to SMILES mapping\n",
        "    atc3toDrug = ATC3toDrug(med_pd) # a dict\n",
        "    druginfo = pd.read_csv(drugbankinfo)\n",
        "    atc3toSMILES = F_atc3toSMILES(atc3toDrug, druginfo)\n",
        "    dill.dump(atc3toSMILES, open(atc3toSMILES_file,'wb'))\n",
        "    med_pd = med_pd[med_pd.ATC3.isin(atc3toSMILES.keys())]\n",
        "    print ('complete medication processing')\n",
        "\n",
        "    # for diagnosis\n",
        "    diag_pd = diag_process(diag_file)\n",
        "\n",
        "    print ('complete diagnosis processing')\n",
        "\n",
        "    # for procedure\n",
        "    pro_pd = procedure_process(procedure_file)\n",
        "    # pro_pd = filter_1000_most_pro(pro_pd)\n",
        "\n",
        "    print ('complete procedure processing')\n",
        "\n",
        "    # combine\n",
        "    data = combine_process(med_pd, diag_pd, pro_pd)\n",
        "    statistics(data)\n",
        "    print ('complete combining')\n",
        "\n",
        "    # create vocab\n",
        "    diag_voc, med_voc, pro_voc = create_str_token_mapping(data)\n",
        "    print (\"obtain voc\")\n",
        "\n",
        "    # create ehr sequence data\n",
        "    records = create_patient_record(data, diag_voc, med_voc, pro_voc)\n",
        "    print (\"obtain ehr sequence data\")\n",
        "\n",
        "    # create ddi adj matrix\n",
        "    ddi_adj = get_ddi_matrix(records, med_voc, ddi_file)\n",
        "    print (\"obtain ddi adj matrix\")\n",
        "\n",
        "    # get ddi_mask_H\n",
        "    ddi_mask_H = get_ddi_mask(atc3toSMILES, med_voc)\n",
        "    dill.dump(ddi_mask_H, open(ddi_mask_H_file, 'wb'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dOxLW85p4Sw",
        "outputId": "eebb5cf6-223c-445d-df29-6542c142efd2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:26: DtypeWarning: Columns (11) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: DtypeWarning: Columns (30) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "complete medication processing\n",
            "complete diagnosis processing\n",
            "complete procedure processing\n",
            "#patients  (6350,)\n",
            "#clinical events  15032\n",
            "#diagnosis  1958\n",
            "#med  112\n",
            "#procedure 1430\n",
            "#avg of diagnoses  10.5089143161256\n",
            "#avg of medicines  11.647751463544438\n",
            "#avg of procedures  3.8436668440659925\n",
            "#avg of vists  2.367244094488189\n",
            "#max of diagnoses  128\n",
            "#max of medicines  64\n",
            "#max of procedures  50\n",
            "#max of visit  29\n",
            "complete combining\n",
            "obtain voc\n",
            "obtain ehr sequence data\n",
            "674.0\n",
            "shape: (112, 112)\n",
            "obtain ddi adj matrix\n"
          ]
        }
      ]
    }
  ]
}