{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LR.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "nnv03EdDWZcd",
        "KA_wq7IUWbe2",
        "_iBuv-JIjKnZ",
        "gCGRfehUg99X"
      ],
      "authorship_tag": "ABX9TyM6P0ZWjOh/8oUhWkzySIeB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/russpv/SafeDrug/blob/main/LR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "nnv03EdDWZcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')\n",
        "\n",
        "! pip install memory_profiler"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jo58nV7ML0C9",
        "outputId": "19398944-66f8-409d-e0d8-b982991347c8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not connected to a GPU\n",
            "Your runtime has 13.6 gigabytes of available RAM\n",
            "\n",
            "Not using a high-RAM runtime\n",
            "Collecting memory_profiler\n",
            "  Downloading memory_profiler-0.60.0.tar.gz (38 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from memory_profiler) (5.4.8)\n",
            "Building wheels for collected packages: memory-profiler\n",
            "  Building wheel for memory-profiler (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for memory-profiler: filename=memory_profiler-0.60.0-py3-none-any.whl size=31284 sha256=7479175c62cdfd7c61818e87b16726a7350e7374af34e3efc78f7edcd3b32323\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/2b/fb/326e30d638c538e69a5eb0aa47f4223d979f502bbdb403950f\n",
            "Successfully built memory-profiler\n",
            "Installing collected packages: memory-profiler\n",
            "Successfully installed memory-profiler-0.60.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Args"
      ],
      "metadata": {
        "id": "KA_wq7IUWbe2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "V1e-f-vxfwa5"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "def arg_parser():\n",
        "    \"\"\" Parse command line arguments\n",
        "\n",
        "    Outputs:\n",
        "        arguments {object} -- object containing command line arguments\n",
        "    \"\"\"\n",
        "\n",
        "    # Initializer\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # Add arguments here\n",
        "    parser.add_argument('--Test', action='store_true', default=False, help=\"test mode\")\n",
        "    parser.add_argument('--model_name', type=str, default='none', help=\"model name\")\n",
        "    parser.add_argument('--resume_path', type=str, default='none', help='resume path')\n",
        "    parser.add_argument('--lr', type=float, default=5e-4, help='learning rate')\n",
        "    parser.add_argument('--target_ddi', type=float, default=0.06, help=\"target ddi\")\n",
        "    parser.add_argument('--dropout', type=float, default=0.5, help=\"dropout for embeddings\")\n",
        "    parser.add_argument('--cuda', type=int, default=0, help='which cuda') ###\n",
        "\n",
        "    parser.add_argument('--smalldata', type=int, default=1, help='debug data set') ###\n",
        "    parser.add_argument('--mydata', type=int, default=1, help='paper code') ###\n",
        "    parser.add_argument('--Inf_time', type=int, default=0, help='inference time test') ###\n",
        " \n",
        "    # Parse and return arguments\n",
        "    return(parser.parse_args(args=[]))\n",
        "\n",
        "args = arg_parser()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install memory_profiler\n",
        "%load_ext memory_profiler"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOFAI4_2QQDn",
        "outputId": "98b4b859-315c-40d9-a0d9-d6070436664f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: memory_profiler in /usr/local/lib/python3.7/dist-packages (0.60.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from memory_profiler) (5.4.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import dill\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys\n",
        "import time\n",
        "import statistics\n",
        "import datetime as dt\n",
        "import logging\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from collections import defaultdict\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import jaccard_score\n",
        "\n",
        "# set seed\n",
        "seed = 1203 #1203\n",
        "random.seed(seed)\n",
        "\n",
        "# define data path\n",
        "DATA_PATH = \"drive/MyDrive/DL4H/Project/PaperCode/processed_orig/\"\n",
        "MYDATA_PATH = \"drive/MyDrive/DL4H/Project/SAFEDRUG_lib/data/processed/\"\n",
        "WORKING_PATH = \"drive/MyDrive/DL4H/Project/LR/\"\n",
        "TEST_PATH = \"drive/MyDrive/DL4H/Project/LR/results/\"\n",
        "\n",
        "# define dataset\n",
        "args.mydata = 0\n",
        "args.smalldata = 0\n",
        "\n",
        "# setting\n",
        "args.model_name = 'LR'\n",
        "# args.resume_path = WORKING_PATH + 'saved/' + ''\n",
        "\n",
        "logger = logging.getLogger('')\n",
        "logger.setLevel(logging.CRITICAL)"
      ],
      "metadata": {
        "id": "gNHaUJ7RgA-I"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Setup"
      ],
      "metadata": {
        "id": "_iBuv-JIjKnZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70h9Tt1YjGz5",
        "outputId": "8769db3f-3a92-4dfd-ae28-35524a74d1a5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data switch\n",
        "if args.mydata == 1:\n",
        "    data_path = MYDATA_PATH + 'ehr.pkl'\n",
        "    voc_path = MYDATA_PATH + 'vocabs.pkl'\n",
        "\n",
        "    # ehr_adj_path = MYDATA_PATH + 'ehradj.pkl'\n",
        "    ddi_adj_path = MYDATA_PATH + 'ddiadj.pkl'\n",
        "    # ddi_mask_path = MYDATA_PATH + 'hmask.pkl'\n",
        "    # molecule_path = MYDATA_PATH + 'atc2SMILES.pkl'\n",
        "\n",
        "    voc = dill.load(open(voc_path, 'rb'))\n",
        "    diag_voc, pro_voc, med_voc = voc['diag_vocab'].index2word, voc['pro_vocab'].index2word, voc['med_vocab'].index2word\n",
        "\n",
        "else:\n",
        "    data_path = DATA_PATH + 'records_final.pkl'\n",
        "    voc_path = DATA_PATH + 'voc_final.pkl'\n",
        "\n",
        "\n",
        "    # ehr_adj_path = DATA_PATH + 'ehr_adj_final.pkl'\n",
        "    ddi_adj_path = DATA_PATH + 'ddi_A_final.pkl'\n",
        "    # ddi_mask_path = DATA_PATH + 'ddi_mask_H.pkl'\n",
        "    # molecule_path = DATA_PATH + 'atc3toSMILES.pkl'\n",
        "    \n",
        "    voc = dill.load(open(voc_path, 'rb'))\n",
        "    diag_voc, pro_voc, med_voc = voc['diag_voc'].idx2word, voc['pro_voc'].idx2word, voc['med_voc'].idx2word\n",
        "\n",
        "# ehr_adj = dill.load(open(ehr_adj_path, 'rb'))\n",
        "# ddi_adj = dill.load(open(ddi_adj_path, 'rb'))\n",
        "# ddi_mask_H = dill.load(open(ddi_mask_path, 'rb'))\n",
        "data = dill.load(open(data_path, 'rb'))\n",
        "# molecule = dill.load(open(molecule_path, 'rb')) \n",
        "\n",
        "if args.smalldata == 1:\n",
        "    data_train = data[:200] \n",
        "    data_test = data[200:250]\n",
        "    data_eval = data[250:300]\n",
        "else:\n",
        "    split_point = int(len(data) * 2 / 3)\n",
        "    data_train = data[:split_point]\n",
        "    eval_len = int(len(data[split_point:]) / 2)\n",
        "    data_test = data[split_point:split_point + eval_len]\n",
        "    data_eval = data[split_point+eval_len:]\n"
      ],
      "metadata": {
        "id": "kYdMNrqNjJJu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# concatenated diagnosis and procedure codes into multi-hot np.arrays\n",
        "def process_data_LR(data, diag_voc, pro_voc, med_voc):\n",
        "    X, y = [], []\n",
        "    for patient in data:\n",
        "        for visit in patient:\n",
        "            multi_hot_input = np.zeros(len(diag_voc) + len(pro_voc))\n",
        "            multi_hot_input[visit[0]] = 1\n",
        "            multi_hot_input[np.array(visit[1]) + len(diag_voc)] = 1  #access proc indices with offset\n",
        "\n",
        "            multi_hot_output = np.zeros(len(med_voc))\n",
        "            multi_hot_output[visit[2]] = 1\n",
        "\n",
        "            X.append(multi_hot_input)\n",
        "            y.append(multi_hot_output)\n",
        "\n",
        "    return np.array(X), np.array(y)"
      ],
      "metadata": {
        "id": "rUnCXGRtou7D"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "no4BySAQgb7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import jaccard_score, roc_auc_score, precision_score, f1_score, average_precision_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def llprint(message):\n",
        "    sys.stdout.write(message)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "def ddi_rate_score_LR(preds, path=ddi_adj_path): ###\n",
        "    # check intersection of predicted meds against ddi matrix\n",
        "    ddi_A = dill.load(open(path, 'rb'))\n",
        "    all_cnt, dd_cnt, med_cnt, visit_cnt = 0, 0, 0, 0\n",
        "    for visit_meds in preds:\n",
        "        visit_cnt += 1\n",
        "        med_code_set = np.where(visit_meds==1)[0]\n",
        "        med_cnt += len(med_code_set)\n",
        "        for i, med_i in enumerate(med_code_set):\n",
        "            for j, med_j in enumerate(med_code_set):\n",
        "                if j <= i:\n",
        "                    continue\n",
        "                all_cnt += 1\n",
        "                if ddi_A[med_i, med_j] == 1 or ddi_A[med_j, med_i] == 1:\n",
        "                    dd_cnt += 1\n",
        "    logger.warning(f'visits: {visit_cnt}  meds: {med_cnt}')\n",
        "    return 0. if all_cnt == 0 else dd_cnt / all_cnt, 0. if visit_cnt == 0 else med_cnt / visit_cnt\n",
        "\n",
        "def multi_label_metric(y_gt, y_pred, y_prob):\n",
        "\n",
        "    def jaccard(y_gt, y_pred):\n",
        "        score = []\n",
        "        for b in range(y_gt.shape[0]):\n",
        "            target = np.where(y_gt[b] == 1)[0]\n",
        "            out_list = np.where(y_pred[b] == 1)[0]\n",
        "            inter = set(out_list) & set(target)\n",
        "            union = set(out_list) | set(target)\n",
        "            jaccard_score = 0 if union == 0 else len(inter) / len(union)\n",
        "            score.append(jaccard_score)\n",
        "        return np.mean(score)\n",
        "\n",
        "    def average_prc(y_gt, y_pred):\n",
        "        score = []\n",
        "        for b in range(y_gt.shape[0]):\n",
        "            target = np.where(y_gt[b] == 1)[0]\n",
        "            out_list = np.where(y_pred[b] == 1)[0]\n",
        "            inter = set(out_list) & set(target)\n",
        "            prc_score = 0 if len(out_list) == 0 else len(inter) / len(out_list)\n",
        "            score.append(prc_score)\n",
        "        return score\n",
        "\n",
        "    def average_recall(y_gt, y_pred):\n",
        "        score = []\n",
        "        for b in range(y_gt.shape[0]):\n",
        "            target = np.where(y_gt[b] == 1)[0]\n",
        "            out_list = np.where(y_pred[b] == 1)[0]\n",
        "            inter = set(out_list) & set(target)\n",
        "            recall_score = 0 if len(target) == 0 else len(inter) / len(target)\n",
        "            score.append(recall_score)\n",
        "        return score\n",
        "\n",
        "    def average_f1(average_prc, average_recall):\n",
        "        score = []\n",
        "        for idx in range(len(average_prc)):\n",
        "            if average_prc[idx] + average_recall[idx] == 0:\n",
        "                score.append(0)\n",
        "            else:\n",
        "                score.append(2*average_prc[idx]*average_recall[idx] / (average_prc[idx] + average_recall[idx]))\n",
        "        return score\n",
        "\n",
        "    def f1(y_gt, y_pred):\n",
        "        all_micro = []\n",
        "        for b in range(y_gt.shape[0]):\n",
        "            all_micro.append(f1_score(y_gt[b], y_pred[b], average='macro'))\n",
        "        return np.mean(all_micro)\n",
        "\n",
        "    def roc_auc(y_gt, y_prob):\n",
        "        all_micro = []\n",
        "        for b in range(len(y_gt)):\n",
        "            all_micro.append(roc_auc_score(y_gt[b], y_prob[b], average='macro'))\n",
        "        return np.mean(all_micro)\n",
        "\n",
        "    def precision_auc(y_gt, y_prob):\n",
        "        all_micro = []\n",
        "        for b in range(len(y_gt)):\n",
        "            all_micro.append(average_precision_score(y_gt[b], y_prob[b], average='macro'))\n",
        "        return np.mean(all_micro)\n",
        "\n",
        "    def precision_at_k(y_gt, y_prob, k=3):\n",
        "        precision = 0\n",
        "        sort_index = np.argsort(y_prob, axis=-1)[:, ::-1][:, :k]\n",
        "        for i in range(len(y_gt)):\n",
        "            TP = 0\n",
        "            for j in range(len(sort_index[i])):\n",
        "                if y_gt[i, sort_index[i, j]] == 1:\n",
        "                    TP += 1\n",
        "            precision += TP / len(sort_index[i])\n",
        "        return precision / len(y_gt)\n",
        "\n",
        "    # roc_auc\n",
        "    try:\n",
        "        auc = roc_auc(y_gt, y_prob)\n",
        "    except:\n",
        "        auc = 0\n",
        "    # precision\n",
        "    p_1 = precision_at_k(y_gt, y_prob, k=1)\n",
        "    p_3 = precision_at_k(y_gt, y_prob, k=3)\n",
        "    p_5 = precision_at_k(y_gt, y_prob, k=5)\n",
        "    # macro f1\n",
        "    f1 = f1(y_gt, y_pred)\n",
        "    # precision\n",
        "    prauc = precision_auc(y_gt, y_prob)\n",
        "    # jaccard\n",
        "    ja = jaccard(y_gt, y_pred)\n",
        "    # pre, recall, f1\n",
        "    avg_prc = average_prc(y_gt, y_pred)\n",
        "    avg_recall = average_recall(y_gt, y_pred)\n",
        "    avg_f1 = average_f1(avg_prc, avg_recall)\n",
        "\n",
        "    return ja, prauc, np.mean(avg_prc), np.mean(avg_recall), np.mean(avg_f1)"
      ],
      "metadata": {
        "id": "GLuX6zKigbgQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "gCGRfehUg99X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    voc_size = (len(diag_voc), len(pro_voc), len(med_voc))\n",
        "    model = LogisticRegression()\n",
        "    csfr = OneVsRestClassifier(model)\n",
        "\n",
        "    history = defaultdict(list)\n",
        "    times_train, times_eval = [], [] ###\n",
        "\n",
        "    for epoch in range(1):\n",
        "        # data processing\n",
        "        X_train, y_train = process_data_LR(data_train, diag_voc, pro_voc, med_voc)\n",
        "        X_test, y_test = process_data_LR(data_test, diag_voc, pro_voc, med_voc)\n",
        "        X_eval, y_eval = process_data_LR(data_eval, diag_voc, pro_voc, med_voc)\n",
        "\n",
        "        time_start = time.time() \n",
        "        csfr.fit(X_train, y_train)\n",
        "        time_end = time.time()  \n",
        "        time_fit = time_end - time_start \n",
        "        print(f'fitting time: {time_fit}') ###\n",
        "\n",
        "        #Test inference\n",
        "        result =[]\n",
        "        for rnd in range(10): \n",
        "            # take a sample of 80% of total test visits\n",
        "            rng = np.random.default_rng()\n",
        "            test_set_indices = rng.choice(a=np.arange(start=0, stop=len(X_test)), size=round(len(X_test) * 0.8), replace=True)\n",
        "            X_set = X_test[test_set_indices]\n",
        "            y_labels = y_test[test_set_indices]\n",
        "            time_start = time.time()\n",
        "            y_preds = csfr.predict(X_set)\n",
        "            logger.debug(f'preds: {y_preds}')\n",
        "            time_pred = time.time() - time_start\n",
        "            print(f'round {rnd} prediction time: {time_pred}')\n",
        "\n",
        "            y_probs = csfr.predict_proba(X_set)\n",
        "\n",
        "            ja, prauc, avg_p, avg_r, avg_f1  = multi_label_metric(y_labels, y_preds, y_probs)\n",
        "            ddi_rate, avg_med = ddi_rate_score_LR(y_preds)\n",
        "            result.append([ddi_rate, ja, avg_f1, prauc, avg_med])\n",
        "            times_eval.append(time_pred)\n",
        "\n",
        "        result = np.array(result)\n",
        "        mean = result.mean(axis=0)\n",
        "        std = result.std(axis=0)\n",
        "\n",
        "        outstring = \"\"\n",
        "        for m, s in zip(mean, std):\n",
        "            outstring += \"  & {:.4f} \"u\"\\u00B1\"\" {:.4f}\".format(m, s) ###\n",
        "        print(outstring)\n",
        "\n",
        "        print('Epoch: {}, DDI Rate: {:.4}, Jaccard: {:.4}, PRAUC: {:.4}, AVG_PRC: {:.4}, AVG_RECALL: {:.4}, AVG_F1: {:.4}, AVG_MED: {:.4}\\n'.format(\n",
        "            epoch, ddi_rate, ja, prauc, avg_p, avg_r, avg_f1, avg_med\n",
        "            ))\n",
        "\n",
        "        times_train.append(time_fit) ###\n",
        "\n",
        "        history['ja'].append(ja)\n",
        "        history['avg_p'].append(avg_p)\n",
        "        history['avg_r'].append(avg_r)\n",
        "        history['avg_f1'].append(avg_f1)\n",
        "        history['prauc'].append(prauc)\n",
        "        history['med'].append(avg_med)\n",
        "\n",
        "        datadump = np.array([mean, std])\n",
        "        df = pd.DataFrame(datadump, columns=['ddi', 'ja', 'prauc', 'f1', 'med'], index=['mean', 'std'])\n",
        "        df.to_csv(TEST_PATH + 'Test_' + args.model_name + f'{dt.datetime.now()}' + '.csv' )\n",
        "\n",
        "    dill.dump(history, open(WORKING_PATH +'history_{}_{}.pkl'.format(args.model_name, dt.datetime.now()), 'wb')) ###\n",
        "\n",
        "    timings = np.array(list(zip(times_train, times_eval))) ###\n",
        "    df = pd.DataFrame(timings, columns=['train', 'test']) ###\n",
        "    df.to_csv(TEST_PATH + 'TimesTrain_' + args.model_name + f'{dt.datetime.now()}' + '.csv' ) ###"
      ],
      "metadata": {
        "id": "dq0YRHIrg9TC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execute"
      ],
      "metadata": {
        "id": "NI9OjHdJhBR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    %reload_ext memory_profiler\n",
        "    %memit -r1 main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UkaHPZ81q3y",
        "outputId": "7540052a-bfd4-42d3-e704-b311acf828f7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fitting time: 459.54309821128845\n",
            "round 0 prediction time: 1.0115156173706055\n",
            "round 1 prediction time: 1.121532917022705\n",
            "round 2 prediction time: 1.0610957145690918\n",
            "round 3 prediction time: 1.0308449268341064\n",
            "round 4 prediction time: 1.035505771636963\n",
            "round 5 prediction time: 1.038905382156372\n",
            "round 6 prediction time: 1.0420901775360107\n",
            "round 7 prediction time: 1.0627961158752441\n",
            "round 8 prediction time: 1.0496745109558105\n",
            "round 9 prediction time: 1.0802490711212158\n",
            "  & 0.0776 ± 0.0012  & 0.4892 ± 0.0033  & 0.6480 ± 0.0030  & 0.7576 ± 0.0028  & 17.1273 ± 0.1544\n",
            "Epoch: 0, DDI Rate: 0.07743, Jaccard: 0.4895, PRAUC: 0.7561, AVG_PRC: 0.7276, AVG_RECALL: 0.6068, AVG_F1: 0.6484, AVG_MED: 17.16\n",
            "\n",
            "peak memory: 909.55 MiB, increment: 712.00 MiB\n"
          ]
        }
      ]
    }
  ]
}