{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "process_data.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "VK5LnY46LnRs",
        "Ph1OdzyKLq3H",
        "ErOOmZKdqBmY"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/russpv/SafeDrug/blob/main/process_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preprocessing**"
      ],
      "metadata": {
        "id": "-4C5xTciLaGv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load things"
      ],
      "metadata": {
        "id": "VK5LnY46LnRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BeBF8sufjvQ",
        "outputId": "2e17e314-5736-478d-b5f6-402feeb34dd5"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install dnc\n",
        "!pip install rdkit-pypi"
      ],
      "metadata": {
        "id": "tQY3TIIPhOAy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0a54660-c749-4a22-eecb-43eadf692713"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rdkit-pypi in /usr/local/lib/python3.7/dist-packages (2022.3.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rdkit-pypi) (1.21.6)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from rdkit-pypi) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn, dill\n",
        "'''\n",
        "pandas: 1.3.0\n",
        "dill: 0.3.4\n",
        "torch: 1.8.0+cu111\n",
        "rdkit: 2021.03.4\n",
        "scikit-learn: 0.24.2\n",
        "numpy: 1.21.1'''\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import csv\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import BRICS\n",
        "from collections import defaultdict\n",
        "\n",
        "# set seed\n",
        "seed = 1234\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "\n",
        "# define data path\n",
        "DATA_PATH = \"drive/MyDrive/DL4H/Project/SAFEDRUG_lib/data/\"\n",
        "WORKING_PATH = \"drive/MyDrive/DL4H/Project/SAFEDRUG/\""
      ],
      "metadata": {
        "id": "I6nzUao3fr8-"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process data"
      ],
      "metadata": {
        "id": "Ph1OdzyKLq3H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process MIMIC"
      ],
      "metadata": {
        "id": "QB3mbkj7zTrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocabulary(object):\n",
        "  # Want to process the MIMIC sets, map code formats, and also token index the codes\n",
        "    def __init__(self, tape=None):\n",
        "        self.word2index = {}\n",
        "        self.index2word = {}\n",
        "        self.vocab = sorted(set(tape))\n",
        "        self.size = len(self.vocab)\n",
        "        for index, word in enumerate(self.vocab):\n",
        "            self.word2index[word] = index\n",
        "            self.index2word[index] = word\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "          return self.index2word[index]\n",
        "\n",
        "    def addword(self, word):\n",
        "        self.word2index[word] = len(self.word2index)\n",
        "        self.index2word[len(self.word2index)] = word\n",
        "\n",
        "    def printStats(self):\n",
        "        print('Num. words:', len(self.word2index))"
      ],
      "metadata": {
        "id": "jaMlHLjzY8qr"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge the 3 MIMIC tables on SUBJECT_ID and HADM_ID "
      ],
      "metadata": {
        "id": "E1o3hchgLtxv"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_prescriptions(file, filtertopn_drug=None, min_visits=None, rxnorm_RXCUI=None, RXCUI_ATC=None):\n",
        "    '''\n",
        "    1 forward fill missing values\n",
        "    2 remove patients with only one visit\n",
        "    3 take topN prescribed drugs\n",
        "    4 sort RECENT LAST\n",
        "    5 transform drug CID into ATC3\n",
        "    '''\n",
        "    df = pd.read_csv(file, parse_dates=['STARTDATE', 'ENDDATE'], \n",
        "                                infer_datetime_format=True, dtype={'NDC': \"category\", \n",
        "                                                              'ICUSTAY_ID': \"object\",\n",
        "                                                              'HADM_ID': \"int64\",\n",
        "                                                              'SUBJECT_ID': \"int64\"} )\\\n",
        "        [['SUBJECT_ID','HADM_ID', 'ICUSTAY_ID','STARTDATE', 'ENDDATE', 'DRUG', 'NDC']]\n",
        "    df = df[df.NDC != '0']  # filter out the zero drug code\n",
        "    df.sort_values(by=['SUBJECT_ID','HADM_ID', 'ICUSTAY_ID','STARTDATE','ENDDATE'], inplace=True, ascending=True)\n",
        "    df.drop(columns=['ENDDATE', 'ICUSTAY_ID'], inplace=True) \n",
        "    df.reset_index(inplace=True, drop=True)\n",
        "\n",
        "    df.fillna(method='ffill', inplace=True) \n",
        "    df.dropna(inplace=True) \n",
        "    df.drop_duplicates(inplace=True)\n",
        "    df.reset_index(inplace=True, drop=True)\n",
        "\n",
        "    print(f'base prescriptions df shape: {df.shape}')\n",
        "\n",
        "    if filtertopn_drug is not None:\n",
        "        topn_drugs = df.groupby(['NDC'], as_index=False).agg(\\\n",
        "                      count_col=pd.NamedAgg(column=\"NDC\", aggfunc=\"count\"))\\\n",
        "                      .nlargest(filtertopn_drug, 'count_col')\n",
        "        df = pd.merge(left=df,\n",
        "              right=topn_drugs, \n",
        "              on ='NDC', \n",
        "              how ='inner').drop(columns=['count_col']).reset_index(drop=True)\n",
        "        print(f'topn prescriptions df shape: {df.shape}')\n",
        "\n",
        "    if min_visits is not None:\n",
        "        admissions = df.groupby(['SUBJECT_ID', 'HADM_ID'], as_index=False).count()\n",
        "        visit_counts = admissions.groupby(['SUBJECT_ID'], as_index=False).agg(\\\n",
        "                      visit_count=pd.NamedAgg(column=\"SUBJECT_ID\", aggfunc=\"count\"))\n",
        "        filtered_patients = visit_counts[visit_counts['visit_count'] > 1]\n",
        "        df = pd.merge(left=df,\n",
        "              right=filtered_patients, \n",
        "              on ='SUBJECT_ID', \n",
        "              how ='inner').drop(columns=['visit_count']).reset_index(drop=True)\n",
        "        print(f'min_visits prescriptions df shape: {df.shape}')\n",
        "\n",
        "    if rxnorm_RXCUI is not None:\n",
        "        with open(rxnorm_RXCUI, 'r') as f:\n",
        "            s = f.read()\n",
        "            table = ast.literal_eval(s)\n",
        "        df['RXCUI'] = df['NDC'].map(table)\n",
        "        df = df[df.RXCUI != ''] # about 2thou get dropped\n",
        "        df['RXCUI'] = df['RXCUI'].astype('int64')\n",
        "        print(f'rxnorm prescriptions df shape: {df.shape}')\n",
        "        \n",
        "    if RXCUI_ATC is not None:\n",
        "        map = pd.read_csv(map_RXCUI_atc4_file, parse_dates={'date': ['YEAR', 'MONTH']},\\\n",
        "                          dtype={'RXCUI': 'int64'})\n",
        "        map.sort_values(by=['RXCUI','date'], inplace=True, ascending=True) #318257 \n",
        "        map.drop(columns=\"NDC\", inplace=True)\n",
        "        map.dropna(inplace=True)\n",
        "        map.drop_duplicates(subset='RXCUI', keep='last', inplace=True) #11478 \n",
        "        \n",
        "        df = pd.merge(df, map, on='RXCUI').drop(columns=['date']).reset_index(drop=True) # about 200thou get dropped\n",
        "        myarray = df['ATC4'].to_numpy().astype('U4')\n",
        "        df['ATC4'] = myarray\n",
        "        df = df.rename(columns={'ATC4':'ATC3'})\n",
        "        df.drop_duplicates(inplace=True)    \n",
        "        df.reset_index(drop=True, inplace=True)\n",
        "        print(f'RXCUI_ATC prescriptions df shape: {df.shape}')\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "ekBFO3zJ-wkz"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_MIMIC_file(file, filtertopn_ICD9=None):\n",
        "    '''\n",
        "    1 take topN ICD9s\n",
        "    2 remove duplicates and na values\n",
        "    3 sort RECENT LAST\n",
        "    '''\n",
        "    df = pd.read_csv(file, dtype={'ICD9_CODE': 'object',\n",
        "                                            'SUBJECT_ID': 'int64',\n",
        "                                            'HADM_ID': 'int64'} )\\\n",
        "        [['SUBJECT_ID','HADM_ID', 'SEQ_NUM', 'ICD9_CODE']] \n",
        "    df.dropna(inplace=True)\n",
        "    df = df[df.ICD9_CODE != '0']\n",
        "    df.sort_values(by=['SUBJECT_ID','HADM_ID', 'SEQ_NUM', 'ICD9_CODE'], inplace=True, ascending=True)\n",
        "    df.drop(columns=['SEQ_NUM'], inplace=True)\n",
        "    df.drop_duplicates(inplace=True)\n",
        "    df.reset_index(inplace=True, drop=True)\n",
        "\n",
        "    if filtertopn_ICD9 is not None:\n",
        "        topn_ICD9_df = df.groupby(['ICD9_CODE'], as_index=False).agg(\\\n",
        "                      count_col=pd.NamedAgg(column=\"ICD9_CODE\", aggfunc=\"count\"))\\\n",
        "                      .nlargest(filtertopn_ICD9, 'count_col')\n",
        "        df = pd.merge(left=df,\n",
        "              right=topn_ICD9_df, \n",
        "              on ='ICD9_CODE', \n",
        "              how ='inner').drop(columns=['count_col']).reset_index(drop=True)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "odwKZD-O9Y49"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mutualfilter(medications_df, diagnoses_df, procedures_df):\n",
        "    \n",
        "    # inner join 3 times to get keys\n",
        "    medications_keys = medications_df[['SUBJECT_ID','HADM_ID']].drop_duplicates() #17329\n",
        "    diagnoses_keys = diagnoses_df[['SUBJECT_ID','HADM_ID']].drop_duplicates() #58857\n",
        "    procedures_keys = procedures_df[['SUBJECT_ID','HADM_ID']].drop_duplicates() #52125\n",
        "\n",
        "    mutual_keys = pd.merge(medications_keys, diagnoses_keys, how='inner') \n",
        "    mutual_keys = pd.merge(mutual_keys, procedures_keys, how='inner') #14975 \n",
        "    \n",
        "    # filter tables by the mutual keys\n",
        "    medications_df = medications_df.merge(mutual_keys, how='inner')\n",
        "    diagnoses_df = diagnoses_df.merge(mutual_keys, how='inner') \n",
        "    procedures_df = procedures_df.merge(mutual_keys, how='inner') \n",
        "    print(f'mutual filtered df shapes - med: {medications_df.shape} diag: {diagnoses_df.shape} proc: {procedures_df.shape}')\n",
        "\n",
        "    # groupby mutual key and combine codes into list\n",
        "    medications_textcombine = medications_df.groupby(by=['SUBJECT_ID','HADM_ID'])['ATC3'].unique().reset_index()\n",
        "    diagnoses_textcombine = diagnoses_df.groupby(by=['SUBJECT_ID','HADM_ID'])['ICD9_CODE'].unique().reset_index()\n",
        "    procedures_textcombine = procedures_df.groupby(by=['SUBJECT_ID','HADM_ID'])['ICD9_CODE'].unique().reset_index()\n",
        "    print(f'groupedby df shapes - med: {medications_textcombine.shape} diag: {diagnoses_textcombine.shape} proc: {procedures_textcombine.shape}')\n",
        "\n",
        "    #assert len(medications_textcombine) == len(procedures_textcombine) == len(diagnoses_textcombine) == 14975\n",
        "    return medications_textcombine, diagnoses_textcombine, procedures_textcombine, mutual_keys, medications_df, diagnoses_df, procedures_df"
      ],
      "metadata": {
        "id": "tsK-f8n9pwGQ"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_flattable(med_df, diag_df, proc_df):\n",
        "  \n",
        "    alldata = med_df.merge(diag_df, how='inner')\n",
        "    proc_df.rename(columns={\"ICD9_CODE\": \"PROD_ICD9_CODE\"}, inplace=True)\n",
        "    alldata = alldata.merge(proc_df, on=['SUBJECT_ID','HADM_ID'], how='inner')\n",
        "\n",
        "    return alldata"
      ],
      "metadata": {
        "id": "2yDGc1sLP59R"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    import ast\n",
        "\n",
        "    data_medications_file = DATA_PATH + 'PRESCRIPTIONS.csv'\n",
        "    data_diagnoses_file = DATA_PATH + 'DIAGNOSES_ICD.csv'\n",
        "    data_procedures_file = DATA_PATH + 'PROCEDURES_ICD.csv'  \n",
        "\n",
        "    map_CID_RXCUI_file = DATA_PATH + 'ndc2rxnorm_mapping.txt'\n",
        "    map_RXCUI_atc4_file = DATA_PATH + 'ndc2atc_level4.csv'\n",
        "\n",
        "    # processed output\n",
        "    data_processed_ehr_file = DATA_PATH + 'processed/ehr.pkl'\n",
        "    vocabs_file  = DATA_PATH + 'processed/vocabs.pkl'\n",
        "    matrix_ddi_graph_file = DATA_PATH + 'processed/ddiadj.pkl'\n",
        "    matrix_ehr_graph_file = DATA_PATH + 'processed/ehradj.pkl' # for GAMENet\n",
        "    map_ATC_SMILES_file = DATA_PATH +  'processed/atc2SMILES.pkl'\n",
        "    matrix_h_mask_file = DATA_PATH + 'processed/hmask.pkl'\n",
        "\n",
        "    # do initial ETL\n",
        "    medications_df = process_prescriptions(data_medications_file, filtertopn_drug=300, min_visits=2, rxnorm_RXCUI=map_CID_RXCUI_file, RXCUI_ATC=map_RXCUI_atc4_file)\n",
        "    diagnoses_df = process_MIMIC_file(data_diagnoses_file, 2000)\n",
        "    procedures_df = process_MIMIC_file(data_procedures_file)\n",
        "    '''\n",
        "    assert len(medications_df) == 647333 # reduce 2mil\n",
        "    assert len(diagnoses_df) == 625434 # reduce a few 10thou\n",
        "    assert len(procedures_df) == 228679 # reduce a few thou\n",
        "    '''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0D4p1Sf-xt-",
        "outputId": "7a8afaed-5d02-4013-9aac-fe14f380973b"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: DtypeWarning: Columns (11) have mixed types.Specify dtype option on import or set low_memory=False.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "base prescriptions df shape: (2970017, 5)\n",
            "topn prescriptions df shape: (2207681, 5)\n",
            "min_visits prescriptions df shape: (830116, 5)\n",
            "rxnorm prescriptions df shape: (828676, 6)\n",
            "RXCUI_ATC prescriptions df shape: (647333, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_statistics(data):\n",
        "    print('#patients ', data['SUBJECT_ID'].unique().shape)\n",
        "    print('#clinical events ', len(data))\n",
        "    \n",
        "    diag = data['ICD9_CODE'].values\n",
        "    med = data['ATC3'].values\n",
        "    pro = data['PROD_ICD9_CODE'].values\n",
        "    \n",
        "    unique_diag = set([j for i in diag for j in list(i)])\n",
        "    unique_med = set([j for i in med for j in list(i)])\n",
        "    unique_pro = set([j for i in pro for j in list(i)])\n",
        "    \n",
        "    print('#diagnosis ', len(unique_diag))\n",
        "    print('#med ', len(unique_med))\n",
        "    print('#procedure', len(unique_pro))\n",
        "    \n",
        "    avg_diag, avg_med, avg_pro, max_diag, max_med, max_pro, cnt, max_visit, avg_visit = [0 for i in range(9)]\n",
        "\n",
        "    for subject_id in data['SUBJECT_ID'].unique():\n",
        "        item_data = data[data['SUBJECT_ID'] == subject_id]\n",
        "        x, y, z = [], [], []\n",
        "        visit_cnt = 0\n",
        "        for index, row in item_data.iterrows():\n",
        "            visit_cnt += 1\n",
        "            cnt += 1\n",
        "            x.extend(list(row['ICD9_CODE']))\n",
        "            y.extend(list(row['ATC3']))\n",
        "            z.extend(list(row['PROD_ICD9_CODE']))\n",
        "        x, y, z = set(x), set(y), set(z)\n",
        "        avg_diag += len(x)\n",
        "        avg_med += len(y)\n",
        "        avg_pro += len(z)\n",
        "        avg_visit += visit_cnt\n",
        "        if len(x) > max_diag:\n",
        "            max_diag = len(x)\n",
        "        if len(y) > max_med:\n",
        "            max_med = len(y) \n",
        "        if len(z) > max_pro:\n",
        "            max_pro = len(z)\n",
        "        if visit_cnt > max_visit:\n",
        "            max_visit = visit_cnt\n",
        "        \n",
        "    print('#avg of diagnoses ', avg_diag/ cnt)\n",
        "    print('#avg of medicines ', avg_med/ cnt)\n",
        "    print('#avg of procedures ', avg_pro/ cnt)\n",
        "    print('#avg of visits ', avg_visit/ len(data['SUBJECT_ID'].unique()))\n",
        "    \n",
        "    print('#max of diagnoses ', max_diag)\n",
        "    print('#max of medicines ', max_med)\n",
        "    print('#max of procedures ', max_pro)\n",
        "    print('#max of visit ', max_visit)"
      ],
      "metadata": {
        "id": "PYpLFOQWBrIO"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Mutually filter all three MIMIC tables for 100% overlap, combine into one table.k,\n",
        "    filtered_meds, filtered_diags, filtered_procs, mutual_keys, med_df, diag_df, proc_df = get_mutualfilter(medications_df, diagnoses_df, procedures_df)\n",
        "    alldata = make_flattable(filtered_meds, filtered_diags, filtered_procs)\n",
        "\n",
        "    # send the post-filter columns to the vocab creator\n",
        "    med_vocab = Vocabulary(tape=med_df['ATC3'])\n",
        "    diagnoses_vocab = Vocabulary(tape=diag_df['ICD9_CODE'])\n",
        "    procedures_vocab = Vocabulary(tape=proc_df['ICD9_CODE'])\n",
        "    patient_vocab = Vocabulary(tape=mutual_keys['SUBJECT_ID'])\n",
        "    hospadm_vocab = Vocabulary(tape=mutual_keys['HADM_ID'])\n",
        "\n",
        "    # convert flat table codes to index\n",
        "    alldata['ATC3'] = [[med_vocab.word2index[token] for token in row] for row in alldata.ATC3]\n",
        "    alldata['ICD9_CODE'] = [[diagnoses_vocab.word2index[token] for token in row] for row in alldata.ICD9_CODE]\n",
        "    alldata['PROD_ICD9_CODE'] = [[procedures_vocab.word2index[token] for token in row] for row in alldata.PROD_ICD9_CODE]\n",
        "    alldata['SUBJECT_ID'] = alldata.SUBJECT_ID.map(patient_vocab.word2index)\n",
        "    alldata['HADM_ID'] = alldata.HADM_ID.map(hospadm_vocab.word2index)\n",
        "\n",
        "    print(f'Unique drugs: {med_vocab.size}')\n",
        "    print(f'Unique diagnoses: {diagnoses_vocab.size}')\n",
        "    print(f'Unique procedures: {procedures_vocab.size}')\n",
        "    print(f'Unique patients: {patient_vocab.size}')\n",
        "    print(f'Unique admissions: {hospadm_vocab.size}')\n",
        "\n",
        "    # combine series into series of series (diag, proc, med)\n",
        "    alldata['value'] = alldata[['ATC3', 'ICD9_CODE', 'PROD_ICD9_CODE']].apply(lambda x: [x[1], x[2], x[0]], axis=1)\n",
        "\n",
        "    # Rollup into data object and serialize\n",
        "    grouped_data = alldata.groupby(by=['SUBJECT_ID']).agg({\"value\": lambda x: list(x) })\n",
        "    nested_list = grouped_data.to_numpy().tolist() \n",
        "    finallist = [i[0] for i in nested_list] # (patient, code_kind:3, codes)  code_kind:diag, proc, med  (cancel xtra [])\n",
        "\n",
        "    assert finallist[0][0][0] == grouped_data.iloc[0][0][0][0] == alldata[alldata.SUBJECT_ID == 0]['value'][0][0]\n",
        "\n",
        "    print(f'Total records: {len(finallist)}')\n",
        "\n",
        "    dill.dump(obj=finallist, file=open(data_processed_ehr_file, 'wb'))\n",
        "    dill.dump(obj={'diag_vocab':diagnoses_vocab, 'med_vocab':med_vocab ,'pro_vocab':procedures_vocab}, file=open(vocabs_file,'wb'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpKLBtz2yBFw",
        "outputId": "1ac6c527-109d-4285-8135-147a2324ad43"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mutual filtered df shapes - med: (601390, 7) diag: (204802, 3) proc: (68114, 3)\n",
            "groupedby df shapes - med: (15008, 3) diag: (15008, 3) proc: (15008, 3)\n",
            "Unique drugs: 57\n",
            "Unique diagnoses: 1957\n",
            "Unique procedures: 1425\n",
            "Unique patients: 6344\n",
            "Unique admissions: 15008\n",
            "Total records: 6344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_statistics(alldata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKViRlcpCxOE",
        "outputId": "05c19dd6-2de1-4e36-a6d6-5ed7afd883f8"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#patients  (6344,)\n",
            "#clinical events  15008\n",
            "#diagnosis  1957\n",
            "#med  57\n",
            "#procedure 1425\n",
            "#avg of diagnoses  10.518923240938166\n",
            "#avg of medicines  8.894922707889126\n",
            "#avg of procedures  3.846015458422175\n",
            "#avg of visits  2.3656998738965953\n",
            "#max of diagnoses  128\n",
            "#max of medicines  49\n",
            "#max of procedures  50\n",
            "#max of visit  29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process DDI"
      ],
      "metadata": {
        "id": "FF1oWhaFrgw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Create DDI adjacency matrix\n",
        "  1 Get Top-40 severity types in TWOSIDES \n",
        "    # Note: the side effect is a string name under the Polypharmacy Side Effect code, many to one\n",
        "  2 Get SMILES molecules from DrugBank, transform into DDI matrix D\n",
        "  3 incorporate the DDI into the graph embeddings using BRICS\n",
        "  4 Ensure ATC Third Level coding (4 chars)\n",
        "'''\n",
        "def make_CID_ATC_map(map_CID_ATClong_file, med_vocab):\n",
        "    # Convert CID-ATC map to ATC Third Level (4 chars), sized to vocab\n",
        "    map_CID_ATC = {}\n",
        "    with open(map_CID_ATClong_file, 'r') as f:\n",
        "        map_CID_ATC = {row[0]: set(token[:4] for token in row if token[:4] in med_vocab.word2index )\\\n",
        "                      for row in csv.reader(f)\\\n",
        "                      if set(token[:4] for token in row if token[:4] in med_vocab.word2index )} # omit ATC codes not found\n",
        "    \n",
        " \n",
        "    assert len(med_vocab.word2index) == 57\n",
        "    assert len(map_CID_ATC) == 389\n",
        "    return map_CID_ATC\n",
        "\n",
        "def process_ddi(external_ddi_file, map_CID_ATClong_file, TOP_N_SIDES=40):\n",
        "    dtypes = {\n",
        "        \"STITCH 1\": \"category\",\n",
        "        \"STITCH 2\": \"category\",\n",
        "        \"Polypharmacy Side Effect\": \"category\",\n",
        "        \"Side Effect Name \": \"category\", # Set 'category' to save mem\n",
        "    }\n",
        "    ddi_df = pd.read_csv(external_ddi_file, dtype=dtypes)\n",
        "\n",
        "    # get top n\n",
        "    ddi_topsides_df = ddi_df.groupby(['Polypharmacy Side Effect', 'Side Effect Name'], as_index=False).count().nlargest(TOP_N_SIDES, 'STITCH 1')\\\n",
        "          .reset_index()\\\n",
        "          .rename(columns={'STITCH 1':'count'}) \\\n",
        "          .drop(columns=['STITCH 2', 'index'])\n",
        "\n",
        "    # get drug CIDs that match the top n\n",
        "    # QUESTION: Grouping by side effect code achieves same impact no?\n",
        "    ddi_filter_df = pd.merge(left=ddi_df,\n",
        "                  right=ddi_topsides_df, \n",
        "                  on ='Side Effect Name', \n",
        "                  how ='inner')\n",
        "\n",
        "    ddi_topsidesdrugpairs_df = ddi_filter_df[['STITCH 1', 'STITCH 2']].drop_duplicates().reset_index(drop=True) # 818796 rows raw, 62791 after dedupe\n",
        "    # NOTE: error in paper: ddi_most_pd = ddi_most_pd.iloc[-TOPN_SIDES:,:] (=bottom N) should be ddi_most_pd = ddi_most_pd.iloc[:TOPN_SIDES,:]\n",
        "\n",
        "    return ddi_topsidesdrugpairs_df"
      ],
      "metadata": {
        "id": "secmtPDvrfsY"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_ddi_adj_matrix(ddi_topsidesdrugpairs_df, map_CID_ATC):\n",
        "    ### construct an adjacency matrix for the med_vocab\n",
        "    # create empty matrix dimensioned to med_vocab\n",
        "    # use the DDI to create edges between drugs in the med_vocab\n",
        "    # if ATC Third Level code is in the med_vocab, place a '1' where those nodes intersect\n",
        "    # mine all potential relations: for every row in DDI table, there are many potential ATC-ATC relations, since one-to-many\n",
        "    # map_CID_ATC tests for set presence\n",
        "\n",
        "    ddi_adj_matrix = np.zeros((med_vocab.size, med_vocab.size))\n",
        "    drug1_list = ddi_topsidesdrugpairs_df['STITCH 1'].to_numpy().tolist()\n",
        "    drug2_list = ddi_topsidesdrugpairs_df['STITCH 2'].to_numpy().tolist()\n",
        "    for i, CID1 in enumerate(drug1_list):\n",
        "        if CID1 in map_CID_ATC:\n",
        "            CID2 = drug2_list[i]\n",
        "            if CID2 in map_CID_ATC:\n",
        "                set_ATC1 = map_CID_ATC[CID1]\n",
        "                set_ATC2 = map_CID_ATC[CID2]\n",
        "                for ATC1 in set_ATC1:\n",
        "                    for ATC2 in set_ATC2:\n",
        "                        index1 = med_vocab.word2index[ATC1]\n",
        "                        index2 = med_vocab.word2index[ATC2]\n",
        "                        ddi_adj_matrix[index1][index1] = 1 # self loop\n",
        "                        ddi_adj_matrix[index2][index2] = 1 # self loop\n",
        "                        ddi_adj_matrix[index1][index2] = 1\n",
        "                        ddi_adj_matrix[index2][index1] = 1 #undirected\n",
        "\n",
        "    assert ddi_adj_matrix.shape == (57, 57)\n",
        "    print(f'Shape of DDI adjacency matrix: {ddi_adj_matrix.shape}')\n",
        "\n",
        "    dill.dump(obj=ddi_adj_matrix, file=open(matrix_ddi_graph_file, 'wb'))\n",
        "\n",
        "    return ddi_adj_matrix"
      ],
      "metadata": {
        "id": "cxsxPcps2z0g"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### construct drug adjacency for set of all patients for GAMENet\n",
        "def make_ehr_adj_matrix(med_vocab, final_ehr):\n",
        "\n",
        "    ehr_adj_matrix = np.zeros((med_vocab.size, med_vocab.size))\n",
        "    for patient in final_ehr:\n",
        "        for admission in patient:\n",
        "            for drug1 in admission[2]:\n",
        "                for drug2 in admission[2]:\n",
        "                    ehr_adj_matrix[drug1][drug2] = 1 # self loop\n",
        "\n",
        "    assert ehr_adj_matrix.shape == (57, 57)\n",
        "    print(f'Shape of ehr adjacency matrix: {ehr_adj_matrix.shape}')\n",
        "\n",
        "    dill.dump(ehr_adj_matrix, open(matrix_ehr_graph_file, 'wb'))  \n",
        "\n",
        "    return ehr_adj_matrix"
      ],
      "metadata": {
        "id": "Hnv_OtgjFuDr"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process molecules"
      ],
      "metadata": {
        "id": "5gSEAAfG2psU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Goal1: make the mask H of size substructures vs drugs\n",
        "### Goal2: make a dict of molecules for MPNN \n",
        "def get_ATC_Drugname_map(med_df):\n",
        "    # gives set() of text names in records per ATC key\n",
        "    #test = 'B02B'\n",
        "    result_map = {}\n",
        "    for ATC, drugname in med_df[['ATC3', 'DRUG']].values:\n",
        "        if ATC in result_map:\n",
        "            result_map[ATC].add(drugname)\n",
        "        else:\n",
        "            result_map[ATC] = set([drugname, drugname]) # prevent treating same string as iterator and returning chars\n",
        "    #assert test in result_map\n",
        "    return result_map\n",
        "\n",
        "def get_ATC_SMILES_map(drugbank, med_df):\n",
        "    # append up to 3 SMILES strings per drug text name\n",
        "    # NOTE: THIS WILL CAPTURE DUPLICATES, test 'N06A'\n",
        "    map_ATC_drugname = get_ATC_Drugname_map(med_df)\n",
        "    db_df = pd.read_csv(drugbank, dtype={'name': 'category', 'moldb_smiles': 'category'})\n",
        "    \n",
        "    map_drugbank_SMILES = defaultdict()\n",
        "    result_map = {}\n",
        "\n",
        "    for drug_altname, drugname, smiles in db_df[['title', 'name', 'moldb_smiles']].values:\n",
        "        if type(smiles) == type('a'):\n",
        "            map_drugbank_SMILES[drugname] = smiles  # drugname to smiles is 1:1\n",
        "            if drug_altname not in map_drugbank_SMILES: # add altnames; there are some MIMIC names in 'title' but not 'name' columns\n",
        "                map_drugbank_SMILES[drug_altname] = smiles \n",
        "\n",
        "    for ATC, drugnames in map_ATC_drugname.items(): # DATA ATCS\n",
        "        group_upto_3 = [map_drugbank_SMILES.get(name)  for name in drugnames if name in map_drugbank_SMILES][:3]\n",
        "        if len(group_upto_3) > 0:\n",
        "            result_map[ATC] = group_upto_3 # if MIMIC contains both name and altname for same ATC, duplicates result\n",
        "\n",
        "    return result_map, map_drugbank_SMILES"
      ],
      "metadata": {
        "id": "SiwQJL9dMtTE"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_chemical_subgroups(ATC_SMILES_map):\n",
        "    def chemset(arg):\n",
        "        return BRICS.BRICSDecompose(Chem.MolFromSmiles(arg)) # returns set\n",
        "\n",
        "    subgroup_set = set()\n",
        "    for ATC, smiles_list in ATC_SMILES_map.items():\n",
        "        for smiles in smiles_list:\n",
        "            subgroup_set.update(chemset(smiles))\n",
        "\n",
        "    map_ATC_BRIC = {}\n",
        "    # map_ATC_BRIC = {ATC: func(smiles) for ATC, smiles_list in ATC_SMILES_map.items() for smiles in smiles_list}\n",
        "    for ATC, smiles_list in ATC_SMILES_map.items():\n",
        "        for smiles in smiles_list:\n",
        "            map_ATC_BRIC[ATC] = chemset(smiles)\n",
        "\n",
        "    return subgroup_set, map_ATC_BRIC"
      ],
      "metadata": {
        "id": "vxD3ZRpf_hcn"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Get H mask, S x M size\n",
        "def make_H_mask(med_vocab, subgroup_set):\n",
        "    # returns map of M*3 SMILES (many SMILES to ATC) to S subgroups \n",
        "    # 1 index the set of all subgroups as S\n",
        "    # - unpack ATCs in medvocab, use the map to get SMILES, run SMILES through BRIC to get subgroups, take set\n",
        "    # 2 make dict of ATC to set of subgroups\n",
        "    # 3 make empty matrix and iterate over all ATCs at mark 1 at subgroup index\n",
        "    H_mask = np.zeros((med_vocab.size, len(subgroup_set))) # should have reversed this to avoid transpose() later\n",
        "\n",
        "    BRIC_list = list(subgroup_set)\n",
        "\n",
        "    for ATC in med_vocab.vocab:\n",
        "        i1 = med_vocab.word2index[ATC]\n",
        "        for subgroup in map_ATC_BRIC[ATC]:\n",
        "            i2 = BRIC_list.index(subgroup)\n",
        "            H_mask[i1][i2] = 1\n",
        "    \n",
        "    print(f'Shape of H mask matrix: {H_mask.shape}')\n",
        "    dill.dump(H_mask, open(matrix_h_mask_file, 'wb'))  \n",
        "\n",
        "    return H_mask"
      ],
      "metadata": {
        "id": "b_gMKx2qWmX6"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    TOP_N_SIDES = 40 # define top n side effects to filter DDIs\n",
        "    map_CID_ATClong_file =  DATA_PATH + 'drug-atc.csv'\n",
        "    external_ddi_file = DATA_PATH + 'drug-DDI.csv' \n",
        "    external_drugbank_file = DATA_PATH + 'drugbank_drugs_info.csv'\n",
        "\n",
        "    ddi_topsides_df = process_ddi(external_ddi_file, map_CID_ATClong_file, TOP_N_SIDES) #63,000 CID matches\n",
        "\n",
        "    map_CID_ATC = make_CID_ATC_map(map_CID_ATClong_file, med_vocab)\n",
        "    map_ATC_SMILES, map_drugbank_SMILES = get_ATC_SMILES_map(external_drugbank_file, med_df)\n",
        "    subgroup_set, map_ATC_BRIC = get_chemical_subgroups(map_ATC_SMILES)\n",
        "    \n",
        "    matrix_ddi_adj = make_ddi_adj_matrix(ddi_topsides_df, map_CID_ATC)\n",
        "    matrix_ehr_adj = make_ehr_adj_matrix(med_vocab, finallist)\n",
        "    matrix_H_mask = make_H_mask(med_vocab, subgroup_set)\n",
        "\n",
        "    dill.dump(map_ATC_SMILES, open(map_ATC_SMILES_file, 'wb')) \n",
        "    print('Matrices made.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBOh8HOEYzzH",
        "outputId": "7d015f22-8343-4388-91bb-7417cec756a7"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: DtypeWarning: Columns (30) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of DDI adjacency matrix: (57, 57)\n",
            "Shape of ehr adjacency matrix: (57, 57)\n",
            "Shape of H mask matrix: (57, 177)\n",
            "Matrices made.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    test = 'B02B'\n",
        "    assert test in med_vocab.vocab\n",
        "    assert test in med_df[['ATC3', 'DRUG']].values\n",
        "    assert test in map_ATC_SMILES\n",
        "    assert test in map_ATC_BRIC"
      ],
      "metadata": {
        "id": "ZfBh1ZL6ZjbY"
      },
      "execution_count": 131,
      "outputs": []
    }
  ]
}