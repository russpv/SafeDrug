{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAMENet.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "GsBMEQlf7vYe",
        "9ACZQA_B7t7l",
        "2nZpS41UFM23",
        "aMkdjJdsG2fj",
        "ZVlnbQyWFTYD",
        "jvdwoUAiFXZk"
      ],
      "authorship_tag": "ABX9TyMF3IxJ5JoTB+fDdWpLWUuW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/russpv/SafeDrug/blob/main/GAMENet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "GsBMEQlf7vYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')\n",
        "\n",
        "! pip install memory_profiler"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nduhxbDCLx45",
        "outputId": "d5ae8d49-08ae-436b-e216-801fe881c0a0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun May  8 19:18:58 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P0    30W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Your runtime has 13.6 gigabytes of available RAM\n",
            "\n",
            "Not using a high-RAM runtime\n",
            "Requirement already satisfied: memory_profiler in /usr/local/lib/python3.7/dist-packages (0.60.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from memory_profiler) (5.4.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Args"
      ],
      "metadata": {
        "id": "9ACZQA_B7t7l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_ghmZMh-FDGV"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "def arg_parser():\n",
        "    \"\"\" Parse command line arguments\n",
        "\n",
        "    Outputs:\n",
        "        arguments {object} -- object containing command line arguments\n",
        "    \"\"\"\n",
        "\n",
        "    # Initializer\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # Add arguments here\n",
        "    parser.add_argument('--Test', action='store_true', default=False, help=\"test mode\")\n",
        "    parser.add_argument('--model_name', type=str, default='none', help=\"model name\")\n",
        "    parser.add_argument('--resume_path', type=str, default='none', help='resume path')\n",
        "    parser.add_argument('--lr', type=float, default=5e-4, help='learning rate')\n",
        "    parser.add_argument('--ddi_loss', action='store_true', default=True, help=\"using ddi loss\")\n",
        "    parser.add_argument('--target_ddi', type=float, default=0.06, help=\"target ddi\")\n",
        "    parser.add_argument('--T', type=float, default=2.0, help='T')\n",
        "    parser.add_argument('--decay_weight', type=float, default=0.85, help=\"decay weight\")\n",
        "    parser.add_argument('--dim', type=int, default=64, help='dimension')\n",
        "    parser.add_argument('--cuda', type=int, default=0, help='which cuda') ###\n",
        "    parser.add_argument('--beta', type=int, default=0, help='SafeDrug PID loss') ###\n",
        "\n",
        "    parser.add_argument('--smalldata', type=int, default=1, help='which cuda') ###\n",
        "    parser.add_argument('--mydata', type=int, default=1, help='which cuda') ###\n",
        "    parser.add_argument('--Inf_time', type=int, default=0, help='which cuda') ###\n",
        " \n",
        "    # Parse and return arguments\n",
        "    return(parser.parse_args(args=[]))\n",
        "\n",
        "args = arg_parser()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import dill\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import pandas as pd\n",
        "import sys\n",
        "import time\n",
        "import statistics\n",
        "import datetime as dt\n",
        "import logging\n",
        "\n",
        "# set seed\n",
        "seed = 1203 #1203\n",
        "random.seed(seed)\n",
        "np.random.seed(seed) #2048\n",
        "torch.manual_seed(seed)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "\n",
        "# define data path\n",
        "DATA_PATH = \"drive/MyDrive/DL4H/Project/PaperCode/processed_orig/\"\n",
        "MYDATA_PATH = \"drive/MyDrive/DL4H/Project/SAFEDRUG_lib/data/processed/\"\n",
        "WORKING_PATH = \"drive/MyDrive/DL4H/Project/GAMENet/\"\n",
        "TEST_PATH = \"drive/MyDrive/DL4H/Project/GAMENet/results/\"\n",
        "\n",
        "# define dataset\n",
        "args.mydata = 0\n",
        "args.smalldata = 0\n",
        "EPOCH = 50\n",
        "\n",
        "# define routine\n",
        "args.Test = False\n",
        "args.Inf_time = True\n",
        "\n",
        "# setting\n",
        "args.model_name = 'GAMENet'\n",
        "\n",
        "args.resume_path = WORKING_PATH + 'saved/' + 'GAMENetEpoch_49_TARGET_0.06_JA_0.7602_DDI_0.05919_2022-05-08 21:32:32.705229.model'\n",
        "#GAMENetEpoch_4_TARGET_0.06_JA_0.2347_DDI_0.1771_2022-05-08 18:42:56.054428.model\n",
        "logger = logging.getLogger('')\n",
        "logger.setLevel(logging.WARNING)"
      ],
      "metadata": {
        "id": "3YVoigj7FKlR"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "2nZpS41UFM23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIKd2LuWFO4F",
        "outputId": "66c86012-d616-45f0-98db-b04450deba5f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data switch\n",
        "if args.mydata == 1:\n",
        "    data_path = MYDATA_PATH + 'ehr.pkl'\n",
        "    voc_path = MYDATA_PATH + 'vocabs.pkl'\n",
        "\n",
        "    ehr_adj_path = MYDATA_PATH + 'ehradj.pkl'\n",
        "    ddi_adj_path = MYDATA_PATH + 'ddiadj.pkl'\n",
        "    ddi_mask_path = MYDATA_PATH + 'hmask.pkl'\n",
        "    molecule_path = MYDATA_PATH + 'atc2SMILES.pkl'\n",
        "\n",
        "    voc = dill.load(open(voc_path, 'rb'))\n",
        "    diag_voc, pro_voc, med_voc = voc['diag_vocab'].index2word, voc['pro_vocab'].index2word, voc['med_vocab'].index2word\n",
        "\n",
        "else:\n",
        "    data_path = DATA_PATH + 'records_final.pkl'\n",
        "    voc_path = DATA_PATH + 'voc_final.pkl'\n",
        "\n",
        "\n",
        "    ehr_adj_path = DATA_PATH + 'ehr_adj_final.pkl'\n",
        "    ddi_adj_path = DATA_PATH + 'ddi_A_final.pkl'\n",
        "    ddi_mask_path = DATA_PATH + 'ddi_mask_H.pkl'\n",
        "    molecule_path = DATA_PATH + 'atc3toSMILES.pkl'\n",
        "    \n",
        "    voc = dill.load(open(voc_path, 'rb'))\n",
        "    diag_voc, pro_voc, med_voc = voc['diag_voc'].idx2word, voc['pro_voc'].idx2word, voc['med_voc'].idx2word\n",
        "\n",
        "ehr_adj = dill.load(open(ehr_adj_path, 'rb'))\n",
        "ddi_adj = dill.load(open(ddi_adj_path, 'rb'))\n",
        "ddi_mask_H = dill.load(open(ddi_mask_path, 'rb'))\n",
        "data = dill.load(open(data_path, 'rb'))\n",
        "molecule = dill.load(open(molecule_path, 'rb')) \n",
        "\n",
        "if args.smalldata == 1:\n",
        "    data_train = data[:200] \n",
        "    data_test = data[200:250]\n",
        "    data_eval = data[250:300]\n",
        "else:\n",
        "    split_point = int(len(data) * 2 / 3)\n",
        "    data_train = data[:split_point]\n",
        "    eval_len = int(len(data[split_point:]) / 2)\n",
        "    data_test = data[split_point:split_point + eval_len]\n",
        "    data_eval = data[split_point+eval_len:]"
      ],
      "metadata": {
        "id": "95gR_HeOFQM6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "aMkdjJdsG2fj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import jaccard_score, roc_auc_score, precision_score, f1_score, average_precision_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sys\n",
        "import warnings\n",
        "import dill\n",
        "from collections import Counter\n",
        "from collections import defaultdict\n",
        "import torch\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def get_n_params(model):\n",
        "    pp=0\n",
        "    for p in list(model.parameters()):\n",
        "        nn=1\n",
        "        for s in list(p.size()):\n",
        "            nn = nn*s\n",
        "        pp += nn\n",
        "    return pp\n",
        "\n",
        "# use the same metric from DMNC\n",
        "def llprint(message):\n",
        "    sys.stdout.write(message)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "def transform_split(X, Y):\n",
        "    x_train, x_eval, y_train, y_eval = train_test_split(X, Y, train_size=2/3, random_state=1203)\n",
        "    x_eval, x_test, y_eval, y_test = train_test_split(x_eval, y_eval, test_size=0.5, random_state=1203)\n",
        "    return x_train, x_eval, x_test, y_train, y_eval, y_test\n",
        "\n",
        "def sequence_output_process(output_logits, filter_token):\n",
        "    pind = np.argsort(output_logits, axis=-1)[:, ::-1]\n",
        "\n",
        "    out_list = []\n",
        "    break_flag = False\n",
        "    for i in range(len(pind)):\n",
        "        if break_flag:\n",
        "            break\n",
        "        for j in range(pind.shape[1]):\n",
        "            label = pind[i][j]\n",
        "            if label in filter_token:\n",
        "                break_flag = True\n",
        "                break\n",
        "            if label not in out_list:\n",
        "                out_list.append(label)\n",
        "                break\n",
        "    y_pred_prob_tmp = []\n",
        "    for idx, item in enumerate(out_list):\n",
        "        y_pred_prob_tmp.append(output_logits[idx, item])\n",
        "    sorted_predict = [x for _, x in sorted(zip(y_pred_prob_tmp, out_list), reverse=True)]\n",
        "    return out_list, sorted_predict\n",
        "\n",
        "\n",
        "def sequence_metric(y_gt, y_pred, y_prob, y_label):\n",
        "    def average_prc(y_gt, y_label):\n",
        "        score = []\n",
        "        for b in range(y_gt.shape[0]):\n",
        "            target = np.where(y_gt[b]==1)[0]\n",
        "            out_list = y_label[b]\n",
        "            inter = set(out_list) & set(target)\n",
        "            prc_score = 0 if len(out_list) == 0 else len(inter) / len(out_list)\n",
        "            score.append(prc_score)\n",
        "        return score\n",
        "\n",
        "\n",
        "    def average_recall(y_gt, y_label):\n",
        "        score = []\n",
        "        for b in range(y_gt.shape[0]):\n",
        "            target = np.where(y_gt[b] == 1)[0]\n",
        "            out_list = y_label[b]\n",
        "            inter = set(out_list) & set(target)\n",
        "            recall_score = 0 if len(target) == 0 else len(inter) / len(target)\n",
        "            score.append(recall_score)\n",
        "        return score\n",
        "\n",
        "\n",
        "    def average_f1(average_prc, average_recall):\n",
        "        score = []\n",
        "        for idx in range(len(average_prc)):\n",
        "            if (average_prc[idx] + average_recall[idx]) == 0:\n",
        "                score.append(0)\n",
        "            else:\n",
        "                score.append(2*average_prc[idx]*average_recall[idx] / (average_prc[idx] + average_recall[idx]))\n",
        "        return score\n",
        "\n",
        "\n",
        "    def jaccard(y_gt, y_label):\n",
        "        score = []\n",
        "        for b in range(y_gt.shape[0]):\n",
        "            target = np.where(y_gt[b] == 1)[0]\n",
        "            out_list = y_label[b]\n",
        "            inter = set(out_list) & set(target)\n",
        "            union = set(out_list) | set(target)\n",
        "            jaccard_score = 0 if union == 0 else len(inter) / len(union)\n",
        "            score.append(jaccard_score)\n",
        "        return np.mean(score)\n",
        "\n",
        "    def f1(y_gt, y_pred):\n",
        "        all_micro = []\n",
        "        for b in range(y_gt.shape[0]):\n",
        "            all_micro.append(f1_score(y_gt[b], y_pred[b], average='macro'))\n",
        "        return np.mean(all_micro)\n",
        "\n",
        "    def roc_auc(y_gt, y_pred_prob):\n",
        "        all_micro = []\n",
        "        for b in range(len(y_gt)):\n",
        "            all_micro.append(roc_auc_score(y_gt[b], y_pred_prob[b], average='macro'))\n",
        "        return np.mean(all_micro)\n",
        "\n",
        "    def precision_auc(y_gt, y_prob):\n",
        "        all_micro = []\n",
        "        for b in range(len(y_gt)):\n",
        "            all_micro.append(average_precision_score(y_gt[b], y_prob[b], average='macro'))\n",
        "        return np.mean(all_micro)\n",
        "\n",
        "    def precision_at_k(y_gt, y_prob_label, k):\n",
        "        precision = 0\n",
        "        for i in range(len(y_gt)):\n",
        "            TP = 0\n",
        "            for j in y_prob_label[i][:k]:\n",
        "                if y_gt[i, j] == 1:\n",
        "                    TP += 1\n",
        "            precision += TP / k\n",
        "        return precision / len(y_gt)\n",
        "    try:\n",
        "        auc = roc_auc(y_gt, y_prob)\n",
        "    except ValueError:\n",
        "        auc = 0\n",
        "    p_1 = precision_at_k(y_gt, y_label, k=1)\n",
        "    p_3 = precision_at_k(y_gt, y_label, k=3)\n",
        "    p_5 = precision_at_k(y_gt, y_label, k=5)\n",
        "    f1 = f1(y_gt, y_pred)\n",
        "    prauc = precision_auc(y_gt, y_prob)\n",
        "    ja = jaccard(y_gt, y_label)\n",
        "    avg_prc = average_prc(y_gt, y_label)\n",
        "    avg_recall = average_recall(y_gt, y_label)\n",
        "    avg_f1 = average_f1(avg_prc, avg_recall)\n",
        "\n",
        "    return ja, prauc, np.mean(avg_prc), np.mean(avg_recall), np.mean(avg_f1)\n",
        "\n",
        "\n",
        "def multi_label_metric(y_gt, y_pred, y_prob):\n",
        "\n",
        "    def jaccard(y_gt, y_pred):\n",
        "        score = []\n",
        "        for b in range(y_gt.shape[0]):\n",
        "            target = np.where(y_gt[b] == 1)[0]\n",
        "            out_list = np.where(y_pred[b] == 1)[0]\n",
        "            inter = set(out_list) & set(target)\n",
        "            union = set(out_list) | set(target)\n",
        "            jaccard_score = 0 if union == 0 else len(inter) / len(union)\n",
        "            score.append(jaccard_score)\n",
        "        return np.mean(score)\n",
        "\n",
        "    def average_prc(y_gt, y_pred):\n",
        "        score = []\n",
        "        for b in range(y_gt.shape[0]):\n",
        "            target = np.where(y_gt[b] == 1)[0]\n",
        "            out_list = np.where(y_pred[b] == 1)[0]\n",
        "            inter = set(out_list) & set(target)\n",
        "            prc_score = 0 if len(out_list) == 0 else len(inter) / len(out_list)\n",
        "            score.append(prc_score)\n",
        "        return score\n",
        "\n",
        "    def average_recall(y_gt, y_pred):\n",
        "        score = []\n",
        "        for b in range(y_gt.shape[0]):\n",
        "            target = np.where(y_gt[b] == 1)[0]\n",
        "            out_list = np.where(y_pred[b] == 1)[0]\n",
        "            inter = set(out_list) & set(target)\n",
        "            recall_score = 0 if len(target) == 0 else len(inter) / len(target)\n",
        "            score.append(recall_score)\n",
        "        return score\n",
        "\n",
        "    def average_f1(average_prc, average_recall):\n",
        "        score = []\n",
        "        for idx in range(len(average_prc)):\n",
        "            if average_prc[idx] + average_recall[idx] == 0:\n",
        "                score.append(0)\n",
        "            else:\n",
        "                score.append(2*average_prc[idx]*average_recall[idx] / (average_prc[idx] + average_recall[idx]))\n",
        "        return score\n",
        "\n",
        "    def f1(y_gt, y_pred):\n",
        "        all_micro = []\n",
        "        for b in range(y_gt.shape[0]):\n",
        "            all_micro.append(f1_score(y_gt[b], y_pred[b], average='macro'))\n",
        "        return np.mean(all_micro)\n",
        "\n",
        "    def roc_auc(y_gt, y_prob):\n",
        "        all_micro = []\n",
        "        for b in range(len(y_gt)):\n",
        "            all_micro.append(roc_auc_score(y_gt[b], y_prob[b], average='macro'))\n",
        "        return np.mean(all_micro)\n",
        "\n",
        "    def precision_auc(y_gt, y_prob):\n",
        "        all_micro = []\n",
        "        for b in range(len(y_gt)):\n",
        "            all_micro.append(average_precision_score(y_gt[b], y_prob[b], average='macro'))\n",
        "        return np.mean(all_micro)\n",
        "\n",
        "    def precision_at_k(y_gt, y_prob, k=3):\n",
        "        precision = 0\n",
        "        sort_index = np.argsort(y_prob, axis=-1)[:, ::-1][:, :k]\n",
        "        for i in range(len(y_gt)):\n",
        "            TP = 0\n",
        "            for j in range(len(sort_index[i])):\n",
        "                if y_gt[i, sort_index[i, j]] == 1:\n",
        "                    TP += 1\n",
        "            precision += TP / len(sort_index[i])\n",
        "        return precision / len(y_gt)\n",
        "\n",
        "    # roc_auc\n",
        "    try:\n",
        "        auc = roc_auc(y_gt, y_prob)\n",
        "    except:\n",
        "        auc = 0\n",
        "    # precision\n",
        "    p_1 = precision_at_k(y_gt, y_prob, k=1)\n",
        "    p_3 = precision_at_k(y_gt, y_prob, k=3)\n",
        "    p_5 = precision_at_k(y_gt, y_prob, k=5)\n",
        "    # macro f1\n",
        "    f1 = f1(y_gt, y_pred)\n",
        "    # precision\n",
        "    prauc = precision_auc(y_gt, y_prob)\n",
        "    # jaccard\n",
        "    ja = jaccard(y_gt, y_pred)\n",
        "    # pre, recall, f1\n",
        "    avg_prc = average_prc(y_gt, y_pred)\n",
        "    avg_recall = average_recall(y_gt, y_pred)\n",
        "    avg_f1 = average_f1(avg_prc, avg_recall)\n",
        "\n",
        "    return ja, prauc, np.mean(avg_prc), np.mean(avg_recall), np.mean(avg_f1)\n",
        "\n",
        "def ddi_rate_score(record, path=ddi_adj_path): ###\n",
        "    # ddi rate\n",
        "    ddi_A = dill.load(open(path, 'rb'))\n",
        "    all_cnt = 0\n",
        "    dd_cnt = 0\n",
        "    for patient in record:\n",
        "        for adm in patient:\n",
        "            med_code_set = adm\n",
        "            for i, med_i in enumerate(med_code_set):\n",
        "                for j, med_j in enumerate(med_code_set):\n",
        "                    if j <= i:\n",
        "                        continue\n",
        "                    all_cnt += 1\n",
        "                    if ddi_A[med_i, med_j] == 1 or ddi_A[med_j, med_i] == 1:\n",
        "                        dd_cnt += 1\n",
        "    if all_cnt == 0:\n",
        "        return 0\n",
        "    return dd_cnt / all_cnt\n",
        "\n",
        "\n",
        "def create_atoms(mol, atom_dict):\n",
        "    \"\"\"Transform the atom types in a molecule (e.g., H, C, and O)\n",
        "    into the indices (e.g., H=0, C=1, and O=2).\n",
        "    Note that each atom index considers the aromaticity.\n",
        "    \"\"\"\n",
        "    atoms = [a.GetSymbol() for a in mol.GetAtoms()]\n",
        "    for a in mol.GetAromaticAtoms():\n",
        "        i = a.GetIdx()\n",
        "        atoms[i] = (atoms[i], 'aromatic')\n",
        "    atoms = [atom_dict[a] for a in atoms]\n",
        "    return np.array(atoms)\n",
        "\n",
        "def create_ijbonddict(mol, bond_dict):\n",
        "    \"\"\"Create a dictionary, in which each key is a node ID\n",
        "    and each value is the tuples of its neighboring node\n",
        "    and chemical bond (e.g., single and double) IDs.\n",
        "    \"\"\"\n",
        "    i_jbond_dict = defaultdict(lambda: [])\n",
        "    for b in mol.GetBonds():\n",
        "        i, j = b.GetBeginAtomIdx(), b.GetEndAtomIdx()\n",
        "        bond = bond_dict[str(b.GetBondType())]\n",
        "        i_jbond_dict[i].append((j, bond))\n",
        "        i_jbond_dict[j].append((i, bond))\n",
        "    return i_jbond_dict\n",
        "\n",
        "def extract_fingerprints(radius, atoms, i_jbond_dict,\n",
        "                         fingerprint_dict, edge_dict):\n",
        "    \"\"\"Extract the fingerprints from a molecular graph\n",
        "    based on Weisfeiler-Lehman algorithm.\n",
        "    \"\"\"\n",
        "\n",
        "    if (len(atoms) == 1) or (radius == 0):\n",
        "        nodes = [fingerprint_dict[a] for a in atoms]\n",
        "\n",
        "    else:\n",
        "        nodes = atoms\n",
        "        i_jedge_dict = i_jbond_dict\n",
        "\n",
        "        for _ in range(radius):\n",
        "\n",
        "            \"\"\"Update each node ID considering its neighboring nodes and edges.\n",
        "            The updated node IDs are the fingerprint IDs.\n",
        "            \"\"\"\n",
        "            nodes_ = []\n",
        "            for i, j_edge in i_jedge_dict.items():\n",
        "                neighbors = [(nodes[j], edge) for j, edge in j_edge]\n",
        "                fingerprint = (nodes[i], tuple(sorted(neighbors)))\n",
        "                nodes_.append(fingerprint_dict[fingerprint])\n",
        "\n",
        "            \"\"\"Also update each edge ID considering\n",
        "            its two nodes on both sides.\n",
        "            \"\"\"\n",
        "            i_jedge_dict_ = defaultdict(lambda: [])\n",
        "            for i, j_edge in i_jedge_dict.items():\n",
        "                for j, edge in j_edge:\n",
        "                    both_side = tuple(sorted((nodes[i], nodes[j])))\n",
        "                    edge = edge_dict[(both_side, edge)]\n",
        "                    i_jedge_dict_[i].append((j, edge))\n",
        "\n",
        "            nodes = nodes_\n",
        "            i_jedge_dict = i_jedge_dict_\n",
        "\n",
        "    return np.array(nodes)\n",
        "\n",
        "\n",
        "def buildMPNN(molecule, med_voc, radius=1, device=\"cpu:0\"):\n",
        "\n",
        "    atom_dict = defaultdict(lambda: len(atom_dict))\n",
        "    bond_dict = defaultdict(lambda: len(bond_dict))\n",
        "    fingerprint_dict = defaultdict(lambda: len(fingerprint_dict))\n",
        "    edge_dict = defaultdict(lambda: len(edge_dict))\n",
        "    MPNNSet, average_index = [], []\n",
        "\n",
        "    for index, atc3 in med_voc.items():\n",
        "\n",
        "        smilesList = list(molecule[atc3])\n",
        "        \"\"\"Create each data with the above defined functions.\"\"\"\n",
        "        counter = 0 # counter how many drugs are under that ATC-3\n",
        "        for smiles in smilesList:\n",
        "            try:\n",
        "                mol = Chem.AddHs(Chem.MolFromSmiles(smiles))\n",
        "                atoms = create_atoms(mol, atom_dict)\n",
        "                molecular_size = len(atoms)\n",
        "                i_jbond_dict = create_ijbonddict(mol, bond_dict)\n",
        "                fingerprints = extract_fingerprints(radius, atoms, i_jbond_dict,\n",
        "                                                    fingerprint_dict, edge_dict)\n",
        "                adjacency = Chem.GetAdjacencyMatrix(mol)\n",
        "                # if fingerprints.shape[0] == adjacency.shape[0]:\n",
        "                for _ in range(adjacency.shape[0] - fingerprints.shape[0]):\n",
        "                    fingerprints = np.append(fingerprints, 1)\n",
        "                \n",
        "                fingerprints = torch.LongTensor(fingerprints).to(device)\n",
        "                adjacency = torch.FloatTensor(adjacency).to(device)\n",
        "                MPNNSet.append((fingerprints, adjacency, molecular_size))\n",
        "                counter += 1\n",
        "            except:\n",
        "                continue\n",
        "        \n",
        "        average_index.append(counter)\n",
        "\n",
        "        \"\"\"Transform the above each data of numpy\n",
        "        to pytorch tensor on a device (i.e., CPU or GPU).\n",
        "        \"\"\"\n",
        "\n",
        "    N_fingerprint = len(fingerprint_dict)\n",
        "    # transform into projection matrix\n",
        "    n_col = sum(average_index)\n",
        "    n_row = len(average_index)\n",
        "\n",
        "    average_projection = np.zeros((n_row, n_col))\n",
        "    col_counter = 0\n",
        "    for i, item in enumerate(average_index):\n",
        "        if item > 0:\n",
        "            average_projection[i, col_counter : col_counter + item] = 1 / item\n",
        "        col_counter += item\n",
        "\n",
        "    return MPNNSet, N_fingerprint, torch.FloatTensor(average_projection)"
      ],
      "metadata": {
        "id": "LP4JTig5nJsB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "ZVlnbQyWFTYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim=16, device=torch.device('cpu:0')):\n",
        "        super(RNN, self).__init__()\n",
        "        \"\"\"\n",
        "        FUNCTIONALITY:\n",
        "            1. embed the input medical code\n",
        "            2. take the average for one visit\n",
        "            3. pass to the RNN visit by visit\n",
        "            4. output embedding seq for each visit\n",
        "        \"\"\"\n",
        "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=emb_dim)\n",
        "        self.rnn = nn.GRU(input_size=emb_dim, hidden_size=emb_dim, bias=True, batch_first=True)\n",
        "        self.init_weights()\n",
        "        self.device = device\n",
        "    \n",
        "    def forward(self, codes):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            codes: [[codes for visit1], [codes for visit2], ...] for ONE patient only\n",
        "                - e.g., [[0,1], [1,2]]  \n",
        "        Output:\n",
        "            embedding seq\n",
        "                - dim: (#visits, emb_dim) \n",
        "        \"\"\"\n",
        "        emb_list = []\n",
        "        for code in codes:\n",
        "            codelist = torch.LongTensor(code).to(self.device) # token indices for retrieving their embeddings\n",
        "            emb = self.embeddings(codelist) # get embed_dim-deep latent representation per token\n",
        "\n",
        "            # take the mean and make one sample per batch\n",
        "            emb_mean = emb.mean(dim=0).unsqueeze(dim=0) #collapse into one embedding per visit CHECK THIS, MAY NOT NEED BATCH DIM\n",
        "            emb_list.append(emb_mean)\n",
        "            \n",
        "        emb_seq = torch.cat(emb_list, dim=0).unsqueeze(dim=0) # CHECK THIS, MAY NOT NEED BATCH DIM or is this to rep (visit, codetype, code)\n",
        "        result, _ = self.rnn(emb_seq) # take output, (batch, num_visits, embed_dim) but we ain't batching?\n",
        "        \n",
        "        logger.debug(f'RNN output size: {result.size()}')\n",
        "        return result\n",
        "    \n",
        "    def init_weights(self):\n",
        "        torch.nn.init.normal_(self.embeddings.weight)\n",
        "        for param in self.rnn.parameters():\n",
        "            if len(param.shape) >= 2:\n",
        "                torch.nn.init.orthogonal_(param.data)\n",
        "            else:\n",
        "                torch.nn.init.normal_(param.data)"
      ],
      "metadata": {
        "id": "h0Sr4ZrAKAFP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatientQuery(nn.Module):\n",
        "    def __init__(self, vocab_diag, vocab_prod, emb_dim=16, device=torch.device('cpu:0')):\n",
        "        super(PatientQuery, self).__init__()\n",
        "        \"\"\"\n",
        "        FUNCTIONALITY:\n",
        "            1. input the embedding given by RNN\n",
        "            2. output the patient representation\n",
        "        \"\"\"\n",
        "        self.rnn_diag = RNN(vocab_diag, emb_dim, device)\n",
        "        self.rnn_prod = RNN(vocab_prod, emb_dim, device)\n",
        "        self.linear = nn.Linear(in_features=emb_dim*2, out_features=emb_dim)\n",
        "        \n",
        "    def forward(self, codes_diag, codes_prod):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            codes_diag: [[diag codes for visit1], [diag codes for visit2], ...]\n",
        "                - e.g., [[0,1], [1,2]]  \n",
        "            codes_prod: [[prod codes for visit1], [prod codes for visit2], ...]\n",
        "                - e.g., [[0,1], [1,2]]  \n",
        "        output:\n",
        "            query embedding:\n",
        "                - size: (#visits, emd_dim)\n",
        "        \"\"\"\n",
        "        emb_diag = self.rnn_diag(codes_diag)\n",
        "        emb_prod = self.rnn_prod(codes_prod)\n",
        "        emb_cat = torch.cat((emb_diag, emb_prod), dim=-1) \n",
        "        \n",
        "        result = self.linear(emb_cat) \n",
        "        logger.debug(f'PatientQuery linear output size: {result.size()}')\n",
        "        return result.squeeze(dim=0)"
      ],
      "metadata": {
        "id": "P1ZkAXFvJ2jD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphConvolution(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        support = torch.mm(input, self.weight)\n",
        "        output = torch.mm(adj, support)\n",
        "        if self.bias is not None:\n",
        "            return output + self.bias\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' \\\n",
        "               + str(self.in_features) + ' -> ' \\\n",
        "               + str(self.out_features) + ')'\n",
        "    \n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, vocab_med, adj, emb_dim=16, device=torch.device('cpu:0')):\n",
        "        super(GCN, self).__init__()\n",
        "        \"\"\"\n",
        "        FUNCTIONALITY: given the adjacency matrix, do graph convolution twice\n",
        "        \"\"\"\n",
        "        \n",
        "        adj = self.normalize(adj + np.eye(adj.shape[0])) # A-tilde\n",
        "        self.adj = torch.FloatTensor(adj).to(device) # A-tilde\n",
        "        \n",
        "        self.gcn1 = GraphConvolution(in_features=vocab_med, out_features=emb_dim)\n",
        "        self.gcn2 = GraphConvolution(in_features=emb_dim, out_features=emb_dim)\n",
        "\n",
        "        # the initial feature\n",
        "        self.x = torch.eye(vocab_med).to(device) ## scalar argument\n",
        "    \n",
        "    def forward(self):\n",
        "\n",
        "        x = self.gcn1(self.x, self.adj)\n",
        "        x = F.relu(x)\n",
        "        x = self.gcn2(x, self.adj)\n",
        "        \n",
        "        logger.debug(f'GCN output size: {x.size()}')\n",
        "        return x\n",
        "    \n",
        "    def normalize(self, adj):\n",
        "        adj = adj / (adj @ np.ones(adj.shape) + 1e-8) # WE DON'T TAKE THE ROOT?\n",
        "        return adj"
      ],
      "metadata": {
        "id": "ILgPh2HnH6W0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MemoryBank(nn.Module):\n",
        "    def __init__(self, vocab_med, adj_ehr, adj_ddi, emb_dim=16, device=torch.device('cpu:0')):\n",
        "        super(MemoryBank, self).__init__()\n",
        "        \"\"\"\n",
        "        FUNCTIONALITY: conbime information from EHR graph and DDI graph\n",
        "        \"\"\"\n",
        "        self.gcn_ehr = GCN(vocab_med, adj_ehr, emb_dim, device)\n",
        "        self.gcn_ddi = GCN(vocab_med, adj_ddi, emb_dim, device)\n",
        "        \n",
        "        self.weight = nn.Parameter(torch.FloatTensor(1))\n",
        "        self.weight.data.uniform_(-0.1, 0.1)\n",
        "        \n",
        "    def forward(self):        \n",
        "\n",
        "        info_ehr = self.gcn_ehr.forward()\n",
        "        info_ddi = self.gcn_ddi.forward()\n",
        "\n",
        "        info_comb = info_ehr - info_ddi * self.weight\n",
        "        logger.debug(f'MB output info_comb size: {info_comb.size()}')\n",
        "        return info_comb"
      ],
      "metadata": {
        "id": "c3cqMDooIP6Z"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DynamicMemory(nn.Module):\n",
        "    def __init__(self, vocab_med, device):\n",
        "        super(DynamicMemory, self).__init__()\n",
        "        \"\"\"\n",
        "        FUNCTIONALITY: generate a historical mapping: query embedding -> multi-hot medication vector\n",
        "        \"\"\"\n",
        "        self.vocab_med = vocab_med # Scalar\n",
        "        self.device = device\n",
        "    \n",
        "    def forward(self, queries, codes_med):\n",
        "        \"\"\"\n",
        "        Input:  queries\n",
        "                    - this is the historical query embedding, given by PatientQuery Module\n",
        "                    - size: (#visits - 1, emb_dim), delete the current query\n",
        "                codes_med\n",
        "                    - this is the historical groud truth med vector\n",
        "                    - format: a list of length (#visits - 1)\n",
        "        \"\"\"\n",
        "        #queries = queries.squeeze(dim=0) # for some reason we need to remove the batch/patient dim\n",
        "        '''HERE remove the last list from within each list of the sequence, dim0 is preserved\n",
        "        '''\n",
        "        DM_key = queries # patient embeddings, the last visit is deleted upstream if at all\n",
        "        DM_value = np.zeros((queries.shape[0], self.vocab_med)) # multi-hot medication vector\n",
        "\n",
        "        for i, _ in enumerate(DM_value): # per visit\n",
        "            if i < len(codes_med): # if meds exist for the visit\n",
        "                for code in codes_med[i]:  # meds of the visit\n",
        "                    DM_value[i][code] = 1\n",
        "        \n",
        "        DM_value = torch.FloatTensor(DM_value).to(self.device)\n",
        "        logger.debug(f'DM_key (visit, embed) size: {DM_key.size()} DM_value (visit, med) size: {DM_value.size()}')\n",
        "        return DM_key, DM_value"
      ],
      "metadata": {
        "id": "bTyZBCp_Ic2W"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Fact1(nn.Module):\n",
        "    def __init__(self, queries):\n",
        "        super(Fact1, self).__init__()\n",
        "        \"\"\"\n",
        "        FUNCTIONALITY: extract the final embedding from input queries, q^t\n",
        "        \"\"\"\n",
        "        self.queries = queries\n",
        "    \n",
        "    def forward(self):\n",
        "        result = self.queries[-1].unsqueeze(dim=0)\n",
        "        logging.debug(f'Fact1 (1, embed): {result.size()}')\n",
        "        return result\n",
        "    \n",
        "\n",
        "class Fact2(nn.Module):\n",
        "    def __init__(self, query, MB):\n",
        "        super(Fact2, self).__init__()\n",
        "        \"\"\"\n",
        "        FUNCTIONALITY: get attention information from MB, o^t_b\n",
        "        Input:\n",
        "            query\n",
        "                - this is the last embedding\n",
        "            MB\n",
        "                - is the memory bank\n",
        "        \"\"\"\n",
        "        self.query = query\n",
        "        self.MB = MB\n",
        "    \n",
        "    def forward(self):\n",
        "        attn_score = torch.mm(self.query, self.MB.t()) # (1, 16) x (16, 131)\n",
        "        attn_matrix = torch.softmax(attn_score, dim=-1) # (1, 131)\n",
        "        result = torch.mm(attn_matrix, self.MB) # (1, 131) x (131, 128)\n",
        "\n",
        "        logger.debug(f'Fact2 (1, embed): {result.size()}')\n",
        "        return result\n",
        "    \n",
        "class Fact3(nn.Module):\n",
        "    def __init__(self, query, MB, DM_key, DM_value):\n",
        "        super(Fact3, self).__init__()\n",
        "        \"\"\"\n",
        "        FUNCTIONALITY: similar to Fact2, get information from the DM, o_d^t\n",
        "        \"\"\"\n",
        "        self.query = query\n",
        "        self.MB = MB\n",
        "        self.DM_key = DM_key\n",
        "        self.DM_value = DM_value\n",
        "    \n",
        "    def forward(self):\n",
        "        attn = torch.softmax(torch.mm(self.query, self.DM_key.t()), dim=1) # (1, 16) x (16, visits)\n",
        "        value = torch.mm(attn, self.DM_value) # (1, visits) x (visits, 131)\n",
        "        out = torch.mm(value, self.MB) # (1, 131) x (131, 16)\n",
        "\n",
        "        logger.debug(f'Fact3 (1, embed): {out.size()}')\n",
        "        return out # (1, 16)"
      ],
      "metadata": {
        "id": "y4-S9vnhImir"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OutNet(nn.Module):\n",
        "    def __init__(self, vocab_med, emb_dim=16):\n",
        "        super(OutNet, self).__init__()\n",
        "        \"\"\"\n",
        "        FUNCTIONALITY: combine fact1, fact2, fact3 to do final prediction\n",
        "        \"\"\"\n",
        "        self.fc1 = nn.Linear(in_features=3*emb_dim, out_features=2*emb_dim, bias=True)\n",
        "        self.fc2 = nn.Linear(in_features=2*emb_dim, out_features=vocab_med, bias=True)\n",
        "        \n",
        "    def forward(self, fact1, fact2, fact3):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            fact1, fact2, fact3:\n",
        "                - three facts q^t, o^t_b, o^t_d\n",
        "                - each size: (1, emb_dim)\n",
        "        \"\"\"\n",
        "        memory_out = torch.cat((fact1, fact2, fact3), dim=-1)\n",
        "        x = self.fc1(memory_out)\n",
        "        x = torch.relu(x)\n",
        "        result = self.fc2(x)\n",
        "\n",
        "        logger.debug(f'OutNet (1, med) size:{result.size()}')\n",
        "        return result # (1, 131)"
      ],
      "metadata": {
        "id": "-li0yJwkI2V-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GAMENet(nn.Module):\n",
        "    def __init__(self, vocab_size, adj_ehr, adj_ddi, emb_dim=64, device=torch.device('cpu:0'), ddi_in_memory=True):\n",
        "        super(GAMENet, self).__init__()\n",
        "\n",
        "        vocab_diag, vocab_prod, vocab_med = vocab_size[0], vocab_size[1], vocab_size[2]\n",
        "        self.patient = PatientQuery(vocab_diag, vocab_prod, emb_dim, device)\n",
        "        self.memorybank = MemoryBank(vocab_med, adj_ehr, adj_ddi, emb_dim, device)\n",
        "        self.dynamicmemory = DynamicMemory(vocab_med, device)\n",
        "        self.outnet = OutNet(vocab_med, emb_dim)\n",
        "        self.tensor_ddi_adj = torch.FloatTensor(ddi_adj).to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        input:\n",
        "                - a list of length #visits / for med -1\n",
        "                - each element is also itself a list\n",
        "        \"\"\"\n",
        "        codes_diag = [visit[0] for visit in x]\n",
        "        codes_prod = [visit[1] for visit in x]\n",
        "        codes_med = [visit[2] for visit in x] \n",
        "\n",
        "        queries = self.patient(codes_diag, codes_prod)\n",
        "        MB = self.memorybank()\n",
        "        DM_key, DM_value = self.dynamicmemory(queries, codes_med) # already squeeze downstream\n",
        "        \n",
        "        # extract three memory outputs, assign to fact1, fact2, fact3\n",
        "        fact1 = Fact1(queries)()\n",
        "        fact2 = Fact2(fact1, MB)()\n",
        "        fact3 = Fact3(fact1, MB, DM_key, DM_value)()\n",
        "\n",
        "        result = self.outnet(fact1, fact2, fact3)\n",
        "        logger.debug(f'GAMENet shape: {result.size()} type: {type(result)}')\n",
        "        if self.training:\n",
        "            neg_pred_prob = torch.sigmoid(result)\n",
        "            neg_pred_prob = neg_pred_prob.t() * neg_pred_prob  # (voc_size, voc_size)\n",
        "            batch_neg = 0.0005 * neg_pred_prob.mul(self.tensor_ddi_adj).sum()\n",
        "            return result, batch_neg\n",
        "        else:\n",
        "            return result"
      ],
      "metadata": {
        "id": "CWYXA5FHFUUj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n"
      ],
      "metadata": {
        "id": "jvdwoUAiFXZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dill\n",
        "import numpy as np\n",
        "import argparse\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import jaccard_score\n",
        "from torch.optim import Adam\n",
        "import os\n",
        "import torch\n",
        "import time\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# evaluate\n",
        "def eval(model, data_eval, voc_size, epoch):\n",
        "    model.eval()\n",
        "\n",
        "    smm_record = []\n",
        "    ja, prauc, avg_p, avg_r, avg_f1 = [[] for _ in range(5)]\n",
        "    med_cnt, visit_cnt = 0, 0\n",
        "\n",
        "    for step, input in enumerate(data_eval):\n",
        "        y_gt, y_pred, y_pred_prob, y_pred_label = [], [], [], []\n",
        "        for adm_idx, adm in enumerate(input):\n",
        "            target_output = model(input[:adm_idx+1])\n",
        "\n",
        "            y_gt_tmp = np.zeros(voc_size[2])\n",
        "            y_gt_tmp[adm[2]] = 1\n",
        "            y_gt.append(y_gt_tmp)\n",
        "\n",
        "            # prediction prod\n",
        "            target_output = torch.sigmoid(target_output).detach().cpu().numpy()[0]\n",
        "            y_pred_prob.append(target_output)\n",
        "            \n",
        "            # prediction med set\n",
        "            y_pred_tmp = target_output.copy()\n",
        "            y_pred_tmp[y_pred_tmp>=0.5] = 1\n",
        "            y_pred_tmp[y_pred_tmp<0.5] = 0\n",
        "            y_pred.append(y_pred_tmp)\n",
        "\n",
        "            # prediction label\n",
        "            y_pred_label_tmp = np.where(y_pred_tmp == 1)[0]\n",
        "            y_pred_label.append(sorted(y_pred_label_tmp))\n",
        "            visit_cnt += 1\n",
        "            med_cnt += len(y_pred_label_tmp)\n",
        "\n",
        "        smm_record.append(y_pred_label)\n",
        "        adm_ja, adm_prauc, adm_avg_p, adm_avg_r, adm_avg_f1 = multi_label_metric(np.array(y_gt), np.array(y_pred), np.array(y_pred_prob))\n",
        "\n",
        "        ja.append(adm_ja)\n",
        "        prauc.append(adm_prauc)\n",
        "        avg_p.append(adm_avg_p)\n",
        "        avg_r.append(adm_avg_r)\n",
        "        avg_f1.append(adm_avg_f1)\n",
        "        llprint('\\rtest step: {} / {}'.format(step+1, len(data_eval)))\n",
        "\n",
        "    # ddi rate\n",
        "    ddi_rate = ddi_rate_score(smm_record, path=ddi_adj_path) ###\n",
        "\n",
        "    llprint('\\nDDI Rate: {:.4}, Jaccard: {:.4},  PRAUC: {:.4}, AVG_PRC: {:.4}, AVG_RECALL: {:.4}, AVG_F1: {:.4}, AVG_MED: {:.4}\\n'.format(\n",
        "        ddi_rate, np.mean(ja), np.mean(prauc), np.mean(avg_p), np.mean(avg_r), np.mean(avg_f1), med_cnt / visit_cnt\n",
        "    ))\n",
        "\n",
        "    return ddi_rate, np.mean(ja), np.mean(prauc), np.mean(avg_p), np.mean(avg_r), np.mean(avg_f1), med_cnt / visit_cnt\n",
        "\n",
        "def main():\n",
        "\n",
        "    device = torch.device(f'cuda:{args.cuda}' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    voc_size = (len(diag_voc), len(pro_voc), len(med_voc))\n",
        "    model = GAMENet(voc_size, ehr_adj, ddi_adj, device=device)\n",
        "    #model.load_state_dict(torch.load(open(args.resume_path, 'rb'))) # switch if resuming\n",
        "\n",
        "    if args.Inf_time:\n",
        "        #https://towardsdatascience.com/the-correct-way-to-measure-inference-time-of-deep-neural-networks-304a54e5187f\n",
        "        model.load_state_dict(torch.load(open(args.resume_path, 'rb'))) \n",
        "        model.to(device=device)\n",
        "        tic = time.time()\n",
        "\n",
        "        starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
        "        repetitions = len(data_test)\n",
        "        timings = np.zeros((repetitions,1))\n",
        "        dummy_input = [[[13, 98, 585, 1065, 21, 37, 454, 278], [69, 47], [4, 22, 12, 2, 67, 0, 86]],\\\n",
        "                       [[377, 326, 21, 46, 454], [115, 94], [3, 6, 12, 14, 5, 22, 2, 29, 1, 16, 11, 86]],\\\n",
        "                       [[377, 246, 453, 46, 21, 454], [151, 127, 128], [14, 2, 6, 29, 18, 0, 86]], [[963, 258, 32, 93, 94, 13, 103, 571, 21], [164, 423, 424, 425, 95, 426, 361, 48, 46, 2], [5, 4, 6, 7, 9, 11, 12, 3, 13, 16, 14, 22, 1, 2, 29, 44, 45, 48, 56, 20, 76, 86]]]\n",
        "\n",
        "        #GPU-WARM-UP\n",
        "        for _ in range(10):\n",
        "            _ = model(dummy_input)\n",
        "        count = 0\n",
        "\n",
        "        # MEASURE PERFORMANCE\n",
        "        with torch.no_grad():\n",
        "            #for rep in range(repetitions):\n",
        "            for rep, example in enumerate(data_test):\n",
        "                starter.record()\n",
        "                _ = model(example)\n",
        "                ender.record()\n",
        "                # WAIT FOR GPU SYNC\n",
        "                torch.cuda.synchronize()\n",
        "                curr_time = starter.elapsed_time(ender)\n",
        "                timings[rep] = curr_time\n",
        "                count += 1\n",
        "\n",
        "        mean_syn = np.sum(timings) / repetitions\n",
        "        std_syn = np.std(timings)\n",
        "        print(f'Inference reps {count}, average: {mean_syn} \\u00B1 {std_syn} seconds')\n",
        "\n",
        "        data = np.array([mean_syn, std_syn, count])\n",
        "        df = pd.DataFrame(data, index=['mean inference time', 'stdev', 'reps'])\n",
        "        df.to_csv(TEST_PATH + 'Inf_' + args.model_name + device.type + f'{dt.datetime.now()}' + '.csv' )\n",
        "\n",
        "        return\n",
        "\n",
        "    if args.Test:\n",
        "        model.load_state_dict(torch.load(open(args.resume_path, 'rb')))\n",
        "        model.to(device=device)\n",
        "        tic = time.time()\n",
        "\n",
        "        ddi_list, ja_list, prauc_list, f1_list, med_list = [], [], [], [], []\n",
        "\n",
        "        result = []\n",
        "        for _ in range(10):\n",
        "            time_start = time.time()\n",
        "            test_sample = np.random.choice(data_test, round(len(data_test) * 0.8), replace=True)\n",
        "            ddi_rate, ja, prauc, avg_p, avg_r, avg_f1, avg_med = eval(model, test_sample, voc_size, 0)\n",
        "            time_sample = time.time() - time_start ###\n",
        "            result.append([ddi_rate, ja, avg_f1, prauc, avg_med, time_sample])\n",
        "            \n",
        "        result = np.array(result)\n",
        "        mean = result.mean(axis=0)\n",
        "        std = result.std(axis=0)\n",
        "\n",
        "        outstring = \"\"\n",
        "        for m, s in zip(mean, std):\n",
        "            outstring += \"{:.4f} \"u\"\\u00B1\"\" {:.4f} & \".format(m, s) ###\n",
        "\n",
        "        print(outstring)\n",
        "        time_round = time.time() - tic\n",
        "        print(f'test time: {time_round}')\n",
        "        \n",
        "        elapsed_time = [0. for _ in range(5)]\n",
        "        elapsed_time.append(time_round)\n",
        "        data = np.array([mean, std, elapsed_time])\n",
        "\n",
        "        df = pd.DataFrame(data, columns=['ddi', 'ja', 'prauc', 'f1', 'med', 'time'], index=['mean', 'std', 'seconds'])\n",
        "        df.to_csv(TEST_PATH + 'Test_' + args.model_name + device.type + f'{dt.datetime.now()}' + '.csv' )\n",
        "\n",
        "        return \n",
        "\n",
        "    if 'cpu' not in device.type:\n",
        "        torch.cuda.reset_peak_memory_stats() # flush \n",
        "    model.to(device=device)\n",
        "    print('parameters', sum(p.numel() for p in model.parameters() if p.requires_grad)) ###\n",
        "    # exit()\n",
        "    optimizer = Adam(list(model.parameters()), lr=args.lr)\n",
        "\n",
        "    # start iterations\n",
        "    history = defaultdict(list)\n",
        "    best_epoch, best_ja = 0, 0\n",
        "\n",
        "    times_train, times_eval = [], [] ###\n",
        "    for epoch in range(EPOCH):\n",
        "        time_start = time.time() ###\n",
        "        print('\\nepoch {} --------------------------'.format(epoch + 1))\n",
        "        normal_loss_count, ddi_loss_count = 0, 0\n",
        "        model.train()\n",
        "        beta_log = [] ###\n",
        "        for step, input in enumerate(data_train): \n",
        "\n",
        "            loss = 0\n",
        "            for idx, adm in enumerate(input):\n",
        "\n",
        "                seq_input = input[:idx+1]\n",
        "                loss_bce_target = np.zeros((1, voc_size[2]))\n",
        "                loss_bce_target[:, adm[2]] = 1\n",
        "\n",
        "                loss_multi_target = np.full((1, voc_size[2]), -1)\n",
        "                for idx, item in enumerate(adm[2]):\n",
        "                    loss_multi_target[0][idx] = item\n",
        "\n",
        "                result, loss_ddi = model(seq_input) # add loss_ddi\n",
        "\n",
        "                loss_bce = F.binary_cross_entropy_with_logits(result, torch.FloatTensor(loss_bce_target).to(device))\n",
        "                loss_multi = F.multilabel_margin_loss(F.sigmoid(result), torch.LongTensor(loss_multi_target).to(device))\n",
        "\n",
        "                if args.ddi_loss:\n",
        "                    result = torch.sigmoid(result).detach().cpu().numpy()[0]\n",
        "                    result[result >= 0.5] = 1\n",
        "                    result[result < 0.5] = 0\n",
        "                    y_label = np.where(result == 1)[0] #indices where 1\n",
        "                    current_ddi_rate = ddi_rate_score([[y_label]], path=ddi_adj_path)\n",
        "                    if current_ddi_rate <= args.target_ddi:\n",
        "                        loss = 0.9 * loss_bce + 0.01 * loss_multi\n",
        "                        normal_loss_count += 1\n",
        "                    elif args.beta == 0:\n",
        "                        rnd = np.exp((args.target_ddi - current_ddi_rate)/args.T)\n",
        "                        if np.random.rand(1) < rnd:\n",
        "                            loss = loss_ddi\n",
        "                            ddi_loss_count += 1\n",
        "                        else:\n",
        "                            loss = 0.9 * loss_bce + 0.01 * loss_multi\n",
        "                            normal_loss_count += 1\n",
        "                    else: \n",
        "                        beta = max(0, 1 - (current_ddi_rate - args.target_ddi) / args.kp) # per SafeDrug\n",
        "                        loss = beta * (0.95 * loss_bce + 0.05 * loss_multi) + (1 - beta) * loss_ddi # alpha = 0.95\n",
        "                    if args.beta == 1: beta_log.append(beta)\n",
        "                else:\n",
        "                    loss = 0.9 * loss_bce + 0.01 * loss_multi\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward(retain_graph=True)\n",
        "                optimizer.step()\n",
        "            \n",
        "            if args.beta == 0:\n",
        "                llprint(f'\\rtraining step: {step+1} / {len(data_train)} loss: {loss} loss_ddi: {loss_ddi} ') ###\n",
        "            else:\n",
        "                llprint(f'\\rtraining step: {step+1} / {len(data_train)} loss: {loss} loss_ddi: {loss_ddi}  beta: {beta}') ###\n",
        "        print()\n",
        "        if args.beta == 1: print(f'\\navg_beta: {statistics.mean(beta_log)}') ###\n",
        "        time_end = time.time()  ###\n",
        "        ddi_rate, ja, prauc, avg_p, avg_r, avg_f1, avg_med = eval(model, data_eval, voc_size, epoch)\n",
        "        time_train = time_end - time_start ###\n",
        "        time_eval = time.time() - time_end  ###\n",
        "        print(f'training time: {time_train}, test time: {time_eval}') ###\n",
        "\n",
        "        times_train.append(time_train) ###\n",
        "        times_eval.append(time_eval) ###\n",
        "\n",
        "        history['ja'].append(ja)\n",
        "        history['ddi_rate'].append(ddi_rate)\n",
        "        history['avg_p'].append(avg_p)\n",
        "        history['avg_r'].append(avg_r)\n",
        "        history['avg_f1'].append(avg_f1)\n",
        "        history['prauc'].append(prauc)\n",
        "        history['med'].append(avg_med)\n",
        "\n",
        "        if epoch >= 5:\n",
        "            print('ddi: {}, Med: {}, Ja: {}, F1: {}, PRAUC: {}'.format(\n",
        "                np.mean(history['ddi_rate'][-5:]),\n",
        "                np.mean(history['med'][-5:]),\n",
        "                np.mean(history['ja'][-5:]),\n",
        "                np.mean(history['avg_f1'][-5:]),\n",
        "                np.mean(history['prauc'][-5:])\n",
        "                ))\n",
        "\n",
        "        torch.save(model.state_dict(), open(WORKING_PATH +''.join(('saved/', args.model_name, \\\n",
        "            'Epoch_{}_TARGET_{:.2}_JA_{:.4}_DDI_{:.4}_{}.model'.format(epoch, args.target_ddi, ja, ddi_rate, dt.datetime.now()))), 'wb')) ###\n",
        "\n",
        "        if epoch != 0 and best_ja < ja:\n",
        "            best_epoch = epoch\n",
        "            best_ja = ja\n",
        "\n",
        "        print('best_epoch: {}'.format(best_epoch))\n",
        "\n",
        "    dill.dump(history, open(WORKING_PATH +'history_{}_{}.pkl'.format(args.model_name, dt.datetime.now()), 'wb')) ###\n",
        "    \n",
        "    timings = np.array(list(zip(times_train, times_eval))) ###\n",
        "    df = pd.DataFrame(timings, columns=['train', 'test']) ###\n",
        "    df.to_csv(TEST_PATH + 'TimesTrain_' + args.model_name + f'{dt.datetime.now()}' + '.csv' ) ###\n",
        "\n",
        "    # Maximum cuda memory allocated\n",
        "    if 'cpu' not in device.type:\n",
        "        print(f'peak training memory allocated: {torch.cuda.max_memory_allocated(device)}')\n",
        "\n"
      ],
      "metadata": {
        "id": "gfKN5U9xFYtg"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execute"
      ],
      "metadata": {
        "id": "D8dIg3bI71IL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    %reload_ext memory_profiler\n",
        "    %memit -r1 main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2h9CBIBIhaYX",
        "outputId": "c3a7b66f-b749-4256-c0e2-0c692bdd2236"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference reps 1058, average: 2.6846971917467894  1.3316779496684863 seconds\n",
            "peak memory: 2798.84 MiB, increment: 0.25 MiB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6A079U0zQGpx",
        "outputId": "2e1d2190-ed54-46b4-dafc-deb99a5ab545"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun May  8 21:32:32 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P0    38W / 250W |    973MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    }
  ]
}